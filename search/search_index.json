{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"DeepLearning/pytorch_intro/","title":"1. Dataset","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p10-p11%20Transforms/","title":"Transforms","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p10-p11%20Transforms/#_1","title":"\u4ecb\u7ecd","text":"<p>transforms\u662fpytorch\u7684\u4e00\u4e2a\u5de5\u5177\u7bb1\uff0c\u91cc\u9762\u5c01\u88c5\u4e00\u4e9b\u5de5\u5177\uff0c\u65cb\u8f6c\u3001\u7f29\u653e\u3001\u6b63\u5219\u5316\u7b49\u4e00\u7cfb\u5217\u64cd\u4f5c\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p10-p11%20Transforms/#_2","title":"\u5177\u4f53\u4f7f\u7528","text":"<pre><code>from PIL import Image\nfrom torchvision import transforms\n\nimage_path = \"..\\..\\data\\\\train\\\\ants\\\\0013035.jpg\"  # \u76f8\u5bf9\u8def\u5f84\nimg_PIL = Image.open(image_path) # PIL\u683c\u5f0f\n\n# \u8f6c\u4e3atensor\ntensor_trans = transforms.ToTensor()\ntensor_img = tensor_trans(img_PIL)\n\nprint(tensor_img)\n\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p10-p11%20Transforms/#tensor","title":"\u4e3a\u4ec0\u4e48\u7528Tensor","text":"<p>\u8fd9\u91cc\u53d1\u73b0Tensor\u548cnumpy\u5dee\u522b\u8fd8\u662f\u5f88\u5927\u7684\uff0c\u5728Tensor\u4e2d\u4fdd\u5b58\u4e86\u68af\u5ea6\u3001\u524d\u5411\u4f20\u64ad\u7b49\u5f88\u591a\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u7528\u7684\u53c2\u6570\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p10-p11%20Transforms/#_3","title":"\u540e\u7eed\u4e00\u70b9\u64cd\u4f5c","text":"<pre><code>from PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.tensorboard import SummaryWriter\n\nimage_path = \"..\\..\\data\\\\train\\\\ants\\\\0013035.jpg\"  # \u76f8\u5bf9\u8def\u5f84\nimg_PIL = Image.open(image_path) # PIL\u683c\u5f0f\n\nwriter = SummaryWriter(\"logs\")\n\n\n# \u8f6c\u4e3atensor\ntensor_trans = transforms.ToTensor()\ntensor_img = tensor_trans(img_PIL)\n\nwriter.add_image(\"tensor_img\",tensor_img)\n\nwriter.close()\n\n# print(tensor_img)\n</code></pre> <p>\u8fd9\u91cc\u5355\u7eaf\u628anumpy\u7c7b\u578b\u6362\u6210\u4e86tensor\u7c7b\u578b</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p12-p13%20%E5%B8%B8%E7%94%A8Transforms/","title":"\u5e38\u7528Transforms","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p12-p13%20%E5%B8%B8%E7%94%A8Transforms/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91ccup\u4e3b\u4ecb\u7ecd\u4e86\u4e00\u4e9btransforms\u65b9\u6cd5\u7684\u4f7f\u7528\uff1a</p> <p>\u81ea\u5df1\u8bb0\u4f4f\u6d41\u7a0b  \u770b\u5b98\u65b9\u6587\u6863\uff0c\u5173\u6ce8\u8f93\u5165\u8f93\u51fa\uff0c\u7528\u7684\u65f6\u5019\u518d\u67e5\u3002</p> <p></p> <p>\u4ee3\u7801\u7684\u8bdd\u5355\u7eaf\u662f\u529f\u80fd\u5b9e\u9645\u4f7f\u7528\uff0c\u8fd8\u662f\u81ea\u5df1\u719f\u6089\u6d41\u7a0b\u6bd4\u8f83\u597d\u3002</p> <pre><code>\nwriter = SummaryWriter(\"logs\")\nimg = Image.open(\"..\\..\\data\\\\train\\\\ants\\\\0013035.jpg\")\nprint(img)\n\n#  ToTensor\ntrans_totensor = transforms.ToTensor()\nimg_tensor = trans_totensor(img)\nwriter.add_image(\"ToTensor\", img_tensor,1)\n\n#  Normalize\n\nprint(img_tensor[0][0][0])  # \u5904\u7406\u524d\uff0c\u770b\u770b\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u7684\u6570\u503c\ntrans_norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # \u56e0\u4e3a\u56fe\u50cf\u65f6\u4e09\u4e2achannel\u7684\uff0c\u6240\u4ee5\uff0c\u5747\u503c\u548c\u65b9\u5dee\u90fd\u662f\u4e09\u7ef4\u7684\nimg_norm = trans_norm(img_tensor)\nprint(img_norm[0][0][0])  # \u5904\u7406\u540e\uff0c\u518d\u770b\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u7684\u6570\u503c\uff0c\u770b\u4e00\u4e0b\u524d\u540e\u53d8\u5316\nwriter.add_image(\"Normalize\", img_norm, 2)\n\n#  Resize  \u8f93\u5165\u662f PIL \u7c7b\u578b\u624d\u884c\n\nprint(img.size)\ntrans_resize = transforms.Resize((128, 128)) # \u5bbd\u548c\u9ad8\uff0c\u5199\u4e00\u4e2a\u4f1a\u7b49\u6bd4\u4f8b\u7f29\u653e\nimg_resize = trans_resize(img)\nimg_resize = trans_totensor(img_resize)\nwriter.add_image(\"Resize\", img_resize, 3)\nprint(img_resize) # \u770b\u4e00\u4e0b\u53d8\u6362\u540e\u7684\u7ef4\u5ea6\n\n#  Compose -resize -2  \u628a\u591a\u4e2atransforms\u5408\u8d77\u6765\uff0c\u524d\u4e00\u4e2a\u7684\u8f93\u51fa\u662f\u540e\u4e00\u4e2a\u7684\u8f93\u5165\n\ntrans_resize_2 = transforms.Resize(128)\ntrans_compose = transforms.Compose([trans_resize_2, trans_totensor])\nimg_resize_2 = trans_compose(img)\nwriter.add_image(\"Resize\", img_resize_2, 4)\n\n#  RandomCrop  \u968f\u673a\u88c1\u526a\n\ntrans_random = transforms.RandomCrop(200, 200)\ntrans_compose_2 = transforms.Compose([trans_random, trans_totensor])\nfor i in range(10):\n    img_crop = trans_compose_2(img)\n    writer.add_image(\"RandomCrop\", img_crop, i)\n\nwriter.close()\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p14%20torchvision/","title":"torchvision","text":"<p>torchvision \u662fpytorch\u6846\u67b6\u4e13\u95e8\u5904\u7406\u6570\u636e\u7684\u4e00\u4e2a\u5305\u3002</p> <p>\u8fd9\u91ccup\u4e3b\u8981\u8bb2\u7684\u662f\u4ecetorchvision\u4e0b\u8f7d\u6bd4\u8f83\u5e38\u7528\u7684\u6570\u636e\u96c6\u3002</p> <pre><code>dataset_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n# \u4e00\u822c\u5bf9\u6570\u636e\u53ef\u80fd\u8981\u505a\u5f88\u591a\u5904\u7406\uff0c\u6211\u4eec\u76f4\u63a5\u521d\u59cb\u5316\u4e00\u4e2acompose\uff0c\u628a\u9700\u8981\u7684\u5904\u7406\u5199\u4e00\u8d77\n# \u6570\u636e\u96c6\u662fPIL\uff0c\u9700\u8981\u8f6cTensor,\u8fd9\u91cc\u7167\u7247\u5f88\u5c0f\uff0c\u6211\u4eec\u4e0d\u505a\u5176\u4ed6\u64cd\u4f5c\n\ntrain_set = torchvision.datasets.CIFAR10(root=\"./dataset\", train=True, transform=dataset_transform, download=True)\ntest_set = torchvision.datasets.CIFAR10(root=\"./dataset\", train=False, transform=dataset_transform, download=True)\n\nprint(test_set[0])\nprint(test_set.classes)\n\nimg, target = test_set[0]\nprint(img)\nprint(target)\nprint(test_set.classes[target])\n\nprint(test_set[0])\nwriter = SummaryWriter(\"p10\")\nfor i in range(10):\n    img, target = test_set[i]\n    writer.add_image(\"test_set\", img, i)\n\nwriter.close()\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e:</p> <p>\u200b   root  \u5b58\u50a8\u8def\u5f84</p> <p>\u200b    train  True/False  \u8bad\u7ec3\u96c6or\u6d4b\u8bd5\u96c6</p> <p>\u200b   transform \u8fdb\u884c\u53d8\u6362</p> <p>\u200b   download   True/False  \u4e0b\u8f7dor\u4e0d\u4e0b\u8f7d\u3002\u5efa\u8bae\u5199True\u3002</p> <p>\u540e\u9762\u5e38\u89c4\u64cd\u4f5c\u4e86\uff0c\u76f8\u5f53\u4e8e\u590d\u4e60\u4e0b\u524d\u9762\u7684\u5185\u5bb9\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p15%20dataloader/","title":"dataloader","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p15%20dataloader/#_1","title":"\u4ecb\u7ecd","text":"<p>up\u5728\u8fd9\u91cc\u7c7b\u522b\u4e86\u6253\u724c\uff0cdataloader\u5c31\u662f\u4ece\u724c\u5806\u4e2d\u6293\u53d6\u6251\u514b\u724c\u3002</p> <pre><code># \u5bfc\u5305\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\ntest_data = torchvision.datasets.CIFAR10(\"./dataset\", train=False, transform=torchvision.transforms.ToTensor(),download=True) # \u52a0\u8f7d\u6570\u636e\u96c6\n\ntest_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e:</p> <p>\u200b       batch_size \u6bcf\u6b21\u6253\u5305\u591a\u5c11\u4e2a\u6570\u636e</p> <p>\u200b       shuffle \u662f\u5426\u6253\u4e71\u987a\u5e8f</p> <p>\u200b       num_workers \u7ebf\u7a0b\u6570\uff0cwindows&gt;0\u53ef\u80fd\u51fa\u95ee\u9898</p> <p>\u200b       drop_last   \u5982\u679c\u6700\u540e\u4e00\u7ec4\u6570\u91cf&lt;batch_size\u662f\u5426\u820d\u5f03</p> <p></p> <p>\u56fe\u7247\u8f85\u52a9\u7406\u89e3batch_size\u3002</p> <pre><code># \u6d4b\u8bd5\u6570\u636e\u96c6\u4e2d\u7b2c\u4e00\u5f20\u56fe\u7247\u53catarget\nimg, target = test_data[0]\n# print(img.shape)  \n# print(target) \n\n# \u8fd9\u91cc\u6253\u5305batch_size\u4e2aimg\u548ctarget \u8fdb\u5165\u4e24\u4e2a\u5217\u8868\nfor data in test_loader:  # \u8fd9\u4e2aloader\uff0c\u8fd4\u56de\u7684\u5185\u5bb9\uff0c\u5c31\u5df2\u7ecf\u662f\u5305\u542b\u4e86 img \u548c target \u4e24\u4e2a\u503c\u4e86\uff0c\u8fd9\u4e2a\u5728 cifar \u6570\u636e\u96c6\u7684 getitem \u51fd\u6570\u91cc\uff0c\u5199\u4e86\n    imgs, targets = data\n    print(imgs.shape)\n    print(targets)\n</code></pre> <p>\u6253\u5370\u8f93\u51fa\u4e0bimgs\u548ctargets\uff0c\u662f\u4e24\u4e2a\u5217\u8868\u3002</p> <pre><code>writer = SummaryWriter(\"dataloader\")\nfor epoch in range(2):  # \u4e24\u8f6e,\u9a8c\u8bc1shuffle\u6bcf\u6b21\u6253\u4e71\u662f\u968f\u673a\u6253\u4e71(\u53ef\u4ee5\u8bbe\u7f6e\u968f\u673a\u79cd\u5b50\uff0c\u6bcf\u6b21\u6253\u4e71\u987a\u5e8f\u4e00\u6837)\n    step = 0\n    for data in test_loader:  \n        imgs, targets = data\n        writer.add_images(\"Epoch: {}\".format(epoch), imgs, step)\n        step = step + 1\n\nwriter.close()\n</code></pre> <p>\u5b58\u4e0btensorboard\uff0c\u770b\u4e0b\u4e24\u6b21\u6253\u4e71\u662f\u5426\u4e0d\u4e00\u6837\u3002</p> <p></p> <p>\u53d1\u73b0\u4e24\u6b21\u662f\u4e0d\u4e00\u6837\u7684\u3002</p> <p>\u5173\u4e8e\u6570\u636e\u9884\u5904\u7406\u7684\u90e8\u5206\u5230\u6b64\u7ed3\u675f\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p16%20nn.module/","title":"nn.module","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p16%20nn.module/#_1","title":"\u4ecb\u7ecd","text":"<p>nn.module\u662fpytorch\u6846\u67b6\u4e2d\u642d\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u7840\uff0c\u6240\u6709\u7684\u795e\u7ecf\u7f51\u7edc\u90fd\u9700\u8981\u7ee7\u627f\u8fd9\u4e2a\u6a21\u5757\u3002</p> <p>ps:\u8fd9\u91cc\u8fd8\u662f\u4ed4\u7ec6\u770b\u5b98\u65b9\u6587\u6863</p> <pre><code>import torch\nfrom torch import nn\n\n\nclass Model(nn.Module):\n    def __init__(self): \n        super().__init__() \n\n    def forward(self, input):  # \u524d\u5411\u4f20\u64ad\n        output = input + 23\n        return output\n\nmodel = Model()  \nx = torch.tensor(1.0)  \noutput = model(x)  \nprint(output)\n\n</code></pre> <p>\u8fd9\u5757\u5c31\u662f\u8bb2\u4e86\u795e\u7ecf\u7f51\u7edc\u7ee7\u627f\u4e0b\u8fd9\u4e2a\u7c7b\uff0c\u627e\u4e2a\u7b80\u5355\u795e\u7ecf\u7f51\u7edc\u5199\u4e00\u4e0b\u5c31\u61c2\u4e86\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p17%20%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C/","title":"\u5377\u79ef\u64cd\u4f5c","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p17%20%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C/#_1","title":"\u4ecb\u7ecd","text":"<p>import torch.nn.functional as F </p> <p>\u8fd9\u91ccfunction\u662fnn\u7684\u5b50\u5305\uff0c\u4f46\u662f\u5f88\u5e38\u7528\uff0c\u4e00\u822c\u5355\u72ec\u5bfc\u5165\u3002</p> <p>\u7136\u540e\u4ecb\u7ecd\u5377\u79ef\u64cd\u4f5c\uff1a</p> <p>\u200b       \u5377\u79ef\u64cd\u4f5c\u76ee\u7684\u662f\u51cf\u5c11\u56fe\u7247\u7ef4\u5ea6(\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf),\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002</p> <p></p> <p>\u5177\u4f53\u64cd\u4f5c\u662f\u5377\u79ef\u6838\u5bf9\u4f4d\u76f8\u4e58\u4f5c\u4e3a\u7ed3\u679c\uff0c\u7b97\u5b8c\u4e4b\u540e\u5f80\u540e\u5f80\u4e0b\u8d70\u3002</p> <p>\u8fd9\u91cc\u4e0d\u60f3\u591a\u8bb0\u3002</p> <p>\u7136\u540e\u4ee3\u7801\uff1a</p> <pre><code>import torch\nimport torch.nn.functional as F  # \u5236\u4f5c\u4e00\u4e2a\u51fd\u6570\u7684\u53e5\u67c4\uff0c\u540e\u9762\u65b9\u4fbf\u76f4\u63a5\u4f7f\u7528\u4e86\n\ninput = torch.tensor([[1, 2, 0, 3, 1],\n                      [0, 1, 2, 3, 1],\n                      [1, 2, 1, 0, 0],\n                      [5, 2, 3, 1, 1],\n                      [2, 1, 0, 1, 1]])\n\nkernel = torch.tensor([[1, 2, 1],\n                       [0, 1, 0],\n                       [2, 1, 0]])\nprint(\"input:\", input)\nprint(\"kernel:\", kernel)\n\nprint(\"input.shape:\", input.shape)\nprint(\"kernel.shape:\", kernel.shape)\n\n\n#  \u8981\u60f3\u7528 torch.nn.functional.conv2d \u8fd9\u4e2a\u51fd\u6570\uff0c\u5c31\u5fc5\u987b\u6ee1\u8db3\u5f62\u72b6\u7684\u8981\u6c42\uff0c\u4e0a\u8ff0\u7684\u5c3a\u5bf8\u4e0d\u6ee1\u8db3\uff0c\u8981\u505a\u5904\u7406\n#  \u4e0a\u8ff0\u7684\u5c3a\u5bf8\uff0c\u53ea\u6709input.shape: torch.Size([5, 5])\uff0c kernel.shape: torch.Size([3, 3])\uff0c\u5e76\u6ca1\u67094\u4e2a\u901a\u9053\n\n\ninput = torch.reshape(input, (1, 1, 5, 5))  # \u6ce8\u610f\u8fd94\u4e2a\u6570\u5b57\u7684\u610f\u4e49\uff0c\u5206\u522b\u662f\uff1abatch_size, in_channel, H, W , \u53d8\u6362\u5f62\u72b6\u4e4b\u540e\uff0c\u91cd\u65b0\u8d4b\u503c\u7ed9 input\nkernel = torch.reshape(kernel, (1, 1, 3, 3))  # \u6ce8\u610f\u8fd94\u4e2a\u6570\u5b57\u7684\u610f\u4e49\uff0c\u8ddf\u4e0a\u9762\u7684\u4e0d\u4e00\u6837\u4e86\n\nprint(\"input.shape:\", input.shape)\nprint(\"kernel.shape:\", kernel.shape)\n\noutput = F.conv2d(input, kernel, stride=1)\nprint(output)\n\noutput2 = F.conv2d(input, kernel, stride=2)\nprint(output2)\n\noutput3 = F.conv2d(input, kernel, stride=1, padding=1)  # padding \u8bbe\u7f6e\u7684\u503c\uff0c\u662f\u5f80\u5916\u6269\u5145\u7684\u884c\u5217\u6570\uff0c\u503c\u90fd\u662f0\uff0c\u81f3\u4e8e\u60f3\u8981\u4fee\u6539\u8fd9\u4e2a\u503c\uff0c\u8fd8\u6709\u53e6\u5916\u4e00\u4e2a\u53c2\u6570\uff0c\u4e00\u822c\u4e0d\u6539\nprint(output3)\n\noutput4 = F.conv2d(input, kernel, stride=1, padding=0)  # padding \u9ed8\u8ba4\u503c\u662f 0\nprint(output4)\n</code></pre> <p>\u6ce8\u610f\u7684\u51e0\u4e2a\u70b9\uff1a</p> <p>\u200b   1.\u6ce8\u610f\u5377\u79ef\u8981\u6c42\u7684\u8f93\u5165\u7ef4\u5ea6\uff0c\u53c2\u6570\u8981\u56db\u4e2abatch_size(\u6279\u91cf\u5927\u5c0f)\uff0cin_channel(\u8f93\u5165\u901a\u9053\u6570)\uff0c\u9ad8\u548c\u5bbd</p> <p>\u200b   2.\u53c2\u6570\u8bf4\u660e:</p> <p>\u200b           stride  \u6b65\u957f\uff0c\u5373\u6bcf\u6b21\u5377\u79ef\u8fd0\u7b97\u5b8c\u6210\u8d70\u51e0\u4e2a\u683c\u5b50\u3002</p> <p>\u200b           padding  \u586b\u5145\uff0c\u4e0a\u4e0b\u5de6\u53f3\u90fd\u586b\u5145\uff0c\u586b\u51451\u53733x3\u53d85x5</p> <p>\u200b           \u56fe\u7247\u8f85\u52a9\u7406\u89e3\uff1a</p> <p></p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p18%20%E5%8D%B7%E7%A7%AF%E5%B1%82/","title":"\u5377\u79ef\u5c42","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p18%20%E5%8D%B7%E7%A7%AF%E5%B1%82/#_1","title":"\u4ecb\u7ecd","text":"<p>\u5377\u79ef\u5c42\u5bf9\u5e94\u524d\u9762\u7684\u5377\u79ef\u64cd\u4f5c\uff0c\u76f4\u63a5\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u5c42\u3002</p> <pre><code>import torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import Conv2d\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n</code></pre> <pre><code>dataset = torchvision.datasets.CIFAR10(\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                       download=True)\ndataloader = DataLoader(dataset, batch_size=64)\n\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.conv1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)  # \u8f93\u5165\u901a\u9053(\u8fd9\u91ccRGB\u662f3)\u3001\u8f93\u51fa\u901a\u9053(\u5377\u79ef\u6838\u6570\u91cf)\n    def forward(self, x):\n        x = self.conv1(x)  # x \u5df2\u7ecf\u653e\u5230\u4e86\u5377\u79ef\u5c42 conv1\u5f53\u4e2d\u4e86\n        return x\n\n\ntudui = Tudui()  # \u521d\u59cb\u5316\u7f51\u7edc\nprint(tudui)\n\n# \u4e0b\u9762\u628a\u6bcf\u4e00\u5f20\u56fe\u50cf\u90fd\u8fdb\u884c\u5377\u79ef\n\nwriter = SummaryWriter(\"logs\")\n\nstep = 0\nfor data in dataloader:\n    imgs, targets = data  \n    output = tudui(imgs)\n    print(\"imgs.shape:\", imgs.shape)  \n    print(\"output.shape:\", output.shape)  \n    # torch.Size([64, 3, 32, 32])\n    writer.add_images(\"input\", imgs, step)\n    # torch.Size([64, 6, 30, 30])  \u7531\u4e8e6\u4e2achannel\u7684\u56fe\u50cf\uff0c\u662f\u65e0\u6cd5\u663e\u793a\u7684\n    # torch.Size([xxx, 3, 30, 30])\n    output = torch.reshape(output, (-1, 3, 30, 30))  # \u5f3a\u884c\u8f6c\uff0c\u65e0\u4f9d\u636e\uff0c\u6b63\u5e38\u53ef\u4ee5\u6c47\u805a\u5c42\u3001\u5947\u5f02\u503c\u5206\u89e3\u7b49\u3002\n    writer.add_images(\"output\", output, step)\n    step += 1\n\nwriter.close()\n\n\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e:</p> <p>\u200b   in_channels \u8f93\u5165\u901a\u9053\u6570</p> <p>\u200b   out_channels    \u8f93\u51fa\u901a\u9053\u6570\uff0c\u5bf9\u5e94\u5377\u79ef\u6838\u6570\u91cf</p> <p>\u200b   kernel_size \u5377\u79ef\u6838\u5927\u5c0f\uff0c\u8fd9\u91cc\u5377\u79ef\u6838\u5927\u5c0f\u662fn*n\u7684</p> <p>\u200b   stride  \u6b65\u957f</p> <p>\u200b   padding \u586b\u5145</p> <p>\u540e\u9762\u6ca1\u5565\u53ef\u4ecb\u7ecd\u7684\uff0c\u6ce8\u610f\u4e0b\u7ec6\u8282\u5c31\u884c\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p19%20%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%E5%B1%82/","title":"\u6700\u5927\u6c60\u5316\u5c42","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p19%20%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%E5%B1%82/#_1","title":"\u4ecb\u7ecd","text":"<p>\u5982\u679c\u8bf4\u5377\u79ef\u5c42\u628a\u76f8\u90bb\u7684\u4fe1\u606f\u6574\u5408\u8d77\u6765\uff0c\u63d0\u53d6\u4e86\u56fe\u7247\u7684\u7279\u5f81\u3002</p> <p>\u90a3\u4e48\u6c60\u5316\u5c42\u5c31\u662f\u628a\u8fd9\u4e9b\u7279\u5f81\u6574\u5408\u8d77\u6765\uff0c\u6765\u964d\u4f4e\u7279\u5f81\u548c\u4f4d\u7f6e\u7684\u5173\u8054\u7a0b\u5ea6\u3002</p> <p>\u4e00\u822c\u4e8c\u8005\u4ea4\u66ff\u51fa\u73b0\u3002</p> <pre><code>import torch\nfrom torch import nn\nfrom torch.nn import MaxPool2d\n\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a</p> <p>\u200b   kernel_size \u5377\u79ef\u6838\u5927\u5c0f</p> <p>\u200b   ceil_mode   \u5bf9\u4e8e\u4e0d\u6ee1\u5377\u79ef\u6838\u5927\u5c0f\u7684\uff0c\u5982\u4f55\u64cd\u4f5c</p> <p></p> <p>\u5982\u4e0a\uff0c\u4e3e\u4f8b\u8bf4\u660e\u4e86True\u548cFalse\u7684\u533a\u522b\u3002</p> <pre><code>dataset = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                       download=True)\n\ndataloader = DataLoader(dataset, batch_size=64)\n\ninput = torch.tensor([[1, 2, 0, 3, 2],\n                      [3, 2, 4, 5, 6],\n                      [3, 4, 5, 6, 2],\n                      [1, 3, 2, 6, 5],\n                      [5, 6, 2, 1, 3]], dtype=torch.float32) #\u5fc5\u987b\u6307\u5b9a\uff0c\u4e0d\u7136\u9ed8\u8ba4\u6574\u578b\u4f1a\u62a5\u9519\n\ninput_reshape = torch.reshape(input, (-1, 1, 5, 5))\n\nprint(input_reshape)\nprint(input_reshape.shape)\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=True)\n\n    def forward(self, input):\n        output = self.maxpool1(input)\n        return output\n\n\ntudui = Tudui()\n\n\nwriter = SummaryWriter(\"maxpool_log\")\nstep = 0\n\n\nfor data in dataloader:\n    imgs, targets = data\n    output = tudui(imgs)\n    writer.add_images(\"input\", imgs, step)\n    writer.add_images(\"maxpool\", output, step)\n    step += 1\n\nwriter.close()\n\n\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p20%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/","title":"\u6fc0\u6d3b\u51fd\u6570","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p20%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91cc\u5b9e\u9645\u4e0a\u7528Relu\u6765\u4e3e\u4f8b\u6fc0\u6d3b\u51fd\u6570\u3002</p> <p>\u6fc0\u6d3b\u51fd\u6570\u628a\u7ebf\u6027\u53d8\u6210\u975e\u7ebf\u6027\uff0c\u5177\u6709\u66f4\u591a\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u66f4\u5bb9\u6613\u62df\u5408\u3002</p> <p>Relu\u5c31\u4e24\u4e2a\u53c2\u6570(input,inplace)</p> <p>inplace\u662f\u8bf4\u662f\u5426\u8986\u76d6input(\u9ed8\u8ba4False)</p> <p></p> <pre><code>import torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import ReLU, Sigmoid\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\n'''\n\u975e\u7ebf\u6027\u6fc0\u6d3b\u5c42\uff0c\u53ea\u6709\u4e00\u4e2abatch_size\u4e00\u4e2a\u53c2\u6570\u9700\u8981\u8bbe\u7f6e\n'''\n\ninput = torch.tensor([[1, -0.5],\n                      [-1, 3]])\n\ninput = torch.reshape(input, (-1, 1, 2, 2))\nprint(input.shape)\n\ndataset = torchvision.datasets.CIFAR10(\"../dataset\", train=False, download=True,\n                                       transform=torchvision.transforms.ToTensor())\n\ndataloader = DataLoader(dataset, batch_size=64)\n\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.relu1 = ReLU()\n        self.sigmoid1 = Sigmoid()\n\n    def forward(self, input):\n        output = self.sigmoid1(input)\n        return output\n\n\ntudui = Tudui()\n\nwriter = SummaryWriter(\"logs_relu\")\nstep = 0\nfor data in dataloader:\n    imgs, targets = data\n    writer.add_images(\"input\", imgs, step)\n    output = tudui(imgs)\n    writer.add_images(\"output\", output, step)\n    step += 1\n\nwriter.close()\n\n'''\n\u5b9e\u73b0\u7684\u6548\u679c\u5c31\u662f\uff1a\ntorch.Size([1, 1, 2, 2])\ntensor([[1., 0.],\n        [0., 3.]])\n'''\n\n'''\n\u975e\u7ebf\u6027\u53d8\u6362\u7684\u76ee\u7684\uff1a\n\u7ed9\u7f51\u7edc\u4e2d\uff0c\u5f15\u5165\u975e\u7ebf\u6027\u7684\u7279\u5f81\uff0c\u975e\u7ebf\u6027\u7279\u5f81\u591a\u7684\u8bdd\uff0c\u624d\u80fd\u8bad\u7ec3\u51fa\u7b26\u5408\u5404\u79cd\u66f2\u7ebf\u6216\u7279\u5f81\u7684\u6a21\u578b\n\u5426\u5219\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u597d\n'''\n\n\n</code></pre> <p>\u4ee3\u7801\u6ca1\u4ec0\u4e48\u503c\u5f97\u5173\u6ce8\u7684\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p21%20%E7%BA%BF%E6%80%A7%E5%B1%82/","title":"\u7ebf\u6027\u5c42","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p21%20%E7%BA%BF%E6%80%A7%E5%B1%82/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fc7\u4e86\u4e0bpytorch\u4e2d\u7684\u5404\u79cd\u5c42\u5427\uff0c\u53ea\u80fd\u8bf4\uff0c\u5b66\u4f1a\u81ea\u5df1\u770b\u5b98\u65b9\u6587\u6863\u3002</p> <pre><code>import torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import Linear\nfrom torch.utils.data import DataLoader\n</code></pre> <pre><code>dataset = torchvision.datasets.CIFAR10(\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                       download=True)\n\ndataloader = DataLoader(dataset, batch_size=64,drop_last=True)\n\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.linear1 = Linear(196608, 10)\n\n    def forward(self, input):\n        output = self.linear1(input)\n        return output\n\n\ntudui = Tudui()\n\nfor data in dataloader:\n    imgs, targets = data\n    # print(imgs.shape)\n    # output = torch.reshape(imgs, (1, 1, 1, -1))\n    output = torch.flatten(imgs)  # \u5c55\u5e73(1\u884c)\n    print(output.shape)\n    output = tudui(output)\n    print(output.shape)\n\n'''\n\u6b63\u5219\u5316\u5c42  Normalization Layers   nn.BatchNorm2d \n\u6709\u4e00\u7bc7\u8bba\u6587\uff0c\u610f\u601d\u662f\u6b63\u5219\u5316\u5c42\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6\n\n\u53c2\u6570\u53ea\u6709\u4e00\u4e2a\uff0cchannel\u4e2d\u7684C\uff0cnum_feature, \u4ee4\u5176\u8ddf channel \u6570\u76f8\u540c\u5373\u53ef\uff0c\u5b98\u65b9\u6587\u6863\u6709\u4e2a\u4e8b\u4f8b\uff1a\n\n&gt;&gt;&gt; # With Learnable Parameters\n&gt;&gt;&gt; m = nn.BatchNorm2d(100)\n&gt;&gt;&gt; # Without Learnable Parameters           # \u4e0d\u542b\u53ef\u5b66\u4e60\u53c2\u6570\n&gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False)    # \u8fd9\u91cc\u7684 100\uff0c\u662f\u8ddf\u7740\u4e0b\u4e00\u884c\u7684100\uff08channel\uff09\u8bbe\u7f6e\u7684\n&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)\n&gt;&gt;&gt; output = m(input)\n\n'''\n\n'''\n\u5b98\u65b9\u6587\u6863\u6709\u4e00\u4e9b\u5199\u597d\u7684\u7f51\u7edc\n'''\n\n</code></pre> <p>\u8fd9\u5757\u592a\u719f\u6089\u4e86\uff0c\u4e0d\u5c55\u5f00\u4e86\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p22%20sequential/","title":"sequential","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p22%20sequential/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91cc\u4ecb\u7ecd\u4e86sequential</p> <p>\u4e0d\u8fc7\u5b9e\u73b0\u7684\u65f6\u5019\u91cd\u70b9\u5728\u7b97\u4e0b\u53c2\u6570\u5427\u3002</p> <p></p> <p>\u4ee3\u7801\u770b\u4e0bnotebook\u5c31OK\u4e86\u3002</p> <pre><code>import torch\nfrom torch import nn\nfrom torch.nn import Conv2d, MaxPool2d, Linear\nfrom torch.nn.modules.flatten import Flatten\nfrom torch.nn.modules import Sequential\n# from torch.utils.tensorboard import SummaryWriter\n\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.conv1 = Conv2d(3, 32, 5, padding=2)  \n        self.maxpool1 = MaxPool2d(2)   \n        self.conv2 = Conv2d(32, 32, 5, padding=2)\n        self.maxpool2 = MaxPool2d(2)\n        self.conv3 = Conv2d(32, 64, 5, padding=2)\n        self.maxpool3 = MaxPool2d(2)\n        self.flatten = Flatten()  # \u5c55\u5e73\u64cd\u4f5c\n        self.linear1 = Linear(64 * 4 * 4, 64)\n        self.linear2 = Linear(64, 10)\n        self.model1 = Sequential(\n            Conv2d(3, 32, 5, padding=2)  ,\n            MaxPool2d(2)   ,\n            Conv2d(32, 32, 5, padding=2),\n            MaxPool2d(2),\n            Conv2d(32, 64, 5, padding=2),\n            MaxPool2d(2),\n            Flatten(),  # \u5c55\u5e73\u64cd\u4f5c\n            Linear(64 * 4 * 4, 64),\n            Linear(64, 10))\n\n    def forward(self, m):\n        # m = self.conv1(m)\n        # m = self.maxpool1(m)\n        # m = self.conv2(m)\n        # m = self.maxpool2(m)\n        # m = self.conv3(m)\n        # m = self.maxpool3(m)\n        # m = self.flatten(m)\n        # m = self.linear1(m)\n        # m = self.linear2(m)\n        m = self.model1(m)\n        return m\n\n\ntudui = Tudui()\nprint(\"tudui:\", tudui)\ninput = torch.ones((64, 3, 32, 32))\noutput = tudui(input)\nprint(\"output.shape:\", output.shape)\n\n\n\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p23%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/","title":"\u635f\u5931\u51fd\u6570\u548c\u53cd\u5411\u4f20\u64ad","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p23%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91ccup\u8bb2\u4e86\u770b\u5b98\u65b9\u6587\u6863\u7684\u635f\u5931\u51fd\u6570\u3002</p> <p></p> <p></p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p24%20%E4%BC%98%E5%8C%96%E5%99%A8/","title":"\u4f18\u5316\u5668","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p24%20%E4%BC%98%E5%8C%96%E5%99%A8/#_1","title":"\u4ecb\u7ecd","text":"<p>\u4f18\u5316\u5668\u8fd9\u91cc\u8fd8\u662f\u8be6\u7ec6\u8bf4\u4e00\u4e0b\u5427\u3002</p> <p>\u6bcf\u4e2a\u4f18\u5316\u5668\u5177\u4f53\u53c2\u6570\u90fd\u4e0d\u592a\u4e00\u6837\uff0c\u4f46\u662f\u4e00\u822c\u90fd\u7528\u9ed8\u8ba4\u503c\uff0c\u53ea\u4f20\u53c2\u6570\u548c\u5b66\u4e60\u7387\u5c31ok\u4e86\u3002</p> <p>\u5b66\u4e60\u7387\u4e0d\u8981\u592a\u5927\uff0c0.01\u597d\u4e00\u4e9b\u3002</p> <pre><code>loss = nn.CrossEntropyLoss()  # \u5b9a\u4e49\u635f\u5931\u51fd\u6570\ntudui = Tudui()\noptim = torch.optim.SGD(tudui.parameters(), lr=0.01)\nfor epoch in range(20):\n    running_loss = 0.0\n    for data in dataloader:\n        imgs, targets = data\n        outputs = tudui(imgs)\n        # print(outputs)\n        # print(targets)\n        result_loss = loss(outputs, targets)  # \u8c03\u7528\u635f\u5931\u51fd\u6570\n        optim.zero_grad()\n        result_loss.backward()  # \u53cd\u5411\u4f20\u64ad\uff0c \u8fd9\u91cc\u8981\u6ce8\u610f\u4e0d\u80fd\u4f7f\u7528\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u90a3\u91cc\u7684 loss\uff0c\u800c\u8981\u4f7f\u7528 \u8c03\u7528\u635f\u5931\u51fd\u6570\u4e4b\u540e\u7684 result_loss\n        optim.step()\n        # print(\"OK\")    # \u8fd9\u90e8\u5206\uff0c\u5728debug\u4e2d\u53ef\u4ee5\u770b\u5230 grad \u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u4e4b\u540e\uff0c\u624d\u6709\u503c\uff0cdebug\u4fee\u597d\u4e86\u4e4b\u540e\uff0c\u518d\u6765\u770b\u8fd9\u91cc\n        # print(result_loss)\n        running_loss = running_loss + result_loss\n    print(running_loss)\n</code></pre> <p>\u53cd\u5411\u4f20\u64ad\u540e\uff0c\u4f18\u5316\u5668\u8fdb\u884c\u4f18\u5316\uff0c\u8bb0\u5f97\u5148\u6e05\u96f6\u68af\u5ea6\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p25%20%E7%8E%B0%E6%9C%89%E6%A8%A1%E5%9E%8B%E4%BF%AE%E6%94%B9/","title":"\u73b0\u6709\u6a21\u578b\u4fee\u6539","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p25%20%E7%8E%B0%E6%9C%89%E6%A8%A1%E5%9E%8B%E4%BF%AE%E6%94%B9/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8bb2\u4e86\u5148\u7528\u6a21\u578b\u5bfc\u5165\uff0c\u548c\u4fee\u6539\u5177\u4f53\u5c42\u3002</p> <pre><code># train_data = torchvision.datasets.ImageNet(\"../data_image_net\", split='train', download=True,\n#                                            transform=torchvision.transforms.ToTensor())\n# \u6570\u636e\u96c6\u592a\u5927\u4e86\uff0c\u4e0d\u4e0b\u8f7d\n\n#\nvgg16_false = torchvision.models.vgg16(pretrained=False)\nvgg16_true = torchvision.models.vgg16(pretrained=True)\n\n\ntrain_data = torchvision.datasets.CIFAR10('../dataset', train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\n\n# vgg16_true.add_module('add_linear',nn.Linear(1000, 10))\n# \u8981\u60f3\u7528\u4e8e CIFAR10 \u6570\u636e\u96c6\uff0c \u53ef\u4ee5\u5728\u7f51\u7edc\u4e0b\u9762\u591a\u52a0\u4e00\u884c\uff0c\u8f6c\u621010\u5206\u7c7b\u7684\u8f93\u51fa\uff0c\u8fd9\u6837\u8f93\u51fa\u7684\u7ed3\u679c\uff0c\u8ddf\u4e0b\u9762\u7684\u4e0d\u4e00\u6837\uff0c\u4f4d\u7f6e\u4e0d\u4e00\u6837\n\nvgg16_true.classifier.add_module('add_linear', nn.Linear(1000, 10))\n# \u5c42\u7ea7\u4e0d\u540c\n# \u5982\u4f55\u5229\u7528\u73b0\u6709\u7684\u7f51\u7edc\uff0c\u6539\u53d8\u7ed3\u6784\nprint(vgg16_true)\n\n# \u4e0a\u9762\u662f\u6dfb\u52a0\u5c42\uff0c\u4e0b\u9762\u662f\u5982\u4f55\u4fee\u6539VGG\u91cc\u9762\u7684\u5c42\u5185\u5bb9\nprint(vgg16_false)\nvgg16_false.classifier[6] = nn.Linear(4096, 10)  # \u4e2d\u62ec\u53f7\u91cc\u7684\u5185\u5bb9\uff0c\u662f\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c\u81ea\u5e26\u7684\u7d22\u5f15\uff0c\u5957\u8fdb\u8fd9\u79cd\u683c\u5f0f\uff0c\u5c31\u53ef\u4ee5\u76f4\u63a5\u4fee\u6539\u90a3\u4e00\u5c42\u7684\u5185\u5bb9\nprint(vgg16_false)\n\n</code></pre> <p>\u4ee3\u7801\u4e0d\u96be\uff0c\u987a\u624b\u95ee\u95eegpt\u5c31\u884c\u4e86\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p26%20%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/","title":"\u6a21\u578b\u4fdd\u5b58\u548c\u4fee\u6539","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p26%20%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/#_1","title":"\u4ecb\u7ecd","text":"<p>\u5c31\u662f\u628a\u53c2\u6570\u4fdd\u5b58\u4e0b\u6765\u3002</p> <p>\u4fdd\u5b58\u7684\u6587\u4ef6\u5b9e\u9645\u4e0a\u6ca1\u5565\u8981\u6c42\u7684\uff0c\u4f46\u662f\u5efa\u8bae\u540e\u7f00.pth</p> <p>\u4e24\u79cd\u65b9\u5f0f\u4fdd\u5b58\uff081\u3001\u5b58\u6a21\u578b+\u53c2\u6570 2\u3001\u53c2\u6570\u5b58\u4e3a\u5b57\u5178\uff09</p> <pre><code>vgg16 = torchvision.models.vgg16(pretrained=False)\n# \u4fdd\u5b58\u65b9\u5f0f1,\u6a21\u578b\u7ed3\u6784+\u6a21\u578b\u53c2\u6570   \u6a21\u578b + \u53c2\u6570 \u90fd\u4fdd\u5b58\ntorch.save(vgg16, \"vgg16_method1.pth\")  # \u5f15\u53f7\u91cc\u662f\u4fdd\u5b58\u8def\u5f84\n# \u4fdd\u5b58\u65b9\u5f0f2\uff0c\u6a21\u578b\u53c2\u6570\uff08\u5b98\u65b9\u63a8\u8350\uff09 \uff0c\u56e0\u4e3a\u8fd9\u4e2a\u65b9\u5f0f\uff0c\u50a8\u5b58\u91cf\u5c0f\uff0c\u5728terminal\u4e2d\uff0cls -all\u53ef\u4ee5\u67e5\u770b\n torch.save(vgg16.state_dict(), \"vgg16_method2.pth\")\n\n</code></pre> <p>\u5bf9\u5e94\u52a0\u8f7d\u65b9\u5f0f</p> <pre><code># \u65b9\u5f0f1\uff0c\u52a0\u8f7d\u6a21\u578b\nmodel = torch.load(\"vgg16_method1.pth\")\nprint(model)\n\n# \u65b9\u5f0f2\uff0c\u52a0\u8f7d\u6a21\u578b\nvgg16 = torchvision.models.vgg16(pretrained=False)\nvgg16.load_state_dict(torch.load(\"vgg16_method2.pth\"))\nmodel = torch.load(\"vgg16_method2.pth\")\nprint(vgg16)\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p26%20%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/#_2","title":"\u9677\u9631","text":"<p>\u5373\u65b9\u5f0f1\u9700\u8981\u58f0\u660e\u4e0b\u6a21\u578b\u5b9a\u4e49/\u5bfc\u5165\u4e0b\u6a21\u578b\u7c7b\u3002</p> <pre><code># \u9677\u9631\uff0c\u7528\u7b2c\u4e00\u79cd\u65b9\u5f0f\u4fdd\u5b58\u65f6\uff0c\u5982\u679c\u662f\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u5c31\u9700\u8981\u5728\u52a0\u8f7d\u4e2d\uff0c\u628aclass\u91cd\u65b0\u5199\u4e00\u904d\uff0c\u4f46\u5e76\u4e0d\u9700\u8981\u5b9e\u4f8b\u5316\uff0c\u5373\u53ef\n# \u8fd9\u4e2a\u9677\u9631\uff0c\u4e5f\u662f\u53ef\u4ee5\u907f\u514d\u7684\uff0c\u6700\u4e0a\u9762\u7684 from model_save import *\uff0c\u5c31\u662f\u5728\u505a\u8fd9\u4e2a\u4e8b\u60c5\uff0c\u907f\u514d\u51fa\u73b0\u9519\u8bef\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\nmodel = torch.load('tudui_method1.pth')\nprint(model)\n\n\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p27-p29%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%A5%97%E8%B7%AF/","title":"\u6a21\u578b\u8bad\u7ec3\u5957\u8def","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p27-p29%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%A5%97%E8%B7%AF/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fc7\u4e00\u904d\u6d41\u7a0b\uff0c\u4e00\u6c14\u5475\u6210\u3002</p> <p>\u9884\u5904\u7406-&gt;\u6a21\u578b-&gt;\u5b66\u4e60\u7387\u3001\u4f18\u5316\u5668\u3001\u635f\u5931\u51fd\u6570\u3001epoch\u3002 </p> <p>\u8fd9\u91cc\u7528tensorboard\u8bb0\u5f55\u635f\u5931\u503c\u8fd8\u662f\u8981\u5173\u6ce8\u4e0b\u7684\uff0c\u8fd8\u6709\u6b63\u786e\u7387\u7684\u8868\u793a\u65b9\u6cd5\u3002</p> <p>model.py</p> <pre><code>import torch\nfrom torch import nn\n\n\n# \u642d\u5efa\u795e\u7ecf\u7f51\u7edc\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(64 * 4 * 4, 64),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\nif __name__ == '__main__':\n    tudui = Tudui()\n    input = torch.ones((64, 3, 32, 32))  # \u4e3a\u4ec0\u4e48\u7528ones\uff1f\u524d\u9762\u4e5f\u662f\u7528\u7684ones\u5417\uff1f\n    output = tudui(input)\n    print(output.shape)\n\n\n</code></pre> <p>train.py</p> <pre><code>import torchvision\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom model import *\n\n# \u51c6\u5907\u6570\u636e\u96c6\ntrain_data = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\ntest_data = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n\n# length \u957f\u5ea6\ntrain_data_size = len(train_data)\ntest_data_size = len(test_data)\n\n# \u5982\u679ctrain_data_size=10, \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a10\nprint(\"\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(train_data_size))\nprint(\"\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(test_data_size))\n\n# \u5229\u7528 DataLoader \u6765\u52a0\u8f7d\u6570\u636e\u96c6\ntrain_dataloader = DataLoader(train_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n\n# \u521b\u5efa\u7f51\u7edc\u6a21\u578b\ntudui = Tudui()\n\n# \u635f\u5931\u51fd\u6570\nloss_fn = nn.CrossEntropyLoss()\n\n# \u4f18\u5316\u5668\n# learning_rate = 0.01\n# 1e-2=1 x (10)^(-2) = 1 /100 = 0.01\nlearning_rate = 1e-2\noptimizer = torch.optim.SGD(tudui.parameters(), lr=learning_rate)  # \u8fd9\u91cc\u7684\u53c2\u6570\uff0cSGD\u91cc\u9762\u7684\uff0c\u53ea\u8981\u5b9a\u4e49\u4e24\u4e2a\u53c2\u6570\uff0c\u4e00\u4e2a\u662ftudui.parameters()\u672c\u8eab\uff0c\u53e6\u4e00\u4e2a\u662flr\n\n# \u8bbe\u7f6e\u8bad\u7ec3\u7f51\u7edc\u7684\u4e00\u4e9b\u53c2\u6570\n\n# \u8bb0\u5f55\u8bad\u7ec3\u7684\u6b21\u6570\ntotal_train_step = 0\n\n# \u8bb0\u5f55\u6d4b\u8bd5\u7684\u6b21\u6570\ntotal_test_step = 0\n\n# \u8bad\u7ec3\u7684\u8f6e\u6570\nepoch = 10\n\n# \u6dfb\u52a0tensorboard\nwriter = SummaryWriter(\"logs_train\")\n\nfor i in range(epoch):\n    print(\"------------\u7b2c {} \u8f6e\u8bad\u7ec3\u5f00\u59cb------------\".format(i + 1))\n\n    # \u8bad\u7ec3\u6b65\u9aa4\u5f00\u59cb\n    tudui.train()  # \u8fd9\u4e24\u4e2a\u5c42\uff0c\u53ea\u5bf9\u4e00\u90e8\u5206\u5c42\u8d77\u4f5c\u7528\uff0c\u6bd4\u5982 dropout\u5c42\uff1b\u5982\u679c\u6709\u8fd9\u4e9b\u7279\u6b8a\u7684\u5c42\uff0c\u624d\u9700\u8981\u8c03\u7528\u8fd9\u4e2a\u8bed\u53e5\n    for data in train_dataloader:\n        imgs, targets = data\n        outputs = tudui(imgs)\n        loss = loss_fn(outputs, targets)\n\n        # \u4f18\u5316\u5668\u4f18\u5316\u6a21\u578b\n        optimizer.zero_grad()  # \u4f18\u5316\u5668\uff0c\u68af\u5ea6\u6e05\u96f6\n        loss.backward()\n        optimizer.step()\n\n        total_train_step = total_train_step + 1\n        if total_train_step % 100 == 0:\n            print(\"\u8bad\u7ec3\u6b21\u6570\uff1a{}, Loss: {}\".format(total_train_step, loss.item()))  # \u8fd9\u91cc\u7528\u5230\u7684 item()\u65b9\u6cd5\uff0c\u6709\u8bf4\u6cd5\u7684\uff0c\u5176\u5b9e\u52a0\u4e0d\u52a0\u90fd\u884c\uff0c\u5c31\u662f\u8f93\u51fa\u7684\u5f62\u5f0f\u4e0d\u4e00\u6837\u800c\u5df2\n            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)  # \u8fd9\u91cc\u662f\u4e0d\u662f\u5728\u753b\u66f2\u7ebf\uff1f\n\n    # \u6bcf\u8bad\u7ec3\u5b8c\u4e00\u8f6e\uff0c\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u4ee5\u6d4b\u8bd5\u96c6\u7684\u635f\u5931\u6216\u8005\u6b63\u786e\u7387\uff0c\u6765\u8bc4\u4f30\u6709\u6ca1\u6709\u8bad\u7ec3\u597d\uff0c\u6d4b\u8bd5\u65f6\uff0c\u5c31\u4e0d\u8981\u8c03\u4f18\u4e86\uff0c\u5c31\u662f\u4ee5\u5f53\u524d\u7684\u6a21\u578b\uff0c\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6240\u4ee5\u4e0d\u7528\u518d\u4f7f\u7528\u68af\u5ea6\uff08with no_grad \u90a3\u53e5\uff09\n\n    # \u6d4b\u8bd5\u6b65\u9aa4\u5f00\u59cb\n    tudui.eval()  # \u8fd9\u4e24\u4e2a\u5c42\uff0c\u53ea\u5bf9\u4e00\u90e8\u5206\u5c42\u8d77\u4f5c\u7528\uff0c\u6bd4\u5982 dropout\u5c42\uff1b\u5982\u679c\u6709\u8fd9\u4e9b\u7279\u6b8a\u7684\u5c42\uff0c\u624d\u9700\u8981\u8c03\u7528\u8fd9\u4e2a\u8bed\u53e5\n    total_test_loss = 0\n    total_accuracy = 0\n    with torch.no_grad():  # \u8fd9\u6837\u540e\u9762\u5c31\u6ca1\u6709\u68af\u5ea6\u4e86\uff0c  \u6d4b\u8bd5\u7684\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u9700\u8981\u66f4\u65b0\u53c2\u6570\uff0c\u6240\u4ee5\u4e0d\u9700\u8981\u68af\u5ea6\uff1f\n        for data in test_dataloader:  # \u5728\u6d4b\u8bd5\u96c6\u4e2d\uff0c\u9009\u53d6\u6570\u636e\n            imgs, targets = data\n            outputs = tudui(imgs)  # \u5206\u7c7b\u7684\u95ee\u9898\uff0c\u662f\u53ef\u4ee5\u8fd9\u6837\u7684\uff0c\u7528\u4e00\u4e2aoutput\u8fdb\u884c\u7ed8\u5236\n            loss = loss_fn(outputs, targets)\n            total_test_loss = total_test_loss + loss.item()  # \u4e3a\u4e86\u67e5\u770b\u603b\u4f53\u6570\u636e\u4e0a\u7684 loss\uff0c\u521b\u5efa\u7684 total_test_loss\uff0c\u521d\u59cb\u503c\u662f0\n            accuracy = (outputs.argmax(1) == targets).sum()  # \u6b63\u786e\u7387\uff0c\u8fd9\u662f\u5206\u7c7b\u95ee\u9898\u4e2d\uff0c\u7279\u6709\u7684\u4e00\u79cd\uff0c\u8bc4\u4ef7\u6307\u6807\uff0c\u8bed\u4e49\u5206\u5272\u4e4b\u7c7b\u7684\uff0c\u4e0d\u4e00\u5b9a\u975e\u8981\u6709\u8fd9\u4e2a\u4e1c\u897f\uff0c\u8fd9\u91cc\u662f\u5b58\u7591\u7684\uff0c\u518d\u770b\u3002\n            total_accuracy = total_accuracy + accuracy\n\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684Loss: {}\".format(total_test_loss))\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6b63\u786e\u7387: {}\".format(total_accuracy / test_data_size))  # \u5373\u4fbf\u662f\u8f93\u51fa\u4e86\u4e0a\u4e00\u884c\u7684 loss\uff0c\u4e5f\u4e0d\u80fd\u5f88\u597d\u7684\u8868\u73b0\u51fa\u6548\u679c\u3002\n    # \u5728\u5206\u7c7b\u95ee\u9898\u4e0a\u6bd4\u8f83\u7279\u6709\uff0c\u901a\u5e38\u4f7f\u7528\u6b63\u786e\u7387\u6765\u8868\u793a\u4f18\u52a3\u3002\u56e0\u4e3a\u5176\u4ed6\u95ee\u9898\uff0c\u53ef\u4ee5\u53ef\u89c6\u5316\u5730\u663e\u793a\u5728tensorbo\u4e2d\u3002\n    # \u8fd9\u91cc\u5728\uff08\u4e8c\uff09\u4e2d\uff0c\u8bb2\u4e86\u5f88\u590d\u6742\u7684\uff0c\u6ca1\u4ed4\u7ec6\u542c\u3002\u8fd9\u91cc\u5f88\u6709\u8bf4\u6cd5\uff0cargmax\uff08\uff09\u76f8\u5173\u7684\uff0c\u6709\u622a\u56fe\u5728word\u7b14\u8bb0\u4e2d\u3002\n    writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n    writer.add_scalar(\"test_accuracy\", total_accuracy / test_data_size, total_test_step)\n    total_test_step = total_test_step + 1\n\n    torch.save(tudui, \"tudui_{}.pth\".format(i))  # \u4fdd\u5b58\u65b9\u5f0f\u4e00\uff0c\u5176\u5b9e\u540e\u7f00\u90fd\u53ef\u4ee5\u81ea\u5df1\u53d6\uff0c\u4e60\u60ef\u7528 .pth\u3002\n    print(\"\u6a21\u578b\u5df2\u4fdd\u5b58\")\n\nwriter.close()\n</code></pre> <p>\u7ec6\u8282\u4e0d\u5c55\u5f00\uff0c\u81ea\u5df1\u56de\u5934\u770b\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p30-p31%20GPU/","title":"\u4f7f\u7528GPU","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p30-p31%20GPU/#_1","title":"\u4ecb\u7ecd","text":"<p>GPU\u7684\u8bad\u7ec3\u901f\u5ea6\u662f\u8fdc\u9ad8\u4e8eCPU\u7684\u3002</p> <p>\u4e24\u79cd\u65b9\u5f0f\u4ea4\u7ed9GPU\u8bad\u7ec3\uff0c\u4e00\u822c\u8fd8\u662f\u65b9\u5f0f2\u3002</p> <p>\u8981\u6539\u7684\u6709\uff1a</p> <p>\u200b   \u6a21\u578b\u3001\u635f\u5931\u51fd\u6570\u3001\u4f18\u5316\u5668\u3002   \u6570\u636e(\u9700\u8981a=a.b)  </p> <p>\u65b9\u5f0f1</p> <p>\u200b   \u9010\u4e2a.cuda()\u5f88\u9ebb\u70e6</p> <pre><code>import time\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\ntrain_data = torchvision.datasets.CIFAR10(root=\"../data\", train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\ntest_data = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n\n# length \u957f\u5ea6\ntrain_data_size = len(train_data)\ntest_data_size = len(test_data)\n# \u5982\u679ctrain_data_size=10, \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a10\nprint(\"\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(train_data_size))\nprint(\"\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(test_data_size))\n\n# \u5229\u7528 DataLoader \u6765\u52a0\u8f7d\u6570\u636e\u96c6\ntrain_dataloader = DataLoader(train_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n\n\n# \u521b\u5efa\u7f51\u7edc\u6a21\u578b\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(64 * 4 * 4, 64),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\ntudui = Tudui()\nif torch.cuda.is_available():\n    tudui = tudui.cuda()  # \u8fd9\u662fGPU\u52a0\u901f\u8bad\u7ec3\u7684\u7b2c\u4e00\u90e8\u5206\n\n# \u635f\u5931\u51fd\u6570\nloss_fn = nn.CrossEntropyLoss()\nif torch.cuda.is_available():\n    loss_fn = loss_fn.cuda()  # \u8fd9\u662fGPU\u52a0\u901f\u8bad\u7ec3\u7684\u7b2c\u4e8c\u90e8\u5206\n# \u4f18\u5316\u5668\n# learning_rate = 0.01\n# 1e-2=1 x (10)^(-2) = 1 /100 = 0.01\nlearning_rate = 1e-2\noptimizer = torch.optim.SGD(tudui.parameters(), lr=learning_rate)\n\n# \u8bbe\u7f6e\u8bad\u7ec3\u7f51\u7edc\u7684\u4e00\u4e9b\u53c2\u6570\n# \u8bb0\u5f55\u8bad\u7ec3\u7684\u6b21\u6570\ntotal_train_step = 0\n# \u8bb0\u5f55\u6d4b\u8bd5\u7684\u6b21\u6570\ntotal_test_step = 0\n# \u8bad\u7ec3\u7684\u8f6e\u6570\nepoch = 10\n\n# \u6dfb\u52a0tensorboard\nwriter = SummaryWriter(\"logs_train\")\n\n# \u6dfb\u52a0\u5f00\u59cb\u65f6\u95f4\n\nstart_time = time.time()\n\nfor i in range(epoch):\n    print(\"-------\u7b2c {} \u8f6e\u8bad\u7ec3\u5f00\u59cb-------\".format(i + 1))\n\n    # \u8bad\u7ec3\u6b65\u9aa4\u5f00\u59cb\n    tudui.train()\n    for data in train_dataloader:\n        imgs, targets = data\n        if torch.cuda.is_available():\n            imgs = imgs.cuda()  # \u8fd9\u4e24\u884c\u662fGPU\u52a0\u901f\u7684\u7b2c\u4e09\u90e8\u5206\uff08\u672a\u5b8c\uff09\n            targets = targets.cuda()\n        outputs = tudui(imgs)\n        loss = loss_fn(outputs, targets)\n\n        # \u4f18\u5316\u5668\u4f18\u5316\u6a21\u578b\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_train_step = total_train_step + 1\n        if total_train_step % 100 == 0:\n            end_time = time.time()   # \u7ed3\u675f\u65f6\u95f4\n            print(end_time - start_time)\n\n            print(\"\u8bad\u7ec3\u6b21\u6570\uff1a{}, Loss: {}\".format(total_train_step, loss.item()))\n            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n\n    # \u6d4b\u8bd5\u6b65\u9aa4\u5f00\u59cb\n    tudui.eval()\n    total_test_loss = 0\n    total_accuracy = 0\n    with torch.no_grad():\n        for data in test_dataloader:\n            imgs, targets = data\n            if torch.cuda.is_available():  # \u8fd9\u4e24\u884c\u4e5f\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\uff0cGPU\u52a0\u901f\u8bad\u7ec3\u7684\u90e8\u5206\n                imgs = imgs.cuda()\n                targets = targets.cuda()\n            outputs = tudui(imgs)\n            loss = loss_fn(outputs, targets)\n            total_test_loss = total_test_loss + loss.item()\n            accuracy = (outputs.argmax(1) == targets).sum()\n            total_accuracy = total_accuracy + accuracy\n\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684 Loss: {}\".format(total_test_loss))\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6b63\u786e\u7387: {}\".format(total_accuracy / test_data_size))\n    writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n    writer.add_scalar(\"test_accuracy\", total_accuracy / test_data_size, total_test_step)\n    total_test_step = total_test_step + 1\n\n    torch.save(tudui, \"tudui_{}.pth\".format(i))\n    print(\"\u6a21\u578b\u5df2\u4fdd\u5b58\")\n\nwriter.close()\n\n'''\n\n\u7f51\u7edc\u6a21\u578b\u3001\u635f\u5931\u51fd\u6570\u3001\u6570\u636e\uff08\u8f93\u5165\u3001\u6807\u6ce8\uff09\n\n\u8c03\u7528 .cuda\n\n\u4ee5\u4e0a\u4e09\u8005\u6709cuda\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0 \n'''\n\n\n</code></pre> <p>\u65b9\u5f0f2</p> <p>\u200b   \u5148\u5b9a\u4e49device \u518dtodevice()</p> <pre><code>import time\nimport torch\nimport torchvision\n# \u51c6\u5907\u6570\u636e\u96c6\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\n# \u5b9a\u4e49\u8bad\u7ec3\u7684\u8bbe\u5907\ndevice = torch.device(\"cuda\")  # \u5b9a\u4e49\u8bad\u7ec3\u7684\u8bbe\u5907\nprint(device)\n\ntrain_data = torchvision.datasets.CIFAR10(root=\"../data\", train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\ntest_data = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n\n# length \u957f\u5ea6\ntrain_data_size = len(train_data)\ntest_data_size = len(test_data)\n# \u5982\u679ctrain_data_size=10, \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a10\nprint(\"\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(train_data_size))\nprint(\"\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(test_data_size))\n\n# \u5229\u7528 DataLoader \u6765\u52a0\u8f7d\u6570\u636e\u96c6\ntrain_dataloader = DataLoader(train_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n\n\n# \u521b\u5efa\u7f51\u7edc\u6a21\u578b\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(64 * 4 * 4, 64),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\ntudui = Tudui()\ntudui.to(device)  # \u8fd9\u91cc\u65b0\u6dfb\u52a0\u4e86gpu\u52a0\u901f\u7684\u5185\u5bb9    \u8fd9\u91cc\uff0c\u5176\u5b9e\u4e0d\u7528\u53e6\u5916\u8d4b\u503c tudui = xxx\uff0c\u76f4\u63a5\u8c03\u7528 tudui.to(device)\u5c31\u53ef\u4ee5\u7684\n\n# \u635f\u5931\u51fd\u6570\nloss_fn = nn.CrossEntropyLoss()\nloss_fn.to(device)  # \u8fd9\u91cc\u6dfb\u52a0\u4e86\u52a0\u901f\u8bbe\u5907\uff0c\u5176\u5b9e\u4e5f\u662f\u4e0d\u9700\u8981\u91cd\u65b0\u8d4b\u503c\u7684\uff0c\u76f4\u63a5\u8c03\u7528\u5c31\u53ef\u4ee5\u4e86\n# \u4f18\u5316\u5668\n# learning_rate = 0.01\n# 1e-2=1 x (10)^(-2) = 1 /100 = 0.01\nlearning_rate = 1e-2\noptimizer = torch.optim.SGD(tudui.parameters(), lr=learning_rate)\n\n# \u8bbe\u7f6e\u8bad\u7ec3\u7f51\u7edc\u7684\u4e00\u4e9b\u53c2\u6570\n# \u8bb0\u5f55\u8bad\u7ec3\u7684\u6b21\u6570\ntotal_train_step = 0\n# \u8bb0\u5f55\u6d4b\u8bd5\u7684\u6b21\u6570\ntotal_test_step = 0\n# \u8bad\u7ec3\u7684\u8f6e\u6570\nepoch = 10\n\n# \u6dfb\u52a0tensorboard\nwriter = SummaryWriter(\"logs_train\")\n\nstart_time = time.time()\nfor i in range(epoch):\n    print(\"-------\u7b2c {} \u8f6e\u8bad\u7ec3\u5f00\u59cb-------\".format(i + 1))\n\n    # \u8bad\u7ec3\u6b65\u9aa4\u5f00\u59cb\n    tudui.train()\n    for data in train_dataloader:\n        imgs, targets = data\n        imgs = imgs.to(device)  # \u8fd9\u91cc\u662f\u6570\u636e\uff0c\u9700\u8981\u91cd\u65b0\u8d4b\u503c\n        targets = targets.to(device)  # \u8fd9\u91cc\u4e00\u6837\n        outputs = tudui(imgs)\n        loss = loss_fn(outputs, targets)\n\n        # \u4f18\u5316\u5668\u4f18\u5316\u6a21\u578b\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_train_step = total_train_step + 1\n        if total_train_step % 100 == 0:\n            end_time = time.time()\n            print(end_time - start_time)\n            print(\"\u8bad\u7ec3\u6b21\u6570\uff1a{}, Loss: {}\".format(total_train_step, loss.item()))\n            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n\n    # \u6d4b\u8bd5\u6b65\u9aa4\u5f00\u59cb\n    tudui.eval()\n    total_test_loss = 0\n    total_accuracy = 0\n    with torch.no_grad():\n        for data in test_dataloader:\n            imgs, targets = data\n            imgs = imgs.to(device)\n            targets = targets.to(device)\n            outputs = tudui(imgs)\n            loss = loss_fn(outputs, targets)\n            total_test_loss = total_test_loss + loss.item()\n            accuracy = (outputs.argmax(1) == targets).sum()\n            total_accuracy = total_accuracy + accuracy\n\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684Loss: {}\".format(total_test_loss))\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6b63\u786e\u7387: {}\".format(total_accuracy / test_data_size))\n    writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n    writer.add_scalar(\"test_accuracy\", total_accuracy / test_data_size, total_test_step)\n    total_test_step = total_test_step + 1\n\n    torch.save(tudui, \"tudui_{}.pth\".format(i))\n    print(\"\u6a21\u578b\u5df2\u4fdd\u5b58\")\n\nwriter.close()\n\n'''\n\nGPU\u52a0\u901f\u7684\u7b2c\u4e8c\u4e2a\u65b9\u6cd5\n\n.to(device)\n\ndevice = torch.device(\u201ccpu\u201d)\n\ntorch.device\uff08\u201ccuda\u201d\uff09\u4e5f\u53ef\u4ee5\n\ntorch.device\uff08\u201ccuda\uff1a0\u201d\uff09\u6709\u591a\u4e2a\u663e\u5361\u65f6\u4f7f\u7528\n\n'''\n\n\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p32%20%E9%AA%8C%E8%AF%81%E5%A5%97%E8%B7%AF/","title":"\u6a21\u578b\u9a8c\u8bc1","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p32%20%E9%AA%8C%E8%AF%81%E5%A5%97%E8%B7%AF/#_1","title":"\u4ecb\u7ecd","text":"<p>\u4e0d\u60f3\u591a\u8bf4\u3002</p> <p>\u200b   \u6ce8\u610f\u4e0b torch.no_grad()\u548cmodel.eval()\u5c31\u884c\u4e86\u3002</p> <p>torch.no_grad()\u7981\u7528\u68af\u5ea6\u64cd\u4f5c\uff0c\u8282\u7ea6\u6027\u80fd\u3002</p> <p>model.eval()\u4e0d\u53ea\u662f\u4e0d\u53cd\u5411\u4f20\u64ad\uff0c\u91cc\u9762\u67d0\u4e9b\u5c42\uff08\u5982dropout)\u662f\u4e0d\u751f\u6548\u7684\u3002</p> <pre><code>image_path = \"../imgs/airplane.png\"\n# image_path = \"TuDui/imgs/airplane.png\"   # \u590d\u5236\u76f8\u5bf9\u8def\u5f84\uff0c\u5c31\u662f\u5bf9\u7684\u4e86\nimage = Image.open(image_path)  # PIL\u7c7b\u578b\u7684\u56fe\u7247\nprint(image)\nimage = image.convert('RGB')  # \u8fd9\u91cc\u5728word\u4e2d\uff0c\u6709\u622a\u56fe\uff0c\u662f\u8ddfpng\u7684\u901a\u9053\u6570\u6709\u5173\u7cfb\u7684\n\n# \u56fe\u50cf\u5927\u5c0f\uff0c\u53ea\u80fd\u662f\u6a21\u578b\u4e2d\u768432\uff0c32\uff0c\u7136\u540e\u8f6c\u4e3a totensor \u6570\u636e\u7c7b\u578b\ntransform = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),\n                                            torchvision.transforms.ToTensor()])\n\nimage = transform(image)  # \u5e94\u7528 transform\nprint(image.shape)  # \u6253\u5370\u56fe\u50cf\u5927\u5c0f\n\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(64 * 4 * 4, 64),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\nmodel = torch.load(\"tudui_0.pth\", map_location=torch.device('cpu'))  # \u52a0\u8f7d\u8bad\u7ec3\u6a21\u578b\nprint(model)\nimage = torch.reshape(image, (1, 3, 32, 32))\nmodel.eval()\nwith torch.no_grad():  # \u8fd9\u6b65\u53ef\u4ee5\u8282\u7ea6\u5185\u5b58\uff0c\u63d0\u9ad8\u6027\u80fd\n    output = model(image)\nprint(output)\n\nprint(output.argmax(1))\n\n\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p4%20dir%E5%92%8Chelp%E5%87%BD%E6%95%B0/","title":"dir \u548c help \u51fd\u6570","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p4%20dir%E5%92%8Chelp%E5%87%BD%E6%95%B0/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91cc\u628apytorch\u770b\u51fa\u4e00\u4e2a\u5de5\u5177\u7bb1\uff0c\u6bcf\u4e2a\u5de5\u5177\u7bb1\u91cc\u9762\u6709\u5c0f\u7684\u5de5\u5177\u5305\uff0c\u4e00\u76f4\u5230\u6700\u5c0f\u7684\u76f4\u63a5\u4f7f\u7528\u7684\u5de5\u5177\u3002</p> <p>dir()  \u6253\u5f00\u4e0e\u67e5\u770b</p> <p>help() \u6559\u4f60\u4f7f\u7528</p> <p>\u4f8b\u5982\uff1a</p> <p>\u200b       dir(pytorch)-&gt;\u8f93\u51fa1\uff0c2\uff0c3\uff0c4</p> <p>\u200b       dir(pytorch.3)-&gt;\u8f93\u51faa,b,c</p> <p>\u800chelp\u5219\u662f\u5177\u4f53\u4f7f\u7528</p> <p>\u4f8b\u5982\uff1a</p> <p>\u200b       help(pytorch.3.a)-&gt;\u8f93\u51fa\uff1a\u87ba\u4e1d\u5200\u7684\u4f7f\u7528\u65b9\u6cd5\u3002</p> <p>vscode\u81ea\u5df1\u5b9e\u64cd\uff1a</p> <pre><code>import torch\nprint(\"\u67e5\u770btorch\u7684\u5de5\u5177\u5305\\n\",dir(torch))\nprint(\"\u67e5\u770btorch.cuda\u7684\u5de5\u5177\u5305\\n\",dir(torch.cuda))\nprint(\"\u67e5\u770btorch.cuda.is_available\u7684\u5de5\u5177\u5305\\n\",dir(torch.cuda.is_available))\nprint(\"\u67e5\u770bis_available\u65b9\u6cd5\u7684\u4f7f\u7528\\n\",help(torch.cuda.is_available))\n</code></pre> <p>\u7ed3\u679c\u5982\u4e0b\uff1a</p> <p>\u67e5\u770btorch\u7684\u5de5\u5177\u5305  ['AVG', 'AggregationType', 'AliasDb', 'Any', 'AnyType', 'Argument', 'ArgumentSpec', 'AwaitType', 'BFloat16Storage', 'BFloat16Tensor', 'BenchmarkConfig', 'preload_cuda_deps', '_prelu_kernel', '_prims', '_prims_common', '_propagate_xla_data', '_refs', '_register_device_module', '_remove_batch_dim', '_reshape_alias_copy', '_reshape_from_tensor', '_resize_output', 'rowwise_prune', '_running_with_deploy', '_sample_dirichlet', '_saturate_weight_to_fp16', '_scaled_dot_product_attention_math', '_scaled_dot_product_efficient_attention', '_scaled_dot_product_flash_attention', '_scaled_mm', '_segment_reduce', '_shape_as_tensor', '_sobol_engine_draw', '_sobol_engine_ff', 'sobol_engine_initialize_state', 'sobol_engine_scramble', 'softmax', '_softmax_backward_data', '_sources', '_sparse_broadcast_to', '_sparse_broadcast_to_copy', '_sparse_coo_tensor_unsafe', '_sparse_csr_prod', '_sparse_csr_sum', '_sparse_log_softmax_backward_data', '_sparse_semi_structured_linear', '_sparse_softmax_backward_data', '_sparse_sparse_matmul', '_sparse_sum', '_stack', '_standard_gamma', '_standard_gamma_grad', '_storage_classes', '_subclasses', '_sync', '_tensor', '_tensor_classes', '_tensor_str', '_test_autograd_multiple_dispatch', '_test_autograd_multiple_dispatch_view', '_test_autograd_multiple_dispatch_view_copy', '_test_check_tensor', '_test_functorch_fallback', '_test_serialization_subcmul', '_to_cpu', '_to_functional_tensor', '_to_sparse_semi_structured', '_transform_bias_rescale_qkv', '_transformer_encoder_layer_fwd', '_trilinear', '_triton_multi_head_attention', '_triton_scaled_dot_attention', '_unique', '_unique2', '_unpack_dual', '_unsafe_index', '_unsafe_index_put', '_use_cudnn_ctc_loss', '_use_cudnn_rnn_flatten_weight', '_utils', '_utils_internal', '_validate_compressed_sparse_indices', '_validate_sparse_bsc_tensor_args', '_validate_sparse_bsr_tensor_args', '_validate_sparse_compressed_tensor_args', '_validate_sparse_coo_tensor_args', '_validate_sparse_csc_tensor_args', '_validate_sparse_csr_tensor_args', '_values_copy', '_vmap_internals', '_warn_typed_storage_removal', '_weight_norm', '_weight_norm_interface', '_weights_only_unpickler', 'abs', 'abs', 'absolute', 'acos', 'acos_', 'acosh', 'acosh_', 'adaptive_avg_pool1d', 'adaptive_max_pool1d', 'add', 'addbmm', 'addcdiv', 'addcmul', 'addmm', 'addmv', 'addmv_', 'addr', 'adjoint', 'affine_grid_generator', 'alias_copy', 'align_tensors', 'all', 'allclose', 'alpha_dropout', 'alpha_dropout_', 'amax', 'amin', 'aminmax', 'amp', 'angle', 'any', 'ao', 'arange', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan2', 'arctan_', 'arctanh', 'arctanh_', 'are_deterministic_algorithms_enabled', 'argmax', 'argmin', 'argsort', 'argwhere', 'as_strided', 'as_strided_', 'as_strided_copy', 'as_strided_scatter', 'as_tensor', 'asarray', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan_', 'atanh', 'atanh_', 'atleast_1d', 'atleast_2d', 'atleast_3d', 'attr', 'autocast', 'autocast_decrement_nesting', 'autocast_increment_nesting', 'autograd', 'avg_pool1d', 'backends', 'baddbmm', 'bartlett_window', 'base_py_dll_path', 'batch_norm', 'batch_norm_backward_elemt', 'batch_norm_backward_reduce', 'batch_norm_elemt', 'batch_norm_gather_stats', 'batch_norm_gather_stats_with_counts', 'batch_norm_stats', 'batch_norm_update_stats', 'bernoulli', 'bfloat16', 'bilinear', 'binary_cross_entropy_with_logits', 'bincount', 'binomial', 'bits16', 'bits1x8', 'bits2x4', 'bits4x2', 'bits8', 'bitwise_and', 'bitwise_left_shift', 'bitwise_not', 'bitwise_or', 'bitwise_right_shift', 'bitwise_xor', 'blackman_window', 'block_diag', 'bmm', 'bool', 'broadcast_shapes', 'broadcast_tensors', 'broadcast_to', 'bucketize', 'builtins', 'can_cast', 'candidate', 'cartesian_prod', 'cat', 'ccol_indices_copy', 'cdist', 'cdouble', 'ceil', 'ceil_', 'celu', 'celu_', 'cfloat', 'chain_matmul', 'chalf', 'channel_shuffle', 'channels_last', 'channels_last_3d', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'choose_qparams_optimized', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'classes', 'classproperty', 'clear_autocast_cache', 'clip', 'clip_', 'clone', 'col_indices_copy', 'column_stack', 'combinations', 'compile', 'compiled_with_cxx11_abi', 'compiler', 'complex', 'complex128', 'complex32', 'complex64', 'concat', 'concatenate', 'conj', 'conj_physical', 'conj_physical_', 'constant_pad_nd', 'contiguous_format', 'conv1d', 'conv2d', 'conv3d', 'conv_tbc', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d', 'convolution', 'copysign', 'corrcoef', 'cos', 'cos_', 'cosh', 'cosh_', 'cosine_embedding_loss', 'cosine_similarity', 'count_nonzero', 'cov', 'cpp', 'cpu', 'cross', 'crow_indices_copy', 'ctc_loss', 'ctypes', 'cuda', 'cuda_path', 'cuda_version', 'cudnn_affine_grid_generator', 'cudnn_batch_norm', 'cudnn_convolution', 'cudnn_convolution_add_relu', 'cudnn_convolution_relu', 'cudnn_convolution_transpose', 'cudnn_grid_sampler', 'cudnn_is_acceptable', 'cummax', 'cummin', 'cumprod', 'cumsum', 'cumulative_trapezoid', 'default_generator', 'deg2rad', 'deg2rad_', 'dequantize', 'det', 'detach', 'detach_', 'detach_copy', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'diagonal_copy', 'diagonal_scatter', 'diff', 'digamma', 'dist', 'distributed', 'distributions', 'div', 'divide', 'dll', 'dll_path', 'dll_paths', 'dlls', 'dot', 'double', 'dropout', 'dropout_', 'dsmm', 'dsplit', 'dstack', 'dtype', 'e', 'eig', 'einsum', 'embedding', 'embedding_bag', 'embedding_renorm_', 'empty', 'empty_like', 'empty_permuted', 'empty_quantized', 'empty_strided', 'enable_grad', 'eq', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'exp', 'exp2', 'exp2_', 'exp_', 'expand_copy', 'expm1', 'expm1_', 'export', 'eye', 'fake_quantize_per_channel_affine', 'fake_quantize_per_tensor_affine', 'fbgemm_linear_fp16_weight', 'fbgemm_linear_fp16_weight_fp32_activation', 'fbgemm_linear_int8_weight', 'fbgemm_linear_int8_weight_fp32_activation', 'fbgemm_linear_quantize_weight', 'fbgemm_pack_gemm_matrix_fp16', 'fbgemm_pack_quantized_matrix', 'feature_alpha_dropout', 'feature_alpha_dropout_', 'feature_dropout', 'feature_dropout_', 'fft', 'fill', 'fill_', 'finfo', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'float16', 'float32', 'float64', 'float8_e4m3fn', 'float8_e5m2', 'float_power', 'floor', 'floor_', 'floor_divide', 'fmax', 'fmin', 'fmod', 'fork', 'frac', 'frac_', 'frexp', 'frobenius_norm', 'from_dlpack', 'from_file', 'from_numpy', 'frombuffer', 'full', 'full_like', 'func', 'functional', 'fused_moving_avg_obs_fake_quant', 'futures', 'fx', 'gather', 'gcd', 'gcd_', 'ge', 'geqrf', 'ger', 'get_autocast_cpu_dtype', 'get_autocast_gpu_dtype', 'get_autocast_ipu_dtype', 'get_autocast_xla_dtype', 'get_default_dtype', 'get_deterministic_debug_mode', 'get_device', 'get_file_path', 'get_float32_matmul_precision', 'get_num_interop_threads', 'get_num_threads', 'get_rng_state', 'glob', 'gradient', 'greater', 'greater_equal', 'grid_sampler', 'grid_sampler_2d', 'grid_sampler_3d', 'group_norm', 'gru', 'gru_cell', 'gt', 'half', 'hamming_window', 'hann_window', 'hardshrink', 'has_lapack', 'has_mkl', 'has_openmp', 'has_spectral', 'heaviside', 'hinge_embedding_loss', 'histc', 'histogram', 'histogramdd', 'hsmm', 'hsplit', 'hspmm', 'hstack', 'hub', 'hypot', 'i0', 'i0_', 'igamma', 'igammac', 'iinfo', 'imag', 'import_ir_module', 'import_ir_module_from_buffer', 'index_add', 'index_copy', 'index_fill', 'index_put', 'index_put_', 'index_reduce', 'index_select', 'indices_copy', 'inf', 'inference_mode', 'init_num_threads', 'initial_seed', 'inner', 'inspect', 'instance_norm', 'int', 'int16', 'int32', 'int64', 'int8', 'int_repr', 'inverse', 'is_anomaly_check_nan_enabled', 'is_anomaly_enabled', 'is_autocast_cache_enabled', 'is_autocast_cpu_enabled', 'is_autocast_enabled', 'is_autocast_ipu_enabled', 'is_autocast_xla_enabled', 'is_complex', 'is_conj', 'is_deterministic_algorithms_warn_only_enabled', 'is_distributed', 'is_floating_point', 'is_grad_enabled', 'is_inference', 'is_inference_mode_enabled', 'is_loaded', 'is_neg', 'is_nonzero', 'is_same_size', 'is_signed', 'is_storage', 'is_tensor', 'is_vulkan_available', 'is_warn_always_enabled', 'isclose', 'isfinite', 'isin', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'jit', 'kaiser_window', 'kernel32', 'kl_div', 'kron', 'kthvalue', 'last_error', 'layer_norm', 'layout', 'lcm', 'lcm_', 'ldexp', 'ldexp_', 'le', 'legacy_contiguous_format', 'lerp', 'less', 'less_equal', 'lgamma', 'library', 'linalg', 'linspace', 'load', 'lobpcg', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_not', 'logical_or', 'logical_xor', 'logit', 'logit_', 'logspace', 'logsumexp', 'long', 'lstm', 'lstm_cell', 'lstsq', 'lt', 'lu', 'lu_solve', 'lu_unpack', 'manual_seed', 'margin_ranking_loss', 'masked',  'masked_fill', 'masked_scatter', 'masked_select', 'math', 'matmul', 'matrix_exp', 'matrix_power', 'matrix_rank', 'max', 'max_pool1d', 'max_pool1d_with_indices', 'max_pool2d', 'max_pool3d', 'maximum', 'mean', 'median', 'memory_format', 'merge_type_from_type_comment', 'meshgrid', 'min', 'minimum', 'miopen_batch_norm', 'miopen_convolution', 'miopen_convolution_add_relu', 'miopen_convolution_relu', 'miopen_convolution_transpose', 'miopen_depthwise_convolution', 'miopen_rnn', 'mkldnn_adaptive_avg_pool2d', 'mkldnn_convolution', 'mkldnn_linear_backward_weights', 'mkldnn_max_pool2d', 'mkldnn_max_pool3d', 'mkldnn_rnn_layer', 'mm', 'mode', 'moveaxis', 'movedim', 'mps', 'msort', 'mul', 'multinomial', 'multiply', 'multiprocessing', 'mv', 'mvlgamma', 'name', 'nan', 'nan_to_num', 'nan_to_num_', 'zeros_like'] \u67e5\u770btorch.cuda\u7684\u5de5\u5177\u5305  ['Any', 'BFloat16Storage', 'BFloat16Tensor', 'BoolStorage', ......,  'warnings'] \u67e5\u770btorch.cuda.is_available\u7684\u5de5\u5177\u5305  ['annotations', 'call', 'class', 'closure', 'code', 'defaults', 'delattr', 'dict', 'dir', 'doc', 'eq', 'format', 'ge', 'get', 'getattribute', 'globals', 'gt', 'hash', 'init', 'init_subclass', 'kwdefaults', 'le', 'lt', 'module', 'name', 'ne', 'new', 'qualname', 'reduce', 'reduce_ex', 'repr', 'setattr', 'sizeof', 'str', 'subclasshook'] Help on function is_available in module torch.cuda:</p> <p>is_available() -&gt; bool     Returns a bool indicating if CUDA is currently available.</p> <p>\u67e5\u770bis_available\u65b9\u6cd5\u7684\u4f7f\u7528  None</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p6-p7%20Dataset/","title":"Dataset","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p6-p7%20Dataset/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91cc\u7528\u5783\u573e\u5206\u7c7b\u6765\u5904\u7406\u3002</p> <p>Dataset: \u63d0\u4f9b\u4e00\u79cd\u65b9\u6cd5\u628a\u6570\u636e\u52a0\u8f7d\u8fdb\u6765\uff0c\u63d0\u4f9b\u4e24\u4e2a\u65b9\u6cd5\uff0c\u5206\u522b\u5b9e\u73b0\u83b7\u53d6\u6570\u636e\u548clabel\u3001\u83b7\u53d6\u6570\u636e\u603b\u91cf\u7684\u529f\u80fd\u3002</p> <p>Dataloader: \u628a\u6570\u636e\u6253\u5305\u4ea4\u7ed9\u6a21\u578b\u8bad\u7ec3\u3001\u5177\u4f53\u540e\u9762\u518d\u63d0\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p6-p7%20Dataset/#_2","title":"\u4ee3\u7801\u5b9e\u6218","text":"<p>\u8fd9\u91cc\u7528\u4e86\u4e00\u4e2a\u4e8c\u5206\u7c7b\u6570\u636e\u96c6\uff1aants\u548cbees</p> <pre><code>from torch.utils.data import Dataset\nfrom PIL import Image\nimport os\n\nclass MyData(Dataset):\n  def __init__(self,root_dir,label_dir):\n    self.root_dir = root_dir\n    self.label_dir = label_dir\n    self.path = os.path.join(self.root_dir,self.label_dir)\n    self.img_path = os.listdir(self.path)\n\n  def __getitem__(self, idx): # \u83b7\u53d6\u6307\u5b9a\u7d22\u5f15\u7684\u6837\u672c\n    img_name = self.img_path[idx]\n    img_item_path = os.path.join(self.root_dir,self.label_dir,img_name)\n    img = Image.open(img_item_path)\n    label = self.label_dir\n    return img,label\n\n  def __len__(self):  # \u83b7\u53d6\u6837\u672c\u603b\u6570\n    return len(self.img_path)\n\n\nif __name__ == '__main__':\n    root_dir = \"data\\\\train\"\n    ants_label_dir = \"ants\"\n    bees_label_dir = \"bees\"\n    ants_dataset = MyData(root_dir, ants_label_dir)\n    bees_dataset = MyData(root_dir, bees_label_dir)\n    train_dataset = ants_dataset + bees_dataset\n    print(len(train_dataset))\n    img,label = train_dataset[200]\n    img.show()\n\n</code></pre> <p>\u200b \u5305\u542b\u56fe\u7247\u53ca\u5176label\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p8-p9%20TensorBoard/","title":"TensorBoard","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p8-p9%20TensorBoard/#_1","title":"\u4ecb\u7ecd","text":"<p>TensorBoard\u53ef\u4ee5\u8ba9\u6211\u4eec\u67e5\u770b\u5177\u4f53\u67d0\u4e00\u6b65\u9aa4\u7684\u60c5\u51b5\uff0c\u5982\u4e0a\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p8-p9%20TensorBoard/#_2","title":"\u4ee3\u7801","text":"<pre><code>from torch.utils.tensorboard import SummaryWriter\n# \u4e00\u4e2a\u7c7b\uff0c\u5f80\u4e8b\u4ef6\u6587\u4ef6\u5939\u91cc\u5199\u4e1c\u897f\n\nwriter = SummaryWriter(\"logs\")\n\nfor i in range(100):\n  writer.add_scalar(\"y=x\",i,i)\n\nwriter.close()\n\n</code></pre> <p>\u8fd9\u91cc\u7684add_scalar\u662f\u5f80\u91cc\u9762\u6dfb\u52a0\u4e00\u4e2a\u6807\u91cf\u7684\u65b9\u6cd5\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p8-p9%20TensorBoard/#_3","title":"\u5177\u4f53\u4f7f\u7528","text":"<p>\u5728\u7ec8\u7aef\u4e2d\u8f93\u5165\uff1atensorboard --logdir=logs --port=6007</p> <p>--logdir\u6307\u5b9a\u6587\u4ef6\u5939  port\u6307\u5b9a\u7aef\u53e3(\u51cf\u5c11\u548c\u522b\u4eba\u7684\u51b2\u7a81)</p> <p>TensorBoard 2.10.0 at http://localhost:6007/ (Press CTRL+C to quit)</p> <p>\u8bbf\u95ee\u8fd9\u4e2a\u5730\u5740\uff1a</p> <p></p> <p>\u5982\u56fe\uff0c\u5177\u4f53\u540e\u7eed\u64cd\u4f5c\u53ef\u4ee5\u81ea\u5df1\u8bd5\u4e00\u4e0b\u3002</p> <p>\u4ee3\u7801\u4e2d\uff1a</p> <pre><code>for i in range(100):\n  writer.add_scalar(\"y=3x\",3*i,i)\n</code></pre> <p>\u524d\u9762\u6807\u7b7e\u4e0d\u6539\u7684\u8bdd\uff0c\u4f1a\u628a\u591a\u4e2a\u6587\u4ef6\u653e\u5728\u4e00\u4e2a\u56fe\u91cc\uff0c\u4f1a\u5f88\u4e71\uff0c\u6240\u4ee5\u8bb0\u5f97\u6539\u6807\u7b7e/\u6362\u4e2a\u6587\u4ef6\u5939\u5b58\u653e\u4e8b\u4ef6\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p8-p9%20TensorBoard/#add_image","title":"add_image\u76f8\u5173\u64cd\u4f5c","text":"<pre><code>from torch.utils.tensorboard import SummaryWriter\n# \u4e00\u4e2a\u7c7b\uff0c\u5f80\u4e8b\u4ef6\u6587\u4ef6\u5939\u91cc\u5199\u4e1c\u897f\n\nimport numpy as np\nfrom PIL import Image\n\nwriter = SummaryWriter(\"logs\")\nimage_path = \"data\\\\train\\\\bees\\\\16838648_415acd9e3f.jpg\"  # \u76f8\u5bf9\u8def\u5f84\nimg_PIL = Image.open(image_path) # PIL\u683c\u5f0f\nimg_array = np.array(img_PIL) # \u8f6c\u4e3anumpy\u683c\u5f0f\nprint(img_array.shape)\n\nwriter.add_image(\"test\",img_array,2,dataformats='HWC')\n\n# for i in range(100):\n#   writer.add_scalar(\"y=x\",i,i)\n\nwriter.close()\n\n</code></pre> <p>\u8fd9\u91ccadd_image\u8981\u6ce8\u610f\u7684\u662f\u56fe\u7247\u7684\u683c\u5f0f\uff0c\u8fd9\u91cc\u56fe\u7247\u683c\u5f0f\u662f(height,width,channel)\uff0c\u8981\u5728\u53c2\u6570\u4e2d\u6307\u51fa\u3002</p> <p>\u7136\u540e\u7b2c\u4e09\u4e2a\u53c2\u6570\u662fstep,\u5373\u7b2cn\u6b65\uff0c\u6211\u4eec\u53ef\u4ee5\u62d6\u52a8\u67e5\u770b\u7b2cn\u6b65\u7684\u56fe\u7247(\u5373\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165)\u3002</p> <p>\u5982\u56fe\uff1a </p>"},{"location":"other/IELS-Stand/","title":"IELS \u9898\u578b","text":""},{"location":"other/IELS-Stand/#_1","title":"\u5199\u4f5c","text":"<p>t1 count &gt;= 150 ( 20 min )</p> <p>t2 count &gt;= 250 (270words 40 min)</p> <p>1.Discuss oth Views 2.AG-DG 3...</p>"},{"location":"projects/KnowledgeBrain/Intro/","title":"Intro","text":"<p>\u9879\u76ee\u5b9e\u62181\uff1a\u667a\u80fd\u5ea7\u8231\u6c7d\u8f66\u77e5\u8bc6\u5927\u8111 \u8be5\u9879\u76ee\u5c5e\u4e8e\u6700\u8fd1\u5f88\u706b\u7684\u5927\u6a21\u578bRAG\u4efb\u52a1\uff0c\u4f7f\u7528\u73b0\u6709\u7684\u8f66\u4e3b\u624b\u518c\u6784\u5efa\u77e5\u8bc6\u5e93\uff0c\u7136\u540e\u9009\u62e9\u77e5\u8bc6\u5e93\u4e2d\u7684\u76f8\u5173\u77e5\u8bc6\u7528\u4e8e\u8f85\u52a9\u5927\u6a21\u578b\u751f\u6210\u3002\u6574\u4e2a\u65b9\u6848\u7684\u6784\u5efa\u6d41\u7a0b\u4e3b\u8981\u5206\u4e3a3\u5927\u90e8\u5206\uff1a\u6784\u5efa\u77e5\u8bc6\u5e93\u3001\u77e5\u8bc6\u68c0\u7d22\u3001\u7b54\u6848\u751f\u6210\u3002 1\u3001\u4ee3\u7801\u7ed3\u6784 . \u251c\u2500\u2500 Dockerfile # \u955c\u50cf\u6587\u4ef6 \u251c\u2500\u2500 README.md # \u8bf4\u660e\u6587\u6863 \u251c\u2500\u2500 bm25_retriever.py # BM25\u53ec\u56de \u251c\u2500\u2500 build.sh # \u955c\u50cf\u7f16\u8bd1\u6253\u5305 \u251c\u2500\u2500 data # \u6570\u636e\u76ee\u5f55 \u2502\u00a0\u00a0 \u251c\u2500\u2500 result.json # \u7ed3\u679c\u63d0\u4ea4\u6587\u4ef6 \u2502\u00a0\u00a0 \u251c\u2500\u2500 test_question.json # \u6d4b\u8bd5\u96c6 \u2502\u00a0\u00a0 \u2514\u2500\u2500 train_a.pdf # \u8bad\u7ec3\u96c6 \u251c\u2500\u2500 faiss_retriever.py # faiss\u5411\u91cf\u53ec\u56de \u251c\u2500\u2500 vllm_model.py # vllm\u5927\u6a21\u578b\u52a0\u901fwrapper \u251c\u2500\u2500 pdf_parse.py # pdf\u6587\u6863\u89e3\u6790\u5668 \u251c\u2500\u2500 pre_train_model # \u9884\u8bad\u7ec3\u5927\u6a21\u578b \u2502\u00a0\u00a0 \u251c\u2500\u2500 Qwen-7B-Chat # Qwen-7B \u2502\u00a0\u00a0 \u2502\u00a0\u00a0 \u2514\u2500\u2500 download.py \u2502\u00a0\u00a0 \u251c\u2500\u2500 bge-reranker-large # bge\u91cd\u6392\u5e8f\u6a21\u578b \u2502\u00a0\u00a0 \u2514\u2500\u2500 m3e-large # \u5411\u91cf\u68c0\u7d22\u6a21\u578b \u251c\u2500\u2500 qwen_generation_utils.py # qwen\u7b54\u6848\u751f\u6210\u7684\u5de5\u5177\u51fd\u6570 \u251c\u2500\u2500 requirements.txt # \u6b64\u9879\u76ee\u7684\u7b2c\u4e09\u65b9\u4f9d\u8d56\u5e93 \u251c\u2500\u2500 rerank_model.py # \u91cd\u6392\u5e8f\u903b\u8f91 \u251c\u2500\u2500 run.py # \u4e3b\u6587\u4ef6 \u2514\u2500\u2500 run.sh # \u4e3b\u8fd0\u884c\u811a\u672c 2 \u3001\u9879\u76ee\u6982\u8ff0 2.1 \u57fa\u4e8e\u5927\u6a21\u578b\u7684\u6587\u6863\u68c0\u7d22\u95ee\u7b54 \u4efb\u52a1\uff1a\u9879\u76ee\u8981\u6c42\u4ee5\u5927\u6a21\u578b\u4e3a\u4e2d\u5fc3\u5236\u4f5c\u4e00\u4e2a\u95ee\u7b54\u7cfb\u7edf\uff0c\u56de\u7b54\u7528\u6237\u7684\u6c7d\u8f66\u76f8\u5173\u95ee\u9898\u3002\u9700\u8981\u6839\u636e\u95ee\u9898\uff0c\u5728\u6587\u6863\u4e2d\u5b9a\u4f4d\u76f8\u5173\u4fe1\u606f\u7684\u4f4d\u7f6e\uff0c\u5e76\u6839\u636e\u6587\u6863\u5185\u5bb9\u901a\u8fc7\u5927\u6a21\u578b\u751f\u6210\u76f8\u5e94\u7684\u7b54\u6848\u3002\u672c\u9879\u76ee\u6d89\u53ca\u7684\u95ee\u9898\u4e3b\u8981\u56f4\u7ed5\u6c7d\u8f66\u4f7f\u7528\u3001\u7ef4\u4fee\u3001\u4fdd\u517b\u7b49\u65b9\u9762\uff0c\u5177\u4f53\u53ef\u53c2\u8003\u4e0b\u9762\u7684\u4f8b\u5b50\uff1a \u95ee\u98981\uff1a\u600e\u4e48\u6253\u5f00\u5371\u9669\u8b66\u544a\u706f\uff1f \u7b54\u68481\uff1a\u5371\u9669\u8b66\u544a\u706f\u5f00\u5173\u5728\u65b9\u5411\u76d8\u4e0b\u65b9\uff0c\u6309\u4e0b\u5f00\u5173\u5373\u53ef\u6253\u5f00\u5371\u9669\u8b66\u544a\u706f\u3002 \u95ee\u98982\uff1a\u8f66\u8f86\u5982\u4f55\u4fdd\u517b\uff1f \u7b54\u68482\uff1a\u4e3a\u4e86\u4fdd\u6301\u8f66\u8f86\u5904\u4e8e\u6700\u4f73\u72b6\u6001\uff0c\u5efa\u8bae\u60a8\u5b9a\u671f\u5173\u6ce8\u8f66\u8f86\u72b6\u6001\uff0c\u5305\u62ec\u5b9a\u671f\u4fdd\u517b\u3001\u6d17\u8f66\u3001\u5185\u90e8\u6e05\u6d01\u3001\u5916\u90e8\u6e05\u6d01\u3001\u8f6e\u80ce\u7684\u4fdd\u517b\u3001\u4f4e\u538b\u84c4\u7535\u6c60\u7684\u4fdd\u517b\u7b49\u3002 \u95ee\u98983\uff1a\u9760\u80cc\u592a\u70ed\u600e\u4e48\u529e\uff1f \u7b54\u68483\uff1a\u60a8\u597d\uff0c\u5982\u679c\u60a8\u7684\u5ea7\u6905\u9760\u80cc\u592a\u70ed\uff0c\u53ef\u4ee5\u5c1d\u8bd5\u5173\u95ed\u5ea7\u6905\u52a0\u70ed\u529f\u80fd\u3002\u5728\u591a\u5a92\u4f53\u663e\u793a\u5c4f\u4e0a\u4f9d\u6b21\u70b9\u51fb\u7a7a\u8c03\u5f00\u542f\u6309\u952e\u2192\u5ea7\u6905\u2192\u52a0\u70ed\uff0c\u5728\u8be5\u754c\u9762\u4e0b\u53ef\u4ee5\u5173\u95ed\u5ea7\u6905\u52a0\u70ed\u3002</p> <p>2.2 \u6570\u636e\u96c6 \u9886\u514b\u6c7d\u8f66\u7684\u7528\u6237\u624b\u518c 3\u3001\u89e3\u51b3\u65b9\u6848 3.1 pdf\u89e3\u6790 3.1.1 pdf\u5206\u5757\u89e3\u6790 \u6211\u4eec\u5e0c\u671bpdf\u89e3\u6790\u80fd\u5c3d\u53ef\u80fd\u7684\u6309\u7167\u5feb\u72b6\u8fdb\u884c\u89e3\u6790\uff0c\u6bcf\u4e00\u5757\u5f53\u505a\u4e00\u4e2a\u6837\u672c\uff0c\u8fd9\u6837\u80fd\u5c3d\u53ef\u80fd\u7684\u4fdd\u8bc1pdf\u4e2d\u6587\u672c\u5185\u5bb9\u7684\u5b8c\u6574\u6027\u3002 3.1.2 pdf \u6ed1\u7a97\u6cd5\u89e3\u6790 \u5982\u4f55\u4fdd\u8bc1\u6587\u672c\u5185\u5bb9\u7684\u8de8\u9875\u8fde\u7eed\u6027\u95ee\u9898\uff0c\u6211\u4eec\u63d0\u51fa\u6ed1\u7a97\u6cd5\u3002 \u5177\u4f53\u7684\u628apdf\u4e2d\u6240\u6709\u5185\u5bb9\u5f53\u505a\u4e00\u4e2a\u5b57\u7b26\u4e32\u6765\u5904\u7406\uff0c\u6309\u7167\u53e5\u53f7\u8fdb\u884c\u5206\u5272\uff0c\u6839\u636e\u5206\u5272\u540e\u7684\u6570\u7ec4\u8fdb\u884c\u6ed1\u7a97\u3002\u5177\u4f53\u7684\u5982\u4e0b\u6240\u793a: [\"aa\",\"bb\",\"cc\",\"dd\"] \u5982\u679c\u5b57\u7b26\u4e32\u957f\u5ea6\u4e3a4, \u7ecf\u8fc7\u6ed1\u7a97\u540e\u7684\u7ed3\u679c\u5982\u4e0b: aabb bbcc ccdd \u6211\u4eec\u5e0c\u671b\u6ed1\u7a97\u6cd5\u50cf\u5377\u79ef\u4e00\u6837\u53ef\u4ee5\u4e0d\u540c\u7684kernel,Stride,\u6765\u5bfb\u627e\u80fd\u8986\u76d6\u5230\u7684\u6700\u4f18\u7684\u6837\u672c\u53ec\u56de\u3002 \u7b80\u5355\u6765\u8bf4\u5c31\u662f\uff0c\u6ed1\u52a8\u7a97\u53e3\u662f\u6709overlap\u7684\uff0c\u4ea4\u53e0\u8fd9\u8fdb\u884c\uff0c\u8fd9\u6837\u53ef\u4ee5\u6700\u5927\u9650\u5ea6\u4fdd\u8bc1\u6587\u6863\u5757\u7684\u5b8c\u6574\u6027\uff0c\u907f\u514d\u628a\u91cd\u8981\u7684\u7b54\u6848\u6b65\u9aa4\u5207\u788e\u6216\u8df3\u8fc7\u3002</p> <p>3.1.3 \u9879\u76ee\u91c7\u53d6\u7684pdf\u89e3\u6790\u65b9\u6848 \u9879\u76ee\u6700\u7ec8\u91c7\u7528\u4e86\u4e09\u79cd\u89e3\u6790\u65b9\u6848\u7684\u7efc\u5408\uff1a \u25cf pdf\u5206\u5757\u89e3\u6790\uff0c\u5c3d\u91cf\u4fdd\u8bc1\u4e00\u4e2a\u5c0f\u6807\u9898+\u5bf9\u5e94\u6587\u6863\u5728\u4e00\u4e2a\u6587\u6863\u5757\uff0c\u5176\u4e2d\u6587\u6863\u5757\u7684\u957f\u5ea6\u5206\u522b\u662f512\u548c1024\u3002 \u25cf pdf\u6ed1\u7a97\u6cd5\u89e3\u6790\uff0c\u628a\u6587\u6863\u53e5\u53f7\u5206\u5272\uff0c\u7136\u540e\u6784\u5efa\u6ed1\u52a8\u7a97\u53e3\uff0c\u5176\u4e2d\u6587\u6863\u5757\u7684\u957f\u5ea6\u5206\u522b\u662f256\u548c512\u3002 \u25cf pdf\u975e\u6ed1\u7a97\u6cd5\u89e3\u6790\uff0c\u628a\u6587\u6863\u53e5\u53f7\u5206\u5272\uff0c\u7136\u540e\u6309\u7167\u6587\u6863\u5757\u9884\u8bbe\u5c3a\u5bf8\u5747\u5300\u5207\u5206\uff0c\u5176\u4e2d\u6587\u6863\u5757\u7684\u957f\u5ea6\u5206\u522b\u662f256\u548c512\u3002 \u6309\u71673\u4e2d\u89e3\u6790\u65b9\u6848\u5bf9\u6570\u636e\u5904\u7406\u4e4b\u540e\uff0c\u7136\u540e\u5bf9\u6587\u6863\u5757\u505a\u4e86\u4e00\u4e2a\u53bb\u91cd\uff0c\u6700\u540e\u628a\u8fd9\u4e9b\u6587\u6863\u5757\u8f93\u5165\u7ed9\u53ec\u56de\u6a21\u5757\u3002</p> <p>3.2 \u53ec\u56de \u53ec\u56de\u4e3b\u8981\u4f7f\u7528langchain\u4e2d\u7684retrievers\u8fdb\u884c\u6587\u672c\u7684\u53ec\u56de\u3002\u6211\u4eec\u77e5\u9053\u5411\u91cf\u53ec\u56de\u548cbm25\u53ec\u56de\u5177\u6709\u4e92\u8865\u6027\uff0c\u524d\u8005\u662f\u6df1\u5ea6\u8bed\u4e49\u53ec\u56de\uff0c\u4fa7\u91cd\u6cdb\u5316\u6027\uff0c\u540e\u8005\u662f\u5b57\u9762\u53ec\u56de\uff0c\u4fa7\u91cd\u5173\u952e\u8bcd/\u5b9e\u4f53\u7684\u5b57\u9762\u76f8\u5173\u6027\uff0c\u8fd9\u4e24\u4e2a\u53ec\u56de\u7b97\u6cd5\u4e5f\u662f\u5de5\u4e1a\u754c\u7528\u7684\u6bd4\u8f83\u591a\u7684\uff0c\u6bd4\u8f83\u6709\u4ee3\u8868\u6027\uff0c\u56e0\u6b64\u9009\u7528\u4e86\u8fd9\u4e24\u4e2a\u8fdb\u884c\u53ec\u56de\u3002</p> <p>3.2.1 \u5411\u91cf\u53ec\u56de \u5411\u91cf\u53ec\u56de\u5229\u7528 FAISS \u8fdb\u884c\u7d22\u5f15\u521b\u5efa\u548c\u67e5\u627e\uff0cembedding \u5229\u7528 M3E-large \u3002 M3E \u662f Moka Massive Mixed Embedding \u7684\u7f29\u5199 \u25cf Moka\uff0c\u6b64\u6a21\u578b\u7531 MokaAI \u8bad\u7ec3\uff0c\u5f00\u6e90\u548c\u8bc4\u6d4b\uff0c\u8bad\u7ec3\u811a\u672c\u4f7f\u7528 uniem \uff0c\u8bc4\u6d4b BenchMark \u4f7f\u7528 MTEB-zh \u25cf Massive\uff0c\u6b64\u6a21\u578b\u901a\u8fc7\u5343\u4e07\u7ea7 (2200w+) \u7684\u4e2d\u6587\u53e5\u5bf9\u6570\u636e\u96c6\u8fdb\u884c\u8bad\u7ec3 \u25cf Mixed\uff0c\u6b64\u6a21\u578b\u652f\u6301\u4e2d\u82f1\u53cc\u8bed\u7684\u540c\u8d28\u6587\u672c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\uff0c\u5f02\u8d28\u6587\u672c\u68c0\u7d22\u7b49\u529f\u80fd\uff0c\u672a\u6765\u8fd8\u4f1a\u652f\u6301\u4ee3\u7801\u68c0\u7d22 \u25cf Embedding\uff0c\u6b64\u6a21\u578b\u662f\u6587\u672c\u5d4c\u5165\u6a21\u578b\uff0c\u53ef\u4ee5\u5c06\u81ea\u7136\u8bed\u8a00\u8f6c\u6362\u6210\u7a20\u5bc6\u7684\u5411\u91cf M3E\u5171\u6709\u4ee5\u4e0b\u4e09\u79cd\u6a21\u578b\uff1asmall, base, large\u3002\u672c\u9879\u76ee\u91c7\u7528\u7684\u662fM3E large\u3002</p> <p>3.2.2 BM25\u53ec\u56de BM25\u7b97\u6cd5\uff0c\u901a\u5e38\u7528\u4e8e\u8ba1\u7b97\u4e24\u4e2a\u6587\u672c\uff0c\u6216\u8005\u6587\u672c\u4e0e\u6587\u6863\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u3002\u6240\u4ee5\u53ef\u4ee5\u7528\u4e8e\u6587\u672c\u76f8\u4f3c\u5ea6\u8ba1\u7b97\u548c\u6587\u672c\u68c0\u7d22\u7b49\u5e94\u7528\u573a\u666f\u3002\u5b83\u7684\u4e3b\u8981\u601d\u60f3\u662f:\u5bf9\u4e8e\u6587\u672cquery\u4e2d\u7684\u6bcf\u4e2a\u8bcdqi\uff0c\u8ba1\u7b97qi\u4e0e\u5019\u9009\u6587\u672c(\u6587\u6863)\u7684\u76f8\u5173\u5ea6\uff0c\u7136\u540e\u5bf9\u6240\u6709\u8bcdqi\u5f97\u5230\u7684\u76f8\u5173\u5ea6\u8fdb\u884c\u52a0\u6743\u6c42\u548c\uff0c\u4ece\u800c\u5f97\u5230query\u4e0e\u6587\u6863document\u7684\u76f8\u5173\u6027\u5f97\u5206\u3002 BM25\u53ec\u56de\u5229\u7528 LangChain\u7684BM25 Retrievers\u3002</p> <p>3.3 \u91cd\u6392\u5e8f Reranker \u662f\u4fe1\u606f\u68c0\u7d22\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u7528\u4e8e\u8bc4\u4f30\u641c\u7d22\u7ed3\u679c\uff0c\u5e76\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\uff0c\u4ece\u800c\u63d0\u5347\u67e5\u8be2\u7ed3\u679c\u76f8\u5173\u6027\u3002\u5728 RAG \u5e94\u7528\u4e2d\uff0c\u4e3b\u8981\u5728\u62ff\u5230\u53ec\u56de\u7ed3\u679c\u540e\u4f7f\u7528 Reranker\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u786e\u5b9a\u6587\u6863\u548c\u67e5\u8be2\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u66f4\u7cbe\u7ec6\u5730\u5bf9\u7ed3\u679c\u91cd\u6392\uff0c\u6700\u7ec8\u63d0\u9ad8\u641c\u7d22\u8d28\u91cf\u3002 \u5c06 Reranker \u6574\u5408\u5230 RAG \u5e94\u7528\u4e2d\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u751f\u6210\u7b54\u6848\u7684\u7cbe\u786e\u5ea6\uff0c\u56e0\u4e3a Reranker \u80fd\u591f\u5728\u5355\u8def\u6216\u591a\u8def\u7684\u53ec\u56de\u7ed3\u679c\u4e2d\u6311\u9009\u51fa\u548c\u95ee\u9898\u6700\u63a5\u8fd1\u7684\u6587\u6863\u3002\u6b64\u5916\uff0c\u6269\u5927\u68c0\u7d22\u7ed3\u679c\u7684\u4e30\u5bcc\u5ea6\uff08\u4f8b\u5982\u591a\u8def\u53ec\u56de\uff09\u914d\u5408\u7cbe\u7ec6\u5316\u7b5b\u9009\u6700\u76f8\u5173\u7ed3\u679c\uff08Reranker\uff09\u8fd8\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6700\u7ec8\u7ed3\u679c\u8d28\u91cf\u3002\u4f7f\u7528 Reranker \u53ef\u4ee5\u6392\u9664\u6389\u7b2c\u4e00\u5c42\u53ec\u56de\u4e2d\u548c\u95ee\u9898\u5173\u7cfb\u4e0d\u5927\u7684\u5185\u5bb9\uff0c\u5c06\u8f93\u5165\u7ed9\u5927\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u8303\u56f4\u8fdb\u4e00\u6b65\u7f29\u5c0f\u5230\u6700\u76f8\u5173\u7684\u4e00\u5c0f\u90e8\u5206\u6587\u6863\u4e2d\u3002\u901a\u8fc7\u7f29\u77ed\u4e0a\u4e0b\u6587\uff0c LLM \u80fd\u591f\u66f4\u201c\u5173\u6ce8\u201d\u4e0a\u4e0b\u6587\u4e2d\u7684\u6240\u6709\u5185\u5bb9\uff0c\u907f\u514d\u5ffd\u7565\u91cd\u70b9\u5185\u5bb9\uff0c\u8fd8\u80fd\u8282\u7701\u63a8\u7406\u6210\u672c\u3002 \u5411\u91cf\u53ec\u56de\u4e2d\u4f7f\u7528\u7684\u662fbi-encoder\u7ed3\u6784\uff0c\u800cbge-reranker-large \u4f7f\u7528\u7684\u662f cross-encoder\u7ed3\u6784\uff0ccross-encoder\u7ed3\u6784\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u8981\u4f18\u4e8ebi-encoder \u53ef\u4ee5\u770b\u51fa\uff0c\u8fd9\u4e2a\u68c0\u7d22\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a \u5728\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u51fa Top-K \u76f8\u5173\u6587\u6863\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u914d\u5408 Sparse embedding\uff08\u7a00\u758f\u5411\u91cf\u6a21\u578b\uff0c\u4f8b\u5982TF-DF\uff09\u8986\u76d6\u5168\u6587\u68c0\u7d22\u80fd\u529b\u3002 Reranker \u6839\u636e\u8fd9\u4e9b\u68c0\u7d22\u51fa\u6765\u7684\u6587\u6863\u4e0e\u67e5\u8be2\u7684\u76f8\u5173\u6027\u8fdb\u884c\u6253\u5206\u548c\u91cd\u6392\u3002\u91cd\u6392\u540e\u6311\u9009\u6700\u9760\u524d\u7684\u7ed3\u679c\u4f5c\u4e3a Prompt \u4e2d\u7684Context \u4f20\u5165 LLM\uff0c\u6700\u7ec8\u751f\u6210\u8d28\u91cf\u66f4\u9ad8\u3001\u76f8\u5173\u6027\u66f4\u5f3a\u7684\u7b54\u6848\u3002 \u4f46\u662f\u9700\u8981\u6ce8\u610f\uff0c\u76f8\u6bd4\u4e8e\u53ea\u8fdb\u884c\u5411\u91cf\u68c0\u7d22\u7684\u57fa\u7840\u67b6\u6784\u7684 RAG\uff0c\u589e\u52a0 Reranker \u4e5f\u4f1a\u5e26\u6765\u4e00\u4e9b\u6311\u6218\uff0c\u589e\u52a0\u4f7f\u7528\u6210\u672c\u3002 \u8fd9\u4e2a\u6210\u672c\u5305\u62ec\u4e24\u65b9\u9762\uff0c\u589e\u52a0\u5ef6\u8fdf\u5bf9\u4e8e\u4e1a\u52a1\u7684\u5f71\u54cd\u3001\u589e\u52a0\u8ba1\u7b97\u91cf\u5bf9\u670d\u52a1\u6210\u672c\u7684\u589e\u52a0\u3002\u6211\u4eec\u5efa\u8bae\u6839\u636e\u81ea\u5df1\u7684\u4e1a\u52a1\u9700\u6c42\uff0c\u5728\u68c0\u7d22\u8d28\u91cf\u3001\u641c\u7d22\u5ef6\u8fdf\u3001\u4f7f\u7528\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u5408\u7406\u8bc4\u4f30\u662f\u5426\u9700\u8981\u4f7f\u7528 Reranker\u3002</p> <p>3.3.1 cross-encoder \u91cd\u6392\u5e8f\u6b64\u5904\u4f7f\u7528\u4e86 bge-reranker-large\u3002 \u76ee\u524drerank\u6a21\u578b\u91cc\u9762\uff0c\u6700\u597d\u7684\u5e94\u8be5\u662fcohere\uff0c\u4e0d\u8fc7\u5b83\u662f\u6536\u8d39\u7684\u3002\u5f00\u6e90\u7684\u662f\u667a\u6e90\u53d1\u5e03\u7684bge-reranker-base\u548cbge-reranker-large\u3002bge-reranker-large\u7684\u80fd\u529b\u57fa\u672c\u4e0a\u63a5\u8fd1cohere\uff0c\u800c\u4e14\u5728\u4e00\u4e9b\u65b9\u9762\u8fd8\u66f4\u597d\uff1b \u51e0\u4e4e\u6240\u6709\u7684Embeddings\u90fd\u5728\u91cd\u6392\u4e4b\u540e\u663e\u793a\u51fa\u66f4\u9ad8\u7684\u547d\u4e2d\u7387\u548cMRR\uff0c\u6240\u4ee5rerank\u7684\u6548\u679c\u662f\u975e\u5e38\u663e\u8457\u7684\uff1b embedding\u6a21\u578b\u548crerank\u6a21\u578b\u7684\u7ec4\u5408\u4e5f\u4f1a\u6709\u5f71\u54cd\uff0c\u9700\u8981\u5f00\u53d1\u8005\u5728\u5b9e\u9645\u8fc7\u7a0b\u4e2d\u53bb\u8c03\u6d4b\u6700\u4f73\u7ec4\u5408\u3002</p> <p>3.4 \u63a8\u7406\u4f18\u5316 3.4 vLLM\u52a0\u901f vLLM\u662f\u4e00\u4e2a\u57fa\u4e8ePython\u7684LLM\u63a8\u7406\u548c\u670d\u52a1\u6846\u67b6\uff0c\u5b83\u7684\u4e3b\u8981\u4f18\u52bf\u5728\u4e8e\u7b80\u5355\u6613\u7528\u548c\u6027\u80fd\u9ad8\u6548\u3002\u901a\u8fc7PagedAttention\u6280\u672f\u3001\u8fde\u7eed\u6279\u5904\u7406\u3001CUDA\u6838\u5fc3\u4f18\u5316\u4ee5\u53ca\u5206\u5e03\u5f0f\u63a8\u7406\u652f\u6301\uff0cvLLM\u80fd\u591f\u663e\u8457\u63d0\u9ad8LLM\u7684\u63a8\u7406\u901f\u5ea6\uff0c\u964d\u4f4e\u663e\u5b58\u5360\u7528\uff0c\u66f4\u597d\u5730\u6ee1\u8db3\u5b9e\u9645\u5e94\u7528\u9700\u6c42\u3002vLLM \u63a8\u7406\u6846\u67b6\u4f7f\u5927\u6a21\u578b\u63a8\u7406\u901f\u5ea6\u5f97\u5230\u660e\u663e\u63d0\u5347\uff0c\u63a8\u7406\u901f\u5ea6\u6bd4\u666e\u901a\u63a8\u7406\u67091\u500d\u7684\u52a0\u901f\u3002\u5728\u4ea7\u54c1\u7ea7\u7684\u90e8\u7f72\u4e0a\uff0cvLLM\u65e2\u80fd\u6ee1\u8db3batch\u63a8\u7406\u7684\u8981\u6c42\uff0c\u53c8\u80fd\u5b9e\u73b0\u9ad8\u5e76\u53d1\u4e0b\u7684continuous batching\uff0c\u5728\u5b9e\u9645\u4ea7\u54c1\u90e8\u7f72\u4e2d\u5e94\u7528\u662f\u975e\u5e38\u5e7f\u6cdb\u7684\u3002 \u8fd9\u91cc\u5bf9Qwen-7B\u6a21\u578b\u8fdb\u884c\u4e86vLLM\u52a0\u901f\uff0c\u9879\u76ee\u4ee3\u7801\u5bf9\u8be5\u52a0\u901f\u903b\u8f91\u505a\u4e86\u4e00\u4e2a\u5c01\u88c5\uff0c\u5177\u4f53\u53ef\u4ee5\u53c2\u89c1\uff1avllm_model.py\u3002</p> <p>3.6 \u4ee3\u7801\u8fd0\u884c</p> <ol> <li>\u547d\u4ee4\u884c\u8fd0\u884c\uff1apython run.py</li> <li>docker\u8fd0\u884c\uff1a\u5148\u6267\u884cbash build.sh\uff0c\u518ddocker run $\u955c\u50cf\u540d</li> </ol> <p>3.7 \u9879\u76ee\u6539\u8fdb</p> <ol> <li>\u91c7\u53d6\u66f4\u591a\u8def\u53ec\u56de\u7b56\u7565\uff0c\u589e\u52a0TF-IDF\u53ec\u56de\uff0cbge\u53ec\u56de\u3001gte\u53ec\u56de\u548cbce-embedding-base_v1\u53ec\u56de\uff0c\u7136\u540e\u4f7f\u7528\u5206\u522b\u4f7f\u7528bge-reranker\u548cbce-reranker-base_v1\u8fdb\u884c\u7cbe\u6392\uff1b</li> <li>LLM\u5206\u522b\u91c7\u7528ChatGLM3-6B, Qwen1.5-7B-Chat\u548cBaichuan2-7B-Chat\u4f5c\u4e3a\u5927\u6a21\u578b\u57fa\u5ea7\uff0c\u4ee3\u7801\u505a\u6210\u53ef\u914d\u7f6e\u3002</li> <li>\u5c06\u62bd\u53d6\u540e\u7684\u6587\u6863\u4f7f\u7528LLM\u91cd\u65b0\u6574\u7406\uff0c\u4f7f\u5f97\u6742\u4e71\u77e5\u8bc6\u5e93\u89c4\u6574\u3002\u7136\u540e\u518d\u9001\u5165\u5230\u7b54\u6848\u751f\u6210\u6a21\u5757\uff0c\u8fd9\u91cc\u9700\u8981\u7528\u5230prompt\u63d0\u793a\u5de5\u7a0b\u6280\u672f\u3002</li> <li>\u5148\u7528LLM\u76f4\u63a5\u751f\u6210\u7b54\u6848\uff0c\u7136\u540e\u5c06\u95ee\u9898\u548c\u8fd9\u4e2a\u751f\u6210\u7684\u7b54\u6848\u62fc\u63a5\uff0c\u5171\u540c\u5b8c\u6210\u68c0\u7d22\uff0c\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002</li> <li>\u5148\u7528LLM\u5148\u5c06\u95ee\u9898\u6539\u5199\u548c\u6269\u5145\u4e00\u904d\uff0c\u7136\u540e\u5c06\u95ee\u9898\u548c\u8fd9\u4e2a\u6539\u5199\u540e\u7684\u95ee\u9898\u62fc\u63a5\uff0c\u63d0\u5347\u68c0\u7d22\u6548\u679c\u3002</li> <li>\u4e00\u6b21\u7ed9LLM\u4e00\u4e2a\u68c0\u7d22\u5230\u7684\u6587\u6863\uff0c\u4e0d\u65ad\u4f18\u5316\u751f\u6210\u7684\u7b54\u6848\uff0c\u5373\u5229\u7528prompt\u6280\u672f\u5bf9LLM\uff0ccontext\u548c\u539f\u7b54\u6848\uff0c\u4f18\u5f97\u5230\u4f18\u5316\u5347\u7ea7\u540e\u7684\u7b54\u6848\u3002</li> </ol> <p>\u5b8c\u6210\u4e0a\u8ff0\u9879\u76ee\u6539\u8fdb\u70b9\uff0c\u8fd9\u4e9b\u6539\u8fdb\u70b9\u90fd\u53ef\u4ee5\u4f5c\u4e3a\u5b9e\u6218\u8c03\u4f18\u7ecf\u9a8c\u5199\u8fdb\u7b80\u5386\uff0c\u4e0a\u4f20\u6539\u8fdb\u540e\u7684\u9879\u76ee\u4ee3\u7801\u5230\u5b66\u4e60\u7a7a\u95f4\u3002</p> <p>\u9879\u76ee\u4ee3\u7801 \u6211\u7528\u5938\u514b\u7f51\u76d8\u5206\u4eab\u4e86\u300c\u7efc\u5408\u9879\u76ee\u5b9e\u6218\u9879\u76ee\u4e00\u300d\uff0c\u70b9\u51fb\u94fe\u63a5\u5373\u53ef\u4fdd\u5b58\u3002\u6253\u5f00\u300c\u5938\u514bAPP\u300d\uff0c\u65e0\u9700\u4e0b\u8f7d\u5728\u7ebf\u64ad\u653e\u89c6\u9891\uff0c\u7545\u4eab\u539f\u753b5\u500d\u901f\uff0c\u652f\u6301\u7535\u89c6\u6295\u5c4f\u3002 \u94fe\u63a5\uff1ahttps://pan.quark.cn/s/447fd3679877 \u63d0\u53d6\u7801\uff1aC2b3</p>"},{"location":"projects/KnowledgeBrain/reference/","title":"\u600e\u4e48\u7406\u89e3\u201c\u53ec\u56de\u201d","text":"<p>\u5728\u4fe1\u606f\u68c0\u7d22\u548c\u673a\u5668\u5b66\u4e60\u9886\u57df\uff0c\u201c\u53ec\u56de\u201d\u662f\u4e00\u4e2a\u91cd\u8981\u7684\u8bc4\u4ef7\u6307\u6807\uff0c\u7528\u4e8e\u8861\u91cf\u6a21\u578b\u6216\u7cfb\u7edf\u5728\u68c0\u7d22\u6216\u5206\u7c7b\u4efb\u52a1\u4e2d\u7684\u8868\u73b0\u3002\u5177\u4f53\u6765\u8bf4\uff0c\u53ec\u56de\u7387\uff08Recall\uff09\u8868\u793a\u7684\u662f\u6a21\u578b\u4ece\u6240\u6709\u76f8\u5173\u6837\u672c\u4e2d\u6b63\u786e\u8bc6\u522b\u51fa\u76f8\u5173\u6837\u672c\u7684\u6bd4\u4f8b\u3002\u5b83\u7684\u8ba1\u7b97\u516c\u5f0f\u662f\uff1a</p> <p>\\[ \\text{\u53ec\u56de\u7387} = \\frac{\\text{\u6b63\u786e\u8bc6\u522b\u7684\u76f8\u5173\u6837\u672c\u6570}}{\\text{\u6240\u6709\u76f8\u5173\u6837\u672c\u6570}} \\]</p> <p>\u4e3e\u4e2a\u4f8b\u5b50\uff0c\u5982\u679c\u6709100\u4e2a\u76f8\u5173\u6587\u6863\uff0c\u800c\u6a21\u578b\u6b63\u786e\u68c0\u7d22\u51fa\u4e8680\u4e2a\u76f8\u5173\u6587\u6863\uff0c\u90a3\u4e48\u53ec\u56de\u7387\u5c31\u662f80%\u3002</p> <p>\u53ec\u56de\u7387\u7684\u9ad8\u4f4e\u53cd\u6620\u4e86\u6a21\u578b\u5728\u8bc6\u522b\u76f8\u5173\u6837\u672c\u65b9\u9762\u7684\u80fd\u529b\u3002\u9ad8\u53ec\u56de\u7387\u610f\u5473\u7740\u6a21\u578b\u80fd\u591f\u8bc6\u522b\u51fa\u5927\u591a\u6570\u7684\u76f8\u5173\u6837\u672c\uff0c\u4f46\u53ef\u80fd\u4f1a\u5e26\u6765\u66f4\u591a\u7684\u8bef\u62a5\uff08\u5373\u975e\u76f8\u5173\u6837\u672c\u88ab\u9519\u8bef\u5730\u8bc6\u522b\u4e3a\u76f8\u5173\u6837\u672c\uff09\u3002\u56e0\u6b64\uff0c\u53ec\u56de\u7387\u901a\u5e38\u4e0e\u7cbe\u786e\u7387\uff08Precision\uff09\u4e00\u8d77\u4f7f\u7528\uff0c\u4ee5\u5168\u9762\u8bc4\u4f30\u6a21\u578b\u7684\u6027\u80fd\u3002</p> <p>\u5728\u5b9e\u9645\u5e94\u7528\u4e2d\uff0c\u53ec\u56de\u7387\u7684\u91cd\u8981\u6027\u53d6\u51b3\u4e8e\u5177\u4f53\u7684\u4efb\u52a1\u9700\u6c42\u3002\u4f8b\u5982\uff0c\u5728\u533b\u7597\u8bca\u65ad\u4e2d\uff0c\u9ad8\u53ec\u56de\u7387\u53ef\u80fd\u66f4\u4e3a\u91cd\u8981\uff0c\u56e0\u4e3a\u6f0f\u8bca\u53ef\u80fd\u5e26\u6765\u4e25\u91cd\u540e\u679c\u3002</p>"},{"location":"projects/KnowledgeBrain/reference/#_2","title":"\u5411\u91cf\u53ec\u56de\u662f\u4ec0\u4e48","text":"<p>\u5411\u91cf\u53ec\u56de\u662f\u4e00\u79cd\u57fa\u4e8e\u5411\u91cf\u7a7a\u95f4\u6a21\u578b\u7684\u68c0\u7d22\u65b9\u6cd5\u3002\u5b83\u901a\u8fc7\u5c06\u6587\u6863\u548c\u67e5\u8be2\u8868\u793a\u4e3a\u5411\u91cf\uff0c\u5e76\u8ba1\u7b97\u5b83\u4eec\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u6765\u8fdb\u884c\u68c0\u7d22\u3002\u5e38\u7528\u7684\u76f8\u4f3c\u5ea6\u5ea6\u91cf\u65b9\u6cd5\u5305\u62ec\u4f59\u5f26\u76f8\u4f3c\u5ea6\u3001\u6b27\u51e0\u91cc\u5f97\u8ddd\u79bb\u7b49\u3002</p> <p>\u5177\u4f53\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <ol> <li>\u5411\u91cf\u5316\uff1a\u5c06\u6587\u6863\u548c\u67e5\u8be2\u8f6c\u6362\u4e3a\u5411\u91cf\u8868\u793a\u3002</li> <li>\u8ba1\u7b97\u76f8\u4f3c\u5ea6\uff1a\u8ba1\u7b97\u67e5\u8be2\u5411\u91cf\u4e0e\u6587\u6863\u5411\u91cf\u4e4b\u95f4\u7684\u76f8\u4f3c\u5ea6\u3002</li> <li>\u6392\u5e8f\uff1a\u6839\u636e\u76f8\u4f3c\u5ea6\u5bf9\u6587\u6863\u8fdb\u884c\u6392\u5e8f\uff0c\u8fd4\u56de\u6700\u76f8\u5173\u7684\u6587\u6863\u3002</li> </ol> <p>\u8fd9\u79cd\u65b9\u6cd5\u5728\u5904\u7406\u5927\u89c4\u6a21\u6587\u672c\u6570\u636e\u65f6\u975e\u5e38\u6709\u6548\uff0c\u5c24\u5176\u662f\u5728\u81ea\u7136\u8bed\u8a00\u5904\u7406\u548c\u4fe1\u606f\u68c0\u7d22\u9886\u57df\u3002</p>"},{"location":"projects/KnowledgeBrain/reference/#faiss","title":"\u4f7f\u7528FAISS\u8fdb\u884c\u5411\u91cf\u53ec\u56de\u7684\u6b65\u9aa4","text":"<p>FAISS\uff08Facebook AI Similarity Search\uff09\u662f\u4e00\u4e2a\u9ad8\u6548\u7684\u76f8\u4f3c\u6027\u641c\u7d22\u5e93\uff0c\u7279\u522b\u9002\u7528\u4e8e\u5927\u89c4\u6a21\u5411\u91cf\u68c0\u7d22\u3002\u5b83\u53ef\u4ee5\u5feb\u901f\u5730\u8fdb\u884c\u5411\u91cf\u7d22\u5f15\u548c\u67e5\u627e\uff0c\u9002\u5408\u5904\u7406\u9ad8\u7ef4\u5411\u91cf\u6570\u636e\u3002</p>"},{"location":"projects/KnowledgeBrain/reference/#bm25","title":"\u4ec0\u4e48\u662fbm25\u53ec\u56de","text":"<p>BM25\uff08Best Matching 25\uff09\u662f\u4e00\u79cd\u57fa\u4e8e\u6982\u7387\u68c0\u7d22\u6a21\u578b\u7684\u6587\u6863\u68c0\u7d22\u7b97\u6cd5\uff0c\u7528\u4e8e\u8ba1\u7b97\u67e5\u8be2\u4e0e\u6587\u6863\u4e4b\u95f4\u7684\u76f8\u5173\u6027\u5f97\u5206\u3002\u5b83\u662fOkapi BM25\u7684\u7b80\u5316\u7248\u672c\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u4fe1\u606f\u68c0\u7d22\u7cfb\u7edf\u4e2d\u3002</p> <p>BM25\u53ec\u56de\u7684\u6b65\u9aa4\u5982\u4e0b\uff1a</p> <ol> <li>\u6587\u6863\u9884\u5904\u7406\uff1a\u5bf9\u6587\u6863\u8fdb\u884c\u5206\u8bcd\u3001\u53bb\u505c\u7528\u8bcd\u7b49\u9884\u5904\u7406\u64cd\u4f5c\u3002</li> <li>\u8ba1\u7b97\u8bcd\u9891\uff1a\u8ba1\u7b97\u6bcf\u4e2a\u8bcd\u5728\u6587\u6863\u4e2d\u7684\u8bcd\u9891\uff08Term Frequency, TF\uff09\u3002</li> <li>\u8ba1\u7b97\u9006\u6587\u6863\u9891\u7387\uff1a\u8ba1\u7b97\u6bcf\u4e2a\u8bcd\u7684\u9006\u6587\u6863\u9891\u7387\uff08Inverse Document Frequency, IDF\uff09\u3002</li> <li>\u8ba1\u7b97BM25\u5f97\u5206\uff1a\u6839\u636eBM25\u516c\u5f0f\u8ba1\u7b97\u67e5\u8be2\u4e0e\u6bcf\u4e2a\u6587\u6863\u7684\u76f8\u5173\u6027\u5f97\u5206\u3002</li> </ol>"},{"location":"projects/KnowledgeBrain/reference/#rag","title":"\u4ec0\u4e48\u662fRAG","text":"<p>RAG\uff08Retrieval-Augmented Generation\uff09\u662f\u4e00\u79cd\u7ed3\u5408\u68c0\u7d22\u548c\u751f\u6210\u7684\u81ea\u7136\u8bed\u8a00\u5904\u7406\u6280\u672f\u3002\u5b83\u901a\u8fc7\u4ece\u4e00\u4e2a\u5927\u578b\u6587\u6863\u96c6\u5408\u4e2d\u68c0\u7d22\u76f8\u5173\u4fe1\u606f\uff0c\u7136\u540e\u4f7f\u7528\u751f\u6210\u6a21\u578b\u6765\u751f\u6210\u6700\u7ec8\u7684\u56de\u7b54\u6216\u6587\u672c\u3002RAG \u6a21\u578b\u901a\u5e38\u7528\u4e8e\u95ee\u7b54\u7cfb\u7edf\u3001\u5bf9\u8bdd\u7cfb\u7edf\u548c\u5176\u4ed6\u9700\u8981\u7ed3\u5408\u5916\u90e8\u77e5\u8bc6\u5e93\u7684\u4efb\u52a1\u3002</p> <p>RAG \u7684\u5de5\u4f5c\u6d41\u7a0b\u5982\u4e0b\uff1a</p> <ol> <li>\u68c0\u7d22\uff08Retrieval\uff09\uff1a\u9996\u5148\uff0c\u4ece\u4e00\u4e2a\u5927\u578b\u6587\u6863\u96c6\u5408\uff08\u5982\u77e5\u8bc6\u5e93\u6216\u4e92\u8054\u7f51\uff09\u4e2d\u68c0\u7d22\u4e0e\u8f93\u5165\u67e5\u8be2\u76f8\u5173\u7684\u6587\u6863\u6216\u6bb5\u843d\u3002\u5e38\u7528\u7684\u68c0\u7d22\u65b9\u6cd5\u5305\u62ec BM25\u3001FAISS \u7b49\u3002</li> <li>\u751f\u6210\uff08Generation\uff09\uff1a\u7136\u540e\uff0c\u4f7f\u7528\u751f\u6210\u6a21\u578b\uff08\u5982 GPT-3\u3001BERT \u7b49\uff09\u7ed3\u5408\u68c0\u7d22\u5230\u7684\u6587\u6863\u751f\u6210\u6700\u7ec8\u7684\u56de\u7b54\u6216\u6587\u672c\u3002</li> </ol> <p>\u8fd9\u79cd\u65b9\u6cd5\u7684\u4f18\u70b9\u662f\u80fd\u591f\u5229\u7528\u5916\u90e8\u77e5\u8bc6\u5e93\u4e2d\u7684\u4e30\u5bcc\u4fe1\u606f\uff0c\u63d0\u9ad8\u751f\u6210\u6587\u672c\u7684\u51c6\u786e\u6027\u548c\u76f8\u5173\u6027\u3002</p> <p>\u4e0b\u9762\u662f\u4e00\u4e2a\u7b80\u5355\u7684 RAG \u5de5\u4f5c\u6d41\u7a0b\u7684\u4f2a\u4ee3\u7801\uff1a</p> <pre><code>1. \u8f93\u5165\u67e5\u8be2 query\n2. \u4f7f\u7528\u68c0\u7d22\u6a21\u578b\u4ece\u6587\u6863\u96c6\u5408\u4e2d\u68c0\u7d22\u76f8\u5173\u6587\u6863 documents\n3. \u5c06 query \u548c documents \u4f5c\u4e3a\u8f93\u5165\uff0c\u4f20\u9012\u7ed9\u751f\u6210\u6a21\u578b\n4. \u751f\u6210\u6a21\u578b\u751f\u6210\u6700\u7ec8\u7684\u56de\u7b54\u6216\u6587\u672c response\n5. \u8f93\u51fa response\n</code></pre> <p>RAG \u6a21\u578b\u7684\u5b9e\u73b0\u53ef\u4ee5\u4f7f\u7528\u73b0\u6709\u7684\u68c0\u7d22\u548c\u751f\u6210\u5de5\u5177\uff0c\u5982 FAISS \u8fdb\u884c\u68c0\u7d22\uff0cGPT-3 \u8fdb\u884c\u751f\u6210\u3002</p>"},{"location":"projects/KnowledgeBrain/reference/#_3","title":"\u4ec0\u4e48\u662f \u91cd\u6392\u5e8f","text":"<p>Reranker \u662f\u4fe1\u606f\u68c0\u7d22\u751f\u6001\u7cfb\u7edf\u4e2d\u7684\u4e00\u4e2a\u91cd\u8981\u7ec4\u6210\u90e8\u5206\uff0c\u7528\u4e8e\u8bc4\u4f30\u641c\u7d22\u7ed3\u679c\uff0c\u5e76\u8fdb\u884c\u91cd\u65b0\u6392\u5e8f\uff0c\u4ece\u800c\u63d0\u5347\u67e5\u8be2\u7ed3\u679c\u76f8\u5173\u6027\u3002\u5728 RAG \u5e94\u7528\u4e2d\uff0c\u4e3b\u8981\u5728\u62ff\u5230\u53ec\u56de\u7ed3\u679c\u540e\u4f7f\u7528 Reranker\uff0c\u80fd\u591f\u66f4\u6709\u6548\u5730\u786e\u5b9a\u6587\u6863\u548c\u67e5\u8be2\u4e4b\u95f4\u7684\u8bed\u4e49\u76f8\u5173\u6027\uff0c\u66f4\u7cbe\u7ec6\u5730\u5bf9\u7ed3\u679c\u91cd\u6392\uff0c\u6700\u7ec8\u63d0\u9ad8\u641c\u7d22\u8d28\u91cf\u3002 \u5c06 Reranker \u6574\u5408\u5230 RAG \u5e94\u7528\u4e2d\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u751f\u6210\u7b54\u6848\u7684\u7cbe\u786e\u5ea6\uff0c\u56e0\u4e3a Reranker \u80fd\u591f\u5728\u5355\u8def\u6216\u591a\u8def\u7684\u53ec\u56de\u7ed3\u679c\u4e2d\u6311\u9009\u51fa\u548c\u95ee\u9898\u6700\u63a5\u8fd1\u7684\u6587\u6863\u3002\u6b64\u5916\uff0c\u6269\u5927\u68c0\u7d22\u7ed3\u679c\u7684\u4e30\u5bcc\u5ea6\uff08\u4f8b\u5982\u591a\u8def\u53ec\u56de\uff09\u914d\u5408\u7cbe\u7ec6\u5316\u7b5b\u9009\u6700\u76f8\u5173\u7ed3\u679c\uff08Reranker\uff09\u8fd8\u80fd\u8fdb\u4e00\u6b65\u63d0\u5347\u6700\u7ec8\u7ed3\u679c\u8d28\u91cf\u3002\u4f7f\u7528 Reranker \u53ef\u4ee5\u6392\u9664\u6389\u7b2c\u4e00\u5c42\u53ec\u56de\u4e2d\u548c\u95ee\u9898\u5173\u7cfb\u4e0d\u5927\u7684\u5185\u5bb9\uff0c\u5c06\u8f93\u5165\u7ed9\u5927\u6a21\u578b\u7684\u4e0a\u4e0b\u6587\u8303\u56f4\u8fdb\u4e00\u6b65\u7f29\u5c0f\u5230\u6700\u76f8\u5173\u7684\u4e00\u5c0f\u90e8\u5206\u6587\u6863\u4e2d\u3002\u901a\u8fc7\u7f29\u77ed\u4e0a\u4e0b\u6587\uff0c LLM \u80fd\u591f\u66f4\u201c\u5173\u6ce8\u201d\u4e0a\u4e0b\u6587\u4e2d\u7684\u6240\u6709\u5185\u5bb9\uff0c\u907f\u514d\u5ffd\u7565\u91cd\u70b9\u5185\u5bb9\uff0c\u8fd8\u80fd\u8282\u7701\u63a8\u7406\u6210\u672c\u3002 \u5411\u91cf\u53ec\u56de\u4e2d\u4f7f\u7528\u7684\u662fbi-encoder\u7ed3\u6784\uff0c\u800cbge-reranker-large \u4f7f\u7528\u7684\u662f cross-encoder\u7ed3\u6784\uff0ccross-encoder\u7ed3\u6784\u4e00\u5b9a\u7a0b\u5ea6\u4e0a\u8981\u4f18\u4e8ebi-encoder \u53ef\u4ee5\u770b\u51fa\uff0c\u8fd9\u4e2a\u68c0\u7d22\u7cfb\u7edf\u5305\u542b\u4e24\u4e2a\u9636\u6bb5\uff1a \u5728\u5411\u91cf\u6570\u636e\u5e93\u4e2d\u68c0\u7d22\u51fa Top-K \u76f8\u5173\u6587\u6863\uff0c\u540c\u65f6\u4e5f\u53ef\u4ee5\u914d\u5408 Sparse embedding\uff08\u7a00\u758f\u5411\u91cf\u6a21\u578b\uff0c\u4f8b\u5982TF-DF\uff09\u8986\u76d6\u5168\u6587\u68c0\u7d22\u80fd\u529b\u3002 Reranker \u6839\u636e\u8fd9\u4e9b\u68c0\u7d22\u51fa\u6765\u7684\u6587\u6863\u4e0e\u67e5\u8be2\u7684\u76f8\u5173\u6027\u8fdb\u884c\u6253\u5206\u548c\u91cd\u6392\u3002\u91cd\u6392\u540e\u6311\u9009\u6700\u9760\u524d\u7684\u7ed3\u679c\u4f5c\u4e3a Prompt \u4e2d\u7684Context \u4f20\u5165 LLM\uff0c\u6700\u7ec8\u751f\u6210\u8d28\u91cf\u66f4\u9ad8\u3001\u76f8\u5173\u6027\u66f4\u5f3a\u7684\u7b54\u6848\u3002 \u4f46\u662f\u9700\u8981\u6ce8\u610f\uff0c\u76f8\u6bd4\u4e8e\u53ea\u8fdb\u884c\u5411\u91cf\u68c0\u7d22\u7684\u57fa\u7840\u67b6\u6784\u7684 RAG\uff0c\u589e\u52a0 Reranker \u4e5f\u4f1a\u5e26\u6765\u4e00\u4e9b\u6311\u6218\uff0c\u589e\u52a0\u4f7f\u7528\u6210\u672c\u3002 \u8fd9\u4e2a\u6210\u672c\u5305\u62ec\u4e24\u65b9\u9762\uff0c\u589e\u52a0\u5ef6\u8fdf\u5bf9\u4e8e\u4e1a\u52a1\u7684\u5f71\u54cd\u3001\u589e\u52a0\u8ba1\u7b97\u91cf\u5bf9\u670d\u52a1\u6210\u672c\u7684\u589e\u52a0\u3002\u6211\u4eec\u5efa\u8bae\u6839\u636e\u81ea\u5df1\u7684\u4e1a\u52a1\u9700\u6c42\uff0c\u5728\u68c0\u7d22\u8d28\u91cf\u3001\u641c\u7d22\u5ef6\u8fdf\u3001\u4f7f\u7528\u6210\u672c\u4e4b\u95f4\u8fdb\u884c\u6743\u8861\uff0c\u5408\u7406\u8bc4\u4f30\u662f\u5426\u9700\u8981\u4f7f\u7528 Reranker\u3002 \u91cd\u6392\u5e8f\u5728\u641c\u7d22\u5f15\u64ce\u3001\u95ee\u7b54\u7cfb\u7edf\u548c\u63a8\u8350\u7cfb\u7edf\u4e2d\u5e7f\u6cdb\u5e94\u7528\uff0c\u901a\u8fc7\u7ed3\u5408\u521d\u59cb\u68c0\u7d22\u548c\u91cd\u6392\u5e8f\u6a21\u578b\uff0c\u53ef\u4ee5\u663e\u8457\u63d0\u9ad8\u68c0\u7d22\u7ed3\u679c\u7684\u8d28\u91cf\u548c\u7528\u6237\u6ee1\u610f\u5ea6\u3002</p>"}]}