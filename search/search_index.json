{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"\ud83d\ude80Home","text":""},{"location":"#welcome-to-mkdocs","title":"Welcome to MkDocs","text":"<p>For full documentation visit mkdocs.org.</p>"},{"location":"#commands","title":"Commands","text":"<ul> <li><code>mkdocs new [dir-name]</code> - Create a new project.</li> <li><code>mkdocs serve</code> - Start the live-reloading docs server.</li> <li><code>mkdocs build</code> - Build the documentation site.</li> <li><code>mkdocs -h</code> - Print help message and exit.</li> </ul>"},{"location":"#project-layout","title":"Project layout","text":"<pre><code>mkdocs.yml    # The configuration file.\ndocs/\n    index.md  # The documentation homepage.\n    ...       # Other markdown pages, images and other files.\n</code></pre>"},{"location":"DeepLearning/pytorch_intro/","title":"1. Dataset","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p10-p11%20Transforms/","title":"Transforms","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p10-p11%20Transforms/#_1","title":"\u4ecb\u7ecd","text":"<p>transforms\u662fpytorch\u7684\u4e00\u4e2a\u5de5\u5177\u7bb1\uff0c\u91cc\u9762\u5c01\u88c5\u4e00\u4e9b\u5de5\u5177\uff0c\u65cb\u8f6c\u3001\u7f29\u653e\u3001\u6b63\u5219\u5316\u7b49\u4e00\u7cfb\u5217\u64cd\u4f5c\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p10-p11%20Transforms/#_2","title":"\u5177\u4f53\u4f7f\u7528","text":"<pre><code>from PIL import Image\nfrom torchvision import transforms\n\nimage_path = \"..\\..\\data\\\\train\\\\ants\\\\0013035.jpg\"  # \u76f8\u5bf9\u8def\u5f84\nimg_PIL = Image.open(image_path) # PIL\u683c\u5f0f\n\n# \u8f6c\u4e3atensor\ntensor_trans = transforms.ToTensor()\ntensor_img = tensor_trans(img_PIL)\n\nprint(tensor_img)\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p10-p11%20Transforms/#tensor","title":"\u4e3a\u4ec0\u4e48\u7528Tensor","text":"<p>\u8fd9\u91cc\u53d1\u73b0Tensor\u548cnumpy\u5dee\u522b\u8fd8\u662f\u5f88\u5927\u7684\uff0c\u5728Tensor\u4e2d\u4fdd\u5b58\u4e86\u68af\u5ea6\u3001\u524d\u5411\u4f20\u64ad\u7b49\u5f88\u591a\u795e\u7ecf\u7f51\u7edc\u9700\u8981\u7528\u7684\u53c2\u6570\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p10-p11%20Transforms/#_3","title":"\u540e\u7eed\u4e00\u70b9\u64cd\u4f5c","text":"<pre><code>from PIL import Image\nfrom torchvision import transforms\nfrom torch.utils.tensorboard import SummaryWriter\n\nimage_path = \"..\\..\\data\\\\train\\\\ants\\\\0013035.jpg\"  # \u76f8\u5bf9\u8def\u5f84\nimg_PIL = Image.open(image_path) # PIL\u683c\u5f0f\n\nwriter = SummaryWriter(\"logs\")\n\n\n# \u8f6c\u4e3atensor\ntensor_trans = transforms.ToTensor()\ntensor_img = tensor_trans(img_PIL)\n\nwriter.add_image(\"tensor_img\",tensor_img)\n\nwriter.close()\n\n# print(tensor_img)\n</code></pre> <p>\u8fd9\u91cc\u5355\u7eaf\u628anumpy\u7c7b\u578b\u6362\u6210\u4e86tensor\u7c7b\u578b</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p12-p13%20%E5%B8%B8%E7%94%A8Transforms/","title":"\u5e38\u7528Transforms","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p12-p13%20%E5%B8%B8%E7%94%A8Transforms/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91ccup\u4e3b\u4ecb\u7ecd\u4e86\u4e00\u4e9btransforms\u65b9\u6cd5\u7684\u4f7f\u7528\uff1a</p> <p>\u81ea\u5df1\u8bb0\u4f4f\u6d41\u7a0b  \u770b\u5b98\u65b9\u6587\u6863\uff0c\u5173\u6ce8\u8f93\u5165\u8f93\u51fa\uff0c\u7528\u7684\u65f6\u5019\u518d\u67e5\u3002</p> <p></p> <p>\u4ee3\u7801\u7684\u8bdd\u5355\u7eaf\u662f\u529f\u80fd\u5b9e\u9645\u4f7f\u7528\uff0c\u8fd8\u662f\u81ea\u5df1\u719f\u6089\u6d41\u7a0b\u6bd4\u8f83\u597d\u3002</p> <pre><code>writer = SummaryWriter(\"logs\")\nimg = Image.open(\"..\\..\\data\\\\train\\\\ants\\\\0013035.jpg\")\nprint(img)\n\n#  ToTensor\ntrans_totensor = transforms.ToTensor()\nimg_tensor = trans_totensor(img)\nwriter.add_image(\"ToTensor\", img_tensor,1)\n\n#  Normalize\n\nprint(img_tensor[0][0][0])  # \u5904\u7406\u524d\uff0c\u770b\u770b\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u7684\u6570\u503c\ntrans_norm = transforms.Normalize([0.5, 0.5, 0.5], [0.5, 0.5, 0.5])  # \u56e0\u4e3a\u56fe\u50cf\u65f6\u4e09\u4e2achannel\u7684\uff0c\u6240\u4ee5\uff0c\u5747\u503c\u548c\u65b9\u5dee\u90fd\u662f\u4e09\u7ef4\u7684\nimg_norm = trans_norm(img_tensor)\nprint(img_norm[0][0][0])  # \u5904\u7406\u540e\uff0c\u518d\u770b\u7b2c\u4e00\u4e2a\u4f4d\u7f6e\u7684\u6570\u503c\uff0c\u770b\u4e00\u4e0b\u524d\u540e\u53d8\u5316\nwriter.add_image(\"Normalize\", img_norm, 2)\n\n#  Resize  \u8f93\u5165\u662f PIL \u7c7b\u578b\u624d\u884c\n\nprint(img.size)\ntrans_resize = transforms.Resize((128, 128)) # \u5bbd\u548c\u9ad8\uff0c\u5199\u4e00\u4e2a\u4f1a\u7b49\u6bd4\u4f8b\u7f29\u653e\nimg_resize = trans_resize(img)\nimg_resize = trans_totensor(img_resize)\nwriter.add_image(\"Resize\", img_resize, 3)\nprint(img_resize) # \u770b\u4e00\u4e0b\u53d8\u6362\u540e\u7684\u7ef4\u5ea6\n\n#  Compose -resize -2  \u628a\u591a\u4e2atransforms\u5408\u8d77\u6765\uff0c\u524d\u4e00\u4e2a\u7684\u8f93\u51fa\u662f\u540e\u4e00\u4e2a\u7684\u8f93\u5165\n\ntrans_resize_2 = transforms.Resize(128)\ntrans_compose = transforms.Compose([trans_resize_2, trans_totensor])\nimg_resize_2 = trans_compose(img)\nwriter.add_image(\"Resize\", img_resize_2, 4)\n\n#  RandomCrop  \u968f\u673a\u88c1\u526a\n\ntrans_random = transforms.RandomCrop(200, 200)\ntrans_compose_2 = transforms.Compose([trans_random, trans_totensor])\nfor i in range(10):\n    img_crop = trans_compose_2(img)\n    writer.add_image(\"RandomCrop\", img_crop, i)\n\nwriter.close()\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p14%20torchvision/","title":"torchvision","text":"<p>torchvision \u662fpytorch\u6846\u67b6\u4e13\u95e8\u5904\u7406\u6570\u636e\u7684\u4e00\u4e2a\u5305\u3002</p> <p>\u8fd9\u91ccup\u4e3b\u8981\u8bb2\u7684\u662f\u4ecetorchvision\u4e0b\u8f7d\u6bd4\u8f83\u5e38\u7528\u7684\u6570\u636e\u96c6\u3002</p> <pre><code>dataset_transform = torchvision.transforms.Compose([torchvision.transforms.ToTensor()])\n# \u4e00\u822c\u5bf9\u6570\u636e\u53ef\u80fd\u8981\u505a\u5f88\u591a\u5904\u7406\uff0c\u6211\u4eec\u76f4\u63a5\u521d\u59cb\u5316\u4e00\u4e2acompose\uff0c\u628a\u9700\u8981\u7684\u5904\u7406\u5199\u4e00\u8d77\n# \u6570\u636e\u96c6\u662fPIL\uff0c\u9700\u8981\u8f6cTensor,\u8fd9\u91cc\u7167\u7247\u5f88\u5c0f\uff0c\u6211\u4eec\u4e0d\u505a\u5176\u4ed6\u64cd\u4f5c\n\ntrain_set = torchvision.datasets.CIFAR10(root=\"./dataset\", train=True, transform=dataset_transform, download=True)\ntest_set = torchvision.datasets.CIFAR10(root=\"./dataset\", train=False, transform=dataset_transform, download=True)\n\nprint(test_set[0])\nprint(test_set.classes)\n\nimg, target = test_set[0]\nprint(img)\nprint(target)\nprint(test_set.classes[target])\n\nprint(test_set[0])\nwriter = SummaryWriter(\"p10\")\nfor i in range(10):\n    img, target = test_set[i]\n    writer.add_image(\"test_set\", img, i)\n\nwriter.close()\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e:</p> <p>\u200b   root  \u5b58\u50a8\u8def\u5f84</p> <p>\u200b    train  True/False  \u8bad\u7ec3\u96c6or\u6d4b\u8bd5\u96c6</p> <p>\u200b   transform \u8fdb\u884c\u53d8\u6362</p> <p>\u200b   download   True/False  \u4e0b\u8f7dor\u4e0d\u4e0b\u8f7d\u3002\u5efa\u8bae\u5199True\u3002</p> <p>\u540e\u9762\u5e38\u89c4\u64cd\u4f5c\u4e86\uff0c\u76f8\u5f53\u4e8e\u590d\u4e60\u4e0b\u524d\u9762\u7684\u5185\u5bb9\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p15%20dataloader/","title":"dataloader","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p15%20dataloader/#_1","title":"\u4ecb\u7ecd","text":"<p>up\u5728\u8fd9\u91cc\u7c7b\u522b\u4e86\u6253\u724c\uff0cdataloader\u5c31\u662f\u4ece\u724c\u5806\u4e2d\u6293\u53d6\u6251\u514b\u724c\u3002</p> <pre><code># \u5bfc\u5305\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\ntest_data = torchvision.datasets.CIFAR10(\"./dataset\", train=False, transform=torchvision.transforms.ToTensor(),download=True) # \u52a0\u8f7d\u6570\u636e\u96c6\n\ntest_loader = DataLoader(dataset=test_data, batch_size=64, shuffle=True, num_workers=0, drop_last=True)\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e:</p> <p>\u200b       batch_size \u6bcf\u6b21\u6253\u5305\u591a\u5c11\u4e2a\u6570\u636e</p> <p>\u200b       shuffle \u662f\u5426\u6253\u4e71\u987a\u5e8f</p> <p>\u200b       num_workers \u7ebf\u7a0b\u6570\uff0cwindows&gt;0\u53ef\u80fd\u51fa\u95ee\u9898</p> <p>\u200b       drop_last   \u5982\u679c\u6700\u540e\u4e00\u7ec4\u6570\u91cf&lt;batch_size\u662f\u5426\u820d\u5f03</p> <p></p> <p>\u56fe\u7247\u8f85\u52a9\u7406\u89e3batch_size\u3002</p> <pre><code># \u6d4b\u8bd5\u6570\u636e\u96c6\u4e2d\u7b2c\u4e00\u5f20\u56fe\u7247\u53catarget\nimg, target = test_data[0]\n# print(img.shape)  \n# print(target) \n\n# \u8fd9\u91cc\u6253\u5305batch_size\u4e2aimg\u548ctarget \u8fdb\u5165\u4e24\u4e2a\u5217\u8868\nfor data in test_loader:  # \u8fd9\u4e2aloader\uff0c\u8fd4\u56de\u7684\u5185\u5bb9\uff0c\u5c31\u5df2\u7ecf\u662f\u5305\u542b\u4e86 img \u548c target \u4e24\u4e2a\u503c\u4e86\uff0c\u8fd9\u4e2a\u5728 cifar \u6570\u636e\u96c6\u7684 getitem \u51fd\u6570\u91cc\uff0c\u5199\u4e86\n    imgs, targets = data\n    print(imgs.shape)\n    print(targets)\n</code></pre> <p>\u6253\u5370\u8f93\u51fa\u4e0bimgs\u548ctargets\uff0c\u662f\u4e24\u4e2a\u5217\u8868\u3002</p> <pre><code>writer = SummaryWriter(\"dataloader\")\nfor epoch in range(2):  # \u4e24\u8f6e,\u9a8c\u8bc1shuffle\u6bcf\u6b21\u6253\u4e71\u662f\u968f\u673a\u6253\u4e71(\u53ef\u4ee5\u8bbe\u7f6e\u968f\u673a\u79cd\u5b50\uff0c\u6bcf\u6b21\u6253\u4e71\u987a\u5e8f\u4e00\u6837)\n    step = 0\n    for data in test_loader:  \n        imgs, targets = data\n        writer.add_images(\"Epoch: {}\".format(epoch), imgs, step)\n        step = step + 1\n\nwriter.close()\n</code></pre> <p>\u5b58\u4e0btensorboard\uff0c\u770b\u4e0b\u4e24\u6b21\u6253\u4e71\u662f\u5426\u4e0d\u4e00\u6837\u3002</p> <p></p> <p>\u53d1\u73b0\u4e24\u6b21\u662f\u4e0d\u4e00\u6837\u7684\u3002</p> <p>\u5173\u4e8e\u6570\u636e\u9884\u5904\u7406\u7684\u90e8\u5206\u5230\u6b64\u7ed3\u675f\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p16%20nn.module/","title":"nn.module","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p16%20nn.module/#_1","title":"\u4ecb\u7ecd","text":"<p>nn.module\u662fpytorch\u6846\u67b6\u4e2d\u642d\u5efa\u795e\u7ecf\u7f51\u7edc\u7684\u57fa\u7840\uff0c\u6240\u6709\u7684\u795e\u7ecf\u7f51\u7edc\u90fd\u9700\u8981\u7ee7\u627f\u8fd9\u4e2a\u6a21\u5757\u3002</p> <p>ps:\u8fd9\u91cc\u8fd8\u662f\u4ed4\u7ec6\u770b\u5b98\u65b9\u6587\u6863</p> <pre><code>import torch\nfrom torch import nn\n\n\nclass Model(nn.Module):\n    def __init__(self): \n        super().__init__() \n\n    def forward(self, input):  # \u524d\u5411\u4f20\u64ad\n        output = input + 23\n        return output\n\nmodel = Model()  \nx = torch.tensor(1.0)  \noutput = model(x)  \nprint(output)\n</code></pre> <p>\u8fd9\u5757\u5c31\u662f\u8bb2\u4e86\u795e\u7ecf\u7f51\u7edc\u7ee7\u627f\u4e0b\u8fd9\u4e2a\u7c7b\uff0c\u627e\u4e2a\u7b80\u5355\u795e\u7ecf\u7f51\u7edc\u5199\u4e00\u4e0b\u5c31\u61c2\u4e86\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p17%20%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C/","title":"\u5377\u79ef\u64cd\u4f5c","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p17%20%E5%8D%B7%E7%A7%AF%E6%93%8D%E4%BD%9C/#_1","title":"\u4ecb\u7ecd","text":"<p>import torch.nn.functional as F </p> <p>\u8fd9\u91ccfunction\u662fnn\u7684\u5b50\u5305\uff0c\u4f46\u662f\u5f88\u5e38\u7528\uff0c\u4e00\u822c\u5355\u72ec\u5bfc\u5165\u3002</p> <p>\u7136\u540e\u4ecb\u7ecd\u5377\u79ef\u64cd\u4f5c\uff1a</p> <p>\u200b       \u5377\u79ef\u64cd\u4f5c\u76ee\u7684\u662f\u51cf\u5c11\u56fe\u7247\u7ef4\u5ea6(\u51cf\u5c11\u4e86\u53c2\u6570\u6570\u91cf),\u4fdd\u7559\u5173\u952e\u4fe1\u606f\u3002</p> <p></p> <p>\u5177\u4f53\u64cd\u4f5c\u662f\u5377\u79ef\u6838\u5bf9\u4f4d\u76f8\u4e58\u4f5c\u4e3a\u7ed3\u679c\uff0c\u7b97\u5b8c\u4e4b\u540e\u5f80\u540e\u5f80\u4e0b\u8d70\u3002</p> <p>\u8fd9\u91cc\u4e0d\u60f3\u591a\u8bb0\u3002</p> <p>\u7136\u540e\u4ee3\u7801\uff1a</p> <pre><code>import torch\nimport torch.nn.functional as F  # \u5236\u4f5c\u4e00\u4e2a\u51fd\u6570\u7684\u53e5\u67c4\uff0c\u540e\u9762\u65b9\u4fbf\u76f4\u63a5\u4f7f\u7528\u4e86\n\ninput = torch.tensor([[1, 2, 0, 3, 1],\n                      [0, 1, 2, 3, 1],\n                      [1, 2, 1, 0, 0],\n                      [5, 2, 3, 1, 1],\n                      [2, 1, 0, 1, 1]])\n\nkernel = torch.tensor([[1, 2, 1],\n                       [0, 1, 0],\n                       [2, 1, 0]])\nprint(\"input:\", input)\nprint(\"kernel:\", kernel)\n\nprint(\"input.shape:\", input.shape)\nprint(\"kernel.shape:\", kernel.shape)\n\n\n#  \u8981\u60f3\u7528 torch.nn.functional.conv2d \u8fd9\u4e2a\u51fd\u6570\uff0c\u5c31\u5fc5\u987b\u6ee1\u8db3\u5f62\u72b6\u7684\u8981\u6c42\uff0c\u4e0a\u8ff0\u7684\u5c3a\u5bf8\u4e0d\u6ee1\u8db3\uff0c\u8981\u505a\u5904\u7406\n#  \u4e0a\u8ff0\u7684\u5c3a\u5bf8\uff0c\u53ea\u6709input.shape: torch.Size([5, 5])\uff0c kernel.shape: torch.Size([3, 3])\uff0c\u5e76\u6ca1\u67094\u4e2a\u901a\u9053\n\n\ninput = torch.reshape(input, (1, 1, 5, 5))  # \u6ce8\u610f\u8fd94\u4e2a\u6570\u5b57\u7684\u610f\u4e49\uff0c\u5206\u522b\u662f\uff1abatch_size, in_channel, H, W , \u53d8\u6362\u5f62\u72b6\u4e4b\u540e\uff0c\u91cd\u65b0\u8d4b\u503c\u7ed9 input\nkernel = torch.reshape(kernel, (1, 1, 3, 3))  # \u6ce8\u610f\u8fd94\u4e2a\u6570\u5b57\u7684\u610f\u4e49\uff0c\u8ddf\u4e0a\u9762\u7684\u4e0d\u4e00\u6837\u4e86\n\nprint(\"input.shape:\", input.shape)\nprint(\"kernel.shape:\", kernel.shape)\n\noutput = F.conv2d(input, kernel, stride=1)\nprint(output)\n\noutput2 = F.conv2d(input, kernel, stride=2)\nprint(output2)\n\noutput3 = F.conv2d(input, kernel, stride=1, padding=1)  # padding \u8bbe\u7f6e\u7684\u503c\uff0c\u662f\u5f80\u5916\u6269\u5145\u7684\u884c\u5217\u6570\uff0c\u503c\u90fd\u662f0\uff0c\u81f3\u4e8e\u60f3\u8981\u4fee\u6539\u8fd9\u4e2a\u503c\uff0c\u8fd8\u6709\u53e6\u5916\u4e00\u4e2a\u53c2\u6570\uff0c\u4e00\u822c\u4e0d\u6539\nprint(output3)\n\noutput4 = F.conv2d(input, kernel, stride=1, padding=0)  # padding \u9ed8\u8ba4\u503c\u662f 0\nprint(output4)\n</code></pre> <p>\u6ce8\u610f\u7684\u51e0\u4e2a\u70b9\uff1a</p> <p>\u200b   1.\u6ce8\u610f\u5377\u79ef\u8981\u6c42\u7684\u8f93\u5165\u7ef4\u5ea6\uff0c\u53c2\u6570\u8981\u56db\u4e2abatch_size(\u6279\u91cf\u5927\u5c0f)\uff0cin_channel(\u8f93\u5165\u901a\u9053\u6570)\uff0c\u9ad8\u548c\u5bbd</p> <p>\u200b   2.\u53c2\u6570\u8bf4\u660e:</p> <p>\u200b           stride  \u6b65\u957f\uff0c\u5373\u6bcf\u6b21\u5377\u79ef\u8fd0\u7b97\u5b8c\u6210\u8d70\u51e0\u4e2a\u683c\u5b50\u3002</p> <p>\u200b           padding  \u586b\u5145\uff0c\u4e0a\u4e0b\u5de6\u53f3\u90fd\u586b\u5145\uff0c\u586b\u51451\u53733x3\u53d85x5</p> <p>\u200b           \u56fe\u7247\u8f85\u52a9\u7406\u89e3\uff1a</p> <p></p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p18%20%E5%8D%B7%E7%A7%AF%E5%B1%82/","title":"\u5377\u79ef\u5c42","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p18%20%E5%8D%B7%E7%A7%AF%E5%B1%82/#_1","title":"\u4ecb\u7ecd","text":"<p>\u5377\u79ef\u5c42\u5bf9\u5e94\u524d\u9762\u7684\u5377\u79ef\u64cd\u4f5c\uff0c\u76f4\u63a5\u4f5c\u4e3a\u795e\u7ecf\u7f51\u7edc\u7684\u4e00\u5c42\u3002</p> <pre><code>import torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import Conv2d\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n</code></pre> <pre><code>dataset = torchvision.datasets.CIFAR10(\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                       download=True)\ndataloader = DataLoader(dataset, batch_size=64)\n\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.conv1 = Conv2d(in_channels=3, out_channels=6, kernel_size=3, stride=1, padding=0)  # \u8f93\u5165\u901a\u9053(\u8fd9\u91ccRGB\u662f3)\u3001\u8f93\u51fa\u901a\u9053(\u5377\u79ef\u6838\u6570\u91cf)\n    def forward(self, x):\n        x = self.conv1(x)  # x \u5df2\u7ecf\u653e\u5230\u4e86\u5377\u79ef\u5c42 conv1\u5f53\u4e2d\u4e86\n        return x\n\n\ntudui = Tudui()  # \u521d\u59cb\u5316\u7f51\u7edc\nprint(tudui)\n\n# \u4e0b\u9762\u628a\u6bcf\u4e00\u5f20\u56fe\u50cf\u90fd\u8fdb\u884c\u5377\u79ef\n\nwriter = SummaryWriter(\"logs\")\n\nstep = 0\nfor data in dataloader:\n    imgs, targets = data  \n    output = tudui(imgs)\n    print(\"imgs.shape:\", imgs.shape)  \n    print(\"output.shape:\", output.shape)  \n    # torch.Size([64, 3, 32, 32])\n    writer.add_images(\"input\", imgs, step)\n    # torch.Size([64, 6, 30, 30])  \u7531\u4e8e6\u4e2achannel\u7684\u56fe\u50cf\uff0c\u662f\u65e0\u6cd5\u663e\u793a\u7684\n    # torch.Size([xxx, 3, 30, 30])\n    output = torch.reshape(output, (-1, 3, 30, 30))  # \u5f3a\u884c\u8f6c\uff0c\u65e0\u4f9d\u636e\uff0c\u6b63\u5e38\u53ef\u4ee5\u6c47\u805a\u5c42\u3001\u5947\u5f02\u503c\u5206\u89e3\u7b49\u3002\n    writer.add_images(\"output\", output, step)\n    step += 1\n\nwriter.close()\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e:</p> <p>\u200b   in_channels \u8f93\u5165\u901a\u9053\u6570</p> <p>\u200b   out_channels    \u8f93\u51fa\u901a\u9053\u6570\uff0c\u5bf9\u5e94\u5377\u79ef\u6838\u6570\u91cf</p> <p>\u200b   kernel_size \u5377\u79ef\u6838\u5927\u5c0f\uff0c\u8fd9\u91cc\u5377\u79ef\u6838\u5927\u5c0f\u662fn*n\u7684</p> <p>\u200b   stride  \u6b65\u957f</p> <p>\u200b   padding \u586b\u5145</p> <p>\u540e\u9762\u6ca1\u5565\u53ef\u4ecb\u7ecd\u7684\uff0c\u6ce8\u610f\u4e0b\u7ec6\u8282\u5c31\u884c\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p19%20%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%E5%B1%82/","title":"\u6700\u5927\u6c60\u5316\u5c42","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p19%20%E6%9C%80%E5%A4%A7%E6%B1%A0%E5%8C%96%E5%B1%82/#_1","title":"\u4ecb\u7ecd","text":"<p>\u5982\u679c\u8bf4\u5377\u79ef\u5c42\u628a\u76f8\u90bb\u7684\u4fe1\u606f\u6574\u5408\u8d77\u6765\uff0c\u63d0\u53d6\u4e86\u56fe\u7247\u7684\u7279\u5f81\u3002</p> <p>\u90a3\u4e48\u6c60\u5316\u5c42\u5c31\u662f\u628a\u8fd9\u4e9b\u7279\u5f81\u6574\u5408\u8d77\u6765\uff0c\u6765\u964d\u4f4e\u7279\u5f81\u548c\u4f4d\u7f6e\u7684\u5173\u8054\u7a0b\u5ea6\u3002</p> <p>\u4e00\u822c\u4e8c\u8005\u4ea4\u66ff\u51fa\u73b0\u3002</p> <pre><code>import torch\nfrom torch import nn\nfrom torch.nn import MaxPool2d\n\nimport torchvision\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n</code></pre> <p>\u53c2\u6570\u8bf4\u660e\uff1a</p> <p>\u200b   kernel_size \u5377\u79ef\u6838\u5927\u5c0f</p> <p>\u200b   ceil_mode   \u5bf9\u4e8e\u4e0d\u6ee1\u5377\u79ef\u6838\u5927\u5c0f\u7684\uff0c\u5982\u4f55\u64cd\u4f5c</p> <p></p> <p>\u5982\u4e0a\uff0c\u4e3e\u4f8b\u8bf4\u660e\u4e86True\u548cFalse\u7684\u533a\u522b\u3002</p> <pre><code>dataset = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                       download=True)\n\ndataloader = DataLoader(dataset, batch_size=64)\n\ninput = torch.tensor([[1, 2, 0, 3, 2],\n                      [3, 2, 4, 5, 6],\n                      [3, 4, 5, 6, 2],\n                      [1, 3, 2, 6, 5],\n                      [5, 6, 2, 1, 3]], dtype=torch.float32) #\u5fc5\u987b\u6307\u5b9a\uff0c\u4e0d\u7136\u9ed8\u8ba4\u6574\u578b\u4f1a\u62a5\u9519\n\ninput_reshape = torch.reshape(input, (-1, 1, 5, 5))\n\nprint(input_reshape)\nprint(input_reshape.shape)\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super().__init__()\n        self.maxpool1 = MaxPool2d(kernel_size=3, ceil_mode=True)\n\n    def forward(self, input):\n        output = self.maxpool1(input)\n        return output\n\n\ntudui = Tudui()\n\n\nwriter = SummaryWriter(\"maxpool_log\")\nstep = 0\n\n\nfor data in dataloader:\n    imgs, targets = data\n    output = tudui(imgs)\n    writer.add_images(\"input\", imgs, step)\n    writer.add_images(\"maxpool\", output, step)\n    step += 1\n\nwriter.close()\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p20%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/","title":"\u6fc0\u6d3b\u51fd\u6570","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p20%20%E6%BF%80%E6%B4%BB%E5%87%BD%E6%95%B0/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91cc\u5b9e\u9645\u4e0a\u7528Relu\u6765\u4e3e\u4f8b\u6fc0\u6d3b\u51fd\u6570\u3002</p> <p>\u6fc0\u6d3b\u51fd\u6570\u628a\u7ebf\u6027\u53d8\u6210\u975e\u7ebf\u6027\uff0c\u5177\u6709\u66f4\u591a\u975e\u7ebf\u6027\u7279\u5f81\uff0c\u66f4\u5bb9\u6613\u62df\u5408\u3002</p> <p>Relu\u5c31\u4e24\u4e2a\u53c2\u6570(input,inplace)</p> <p>inplace\u662f\u8bf4\u662f\u5426\u8986\u76d6input(\u9ed8\u8ba4False)</p> <p></p> <pre><code>import torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import ReLU, Sigmoid\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\n'''\n\u975e\u7ebf\u6027\u6fc0\u6d3b\u5c42\uff0c\u53ea\u6709\u4e00\u4e2abatch_size\u4e00\u4e2a\u53c2\u6570\u9700\u8981\u8bbe\u7f6e\n'''\n\ninput = torch.tensor([[1, -0.5],\n                      [-1, 3]])\n\ninput = torch.reshape(input, (-1, 1, 2, 2))\nprint(input.shape)\n\ndataset = torchvision.datasets.CIFAR10(\"../dataset\", train=False, download=True,\n                                       transform=torchvision.transforms.ToTensor())\n\ndataloader = DataLoader(dataset, batch_size=64)\n\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.relu1 = ReLU()\n        self.sigmoid1 = Sigmoid()\n\n    def forward(self, input):\n        output = self.sigmoid1(input)\n        return output\n\n\ntudui = Tudui()\n\nwriter = SummaryWriter(\"logs_relu\")\nstep = 0\nfor data in dataloader:\n    imgs, targets = data\n    writer.add_images(\"input\", imgs, step)\n    output = tudui(imgs)\n    writer.add_images(\"output\", output, step)\n    step += 1\n\nwriter.close()\n\n'''\n\u5b9e\u73b0\u7684\u6548\u679c\u5c31\u662f\uff1a\ntorch.Size([1, 1, 2, 2])\ntensor([[1., 0.],\n        [0., 3.]])\n'''\n\n'''\n\u975e\u7ebf\u6027\u53d8\u6362\u7684\u76ee\u7684\uff1a\n\u7ed9\u7f51\u7edc\u4e2d\uff0c\u5f15\u5165\u975e\u7ebf\u6027\u7684\u7279\u5f81\uff0c\u975e\u7ebf\u6027\u7279\u5f81\u591a\u7684\u8bdd\uff0c\u624d\u80fd\u8bad\u7ec3\u51fa\u7b26\u5408\u5404\u79cd\u66f2\u7ebf\u6216\u7279\u5f81\u7684\u6a21\u578b\n\u5426\u5219\uff0c\u6cdb\u5316\u80fd\u529b\u4e0d\u597d\n'''\n</code></pre> <p>\u4ee3\u7801\u6ca1\u4ec0\u4e48\u503c\u5f97\u5173\u6ce8\u7684\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p21%20%E7%BA%BF%E6%80%A7%E5%B1%82/","title":"\u7ebf\u6027\u5c42","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p21%20%E7%BA%BF%E6%80%A7%E5%B1%82/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fc7\u4e86\u4e0bpytorch\u4e2d\u7684\u5404\u79cd\u5c42\u5427\uff0c\u53ea\u80fd\u8bf4\uff0c\u5b66\u4f1a\u81ea\u5df1\u770b\u5b98\u65b9\u6587\u6863\u3002</p> <pre><code>import torch\nimport torchvision\nfrom torch import nn\nfrom torch.nn import Linear\nfrom torch.utils.data import DataLoader\n</code></pre> <pre><code>dataset = torchvision.datasets.CIFAR10(\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                       download=True)\n\ndataloader = DataLoader(dataset, batch_size=64,drop_last=True)\n\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.linear1 = Linear(196608, 10)\n\n    def forward(self, input):\n        output = self.linear1(input)\n        return output\n\n\ntudui = Tudui()\n\nfor data in dataloader:\n    imgs, targets = data\n    # print(imgs.shape)\n    # output = torch.reshape(imgs, (1, 1, 1, -1))\n    output = torch.flatten(imgs)  # \u5c55\u5e73(1\u884c)\n    print(output.shape)\n    output = tudui(output)\n    print(output.shape)\n\n'''\n\u6b63\u5219\u5316\u5c42  Normalization Layers   nn.BatchNorm2d \n\u6709\u4e00\u7bc7\u8bba\u6587\uff0c\u610f\u601d\u662f\u6b63\u5219\u5316\u5c42\u53ef\u4ee5\u63d0\u9ad8\u8bad\u7ec3\u901f\u5ea6\n\n\u53c2\u6570\u53ea\u6709\u4e00\u4e2a\uff0cchannel\u4e2d\u7684C\uff0cnum_feature, \u4ee4\u5176\u8ddf channel \u6570\u76f8\u540c\u5373\u53ef\uff0c\u5b98\u65b9\u6587\u6863\u6709\u4e2a\u4e8b\u4f8b\uff1a\n\n&gt;&gt;&gt; # With Learnable Parameters\n&gt;&gt;&gt; m = nn.BatchNorm2d(100)\n&gt;&gt;&gt; # Without Learnable Parameters           # \u4e0d\u542b\u53ef\u5b66\u4e60\u53c2\u6570\n&gt;&gt;&gt; m = nn.BatchNorm2d(100, affine=False)    # \u8fd9\u91cc\u7684 100\uff0c\u662f\u8ddf\u7740\u4e0b\u4e00\u884c\u7684100\uff08channel\uff09\u8bbe\u7f6e\u7684\n&gt;&gt;&gt; input = torch.randn(20, 100, 35, 45)\n&gt;&gt;&gt; output = m(input)\n\n'''\n\n'''\n\u5b98\u65b9\u6587\u6863\u6709\u4e00\u4e9b\u5199\u597d\u7684\u7f51\u7edc\n'''\n</code></pre> <p>\u8fd9\u5757\u592a\u719f\u6089\u4e86\uff0c\u4e0d\u5c55\u5f00\u4e86\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p22%20sequential/","title":"sequential","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p22%20sequential/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91cc\u4ecb\u7ecd\u4e86sequential</p> <p>\u4e0d\u8fc7\u5b9e\u73b0\u7684\u65f6\u5019\u91cd\u70b9\u5728\u7b97\u4e0b\u53c2\u6570\u5427\u3002</p> <p></p> <p>\u4ee3\u7801\u770b\u4e0bnotebook\u5c31OK\u4e86\u3002</p> <pre><code>import torch\nfrom torch import nn\nfrom torch.nn import Conv2d, MaxPool2d, Linear\nfrom torch.nn.modules.flatten import Flatten\nfrom torch.nn.modules import Sequential\n# from torch.utils.tensorboard import SummaryWriter\n\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.conv1 = Conv2d(3, 32, 5, padding=2)  \n        self.maxpool1 = MaxPool2d(2)   \n        self.conv2 = Conv2d(32, 32, 5, padding=2)\n        self.maxpool2 = MaxPool2d(2)\n        self.conv3 = Conv2d(32, 64, 5, padding=2)\n        self.maxpool3 = MaxPool2d(2)\n        self.flatten = Flatten()  # \u5c55\u5e73\u64cd\u4f5c\n        self.linear1 = Linear(64 * 4 * 4, 64)\n        self.linear2 = Linear(64, 10)\n        self.model1 = Sequential(\n            Conv2d(3, 32, 5, padding=2)  ,\n            MaxPool2d(2)   ,\n            Conv2d(32, 32, 5, padding=2),\n            MaxPool2d(2),\n            Conv2d(32, 64, 5, padding=2),\n            MaxPool2d(2),\n            Flatten(),  # \u5c55\u5e73\u64cd\u4f5c\n            Linear(64 * 4 * 4, 64),\n            Linear(64, 10))\n\n    def forward(self, m):\n        # m = self.conv1(m)\n        # m = self.maxpool1(m)\n        # m = self.conv2(m)\n        # m = self.maxpool2(m)\n        # m = self.conv3(m)\n        # m = self.maxpool3(m)\n        # m = self.flatten(m)\n        # m = self.linear1(m)\n        # m = self.linear2(m)\n        m = self.model1(m)\n        return m\n\n\ntudui = Tudui()\nprint(\"tudui:\", tudui)\ninput = torch.ones((64, 3, 32, 32))\noutput = tudui(input)\nprint(\"output.shape:\", output.shape)\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p23%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/","title":"\u635f\u5931\u51fd\u6570\u548c\u53cd\u5411\u4f20\u64ad","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p23%20%E6%8D%9F%E5%A4%B1%E5%87%BD%E6%95%B0%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91ccup\u8bb2\u4e86\u770b\u5b98\u65b9\u6587\u6863\u7684\u635f\u5931\u51fd\u6570\u3002</p> <p></p> <p></p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p24%20%E4%BC%98%E5%8C%96%E5%99%A8/","title":"\u4f18\u5316\u5668","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p24%20%E4%BC%98%E5%8C%96%E5%99%A8/#_1","title":"\u4ecb\u7ecd","text":"<p>\u4f18\u5316\u5668\u8fd9\u91cc\u8fd8\u662f\u8be6\u7ec6\u8bf4\u4e00\u4e0b\u5427\u3002</p> <p>\u6bcf\u4e2a\u4f18\u5316\u5668\u5177\u4f53\u53c2\u6570\u90fd\u4e0d\u592a\u4e00\u6837\uff0c\u4f46\u662f\u4e00\u822c\u90fd\u7528\u9ed8\u8ba4\u503c\uff0c\u53ea\u4f20\u53c2\u6570\u548c\u5b66\u4e60\u7387\u5c31ok\u4e86\u3002</p> <p>\u5b66\u4e60\u7387\u4e0d\u8981\u592a\u5927\uff0c0.01\u597d\u4e00\u4e9b\u3002</p> <pre><code>loss = nn.CrossEntropyLoss()  # \u5b9a\u4e49\u635f\u5931\u51fd\u6570\ntudui = Tudui()\noptim = torch.optim.SGD(tudui.parameters(), lr=0.01)\nfor epoch in range(20):\n    running_loss = 0.0\n    for data in dataloader:\n        imgs, targets = data\n        outputs = tudui(imgs)\n        # print(outputs)\n        # print(targets)\n        result_loss = loss(outputs, targets)  # \u8c03\u7528\u635f\u5931\u51fd\u6570\n        optim.zero_grad()\n        result_loss.backward()  # \u53cd\u5411\u4f20\u64ad\uff0c \u8fd9\u91cc\u8981\u6ce8\u610f\u4e0d\u80fd\u4f7f\u7528\u5b9a\u4e49\u635f\u5931\u51fd\u6570\u90a3\u91cc\u7684 loss\uff0c\u800c\u8981\u4f7f\u7528 \u8c03\u7528\u635f\u5931\u51fd\u6570\u4e4b\u540e\u7684 result_loss\n        optim.step()\n        # print(\"OK\")    # \u8fd9\u90e8\u5206\uff0c\u5728debug\u4e2d\u53ef\u4ee5\u770b\u5230 grad \u901a\u8fc7\u53cd\u5411\u4f20\u64ad\u4e4b\u540e\uff0c\u624d\u6709\u503c\uff0cdebug\u4fee\u597d\u4e86\u4e4b\u540e\uff0c\u518d\u6765\u770b\u8fd9\u91cc\n        # print(result_loss)\n        running_loss = running_loss + result_loss\n    print(running_loss)\n</code></pre> <p>\u53cd\u5411\u4f20\u64ad\u540e\uff0c\u4f18\u5316\u5668\u8fdb\u884c\u4f18\u5316\uff0c\u8bb0\u5f97\u5148\u6e05\u96f6\u68af\u5ea6\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p25%20%E7%8E%B0%E6%9C%89%E6%A8%A1%E5%9E%8B%E4%BF%AE%E6%94%B9/","title":"\u73b0\u6709\u6a21\u578b\u4fee\u6539","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p25%20%E7%8E%B0%E6%9C%89%E6%A8%A1%E5%9E%8B%E4%BF%AE%E6%94%B9/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8bb2\u4e86\u5148\u7528\u6a21\u578b\u5bfc\u5165\uff0c\u548c\u4fee\u6539\u5177\u4f53\u5c42\u3002</p> <pre><code># train_data = torchvision.datasets.ImageNet(\"../data_image_net\", split='train', download=True,\n#                                            transform=torchvision.transforms.ToTensor())\n# \u6570\u636e\u96c6\u592a\u5927\u4e86\uff0c\u4e0d\u4e0b\u8f7d\n\n#\nvgg16_false = torchvision.models.vgg16(pretrained=False)\nvgg16_true = torchvision.models.vgg16(pretrained=True)\n\n\ntrain_data = torchvision.datasets.CIFAR10('../dataset', train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\n\n# vgg16_true.add_module('add_linear',nn.Linear(1000, 10))\n# \u8981\u60f3\u7528\u4e8e CIFAR10 \u6570\u636e\u96c6\uff0c \u53ef\u4ee5\u5728\u7f51\u7edc\u4e0b\u9762\u591a\u52a0\u4e00\u884c\uff0c\u8f6c\u621010\u5206\u7c7b\u7684\u8f93\u51fa\uff0c\u8fd9\u6837\u8f93\u51fa\u7684\u7ed3\u679c\uff0c\u8ddf\u4e0b\u9762\u7684\u4e0d\u4e00\u6837\uff0c\u4f4d\u7f6e\u4e0d\u4e00\u6837\n\nvgg16_true.classifier.add_module('add_linear', nn.Linear(1000, 10))\n# \u5c42\u7ea7\u4e0d\u540c\n# \u5982\u4f55\u5229\u7528\u73b0\u6709\u7684\u7f51\u7edc\uff0c\u6539\u53d8\u7ed3\u6784\nprint(vgg16_true)\n\n# \u4e0a\u9762\u662f\u6dfb\u52a0\u5c42\uff0c\u4e0b\u9762\u662f\u5982\u4f55\u4fee\u6539VGG\u91cc\u9762\u7684\u5c42\u5185\u5bb9\nprint(vgg16_false)\nvgg16_false.classifier[6] = nn.Linear(4096, 10)  # \u4e2d\u62ec\u53f7\u91cc\u7684\u5185\u5bb9\uff0c\u662f\u7f51\u7edc\u8f93\u51fa\u7ed3\u679c\u81ea\u5e26\u7684\u7d22\u5f15\uff0c\u5957\u8fdb\u8fd9\u79cd\u683c\u5f0f\uff0c\u5c31\u53ef\u4ee5\u76f4\u63a5\u4fee\u6539\u90a3\u4e00\u5c42\u7684\u5185\u5bb9\nprint(vgg16_false)\n</code></pre> <p>\u4ee3\u7801\u4e0d\u96be\uff0c\u987a\u624b\u95ee\u95eegpt\u5c31\u884c\u4e86\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p26%20%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/","title":"\u6a21\u578b\u4fdd\u5b58\u548c\u4fee\u6539","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p26%20%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/#_1","title":"\u4ecb\u7ecd","text":"<p>\u5c31\u662f\u628a\u53c2\u6570\u4fdd\u5b58\u4e0b\u6765\u3002</p> <p>\u4fdd\u5b58\u7684\u6587\u4ef6\u5b9e\u9645\u4e0a\u6ca1\u5565\u8981\u6c42\u7684\uff0c\u4f46\u662f\u5efa\u8bae\u540e\u7f00.pth</p> <p>\u4e24\u79cd\u65b9\u5f0f\u4fdd\u5b58\uff081\u3001\u5b58\u6a21\u578b+\u53c2\u6570 2\u3001\u53c2\u6570\u5b58\u4e3a\u5b57\u5178\uff09</p> <pre><code>vgg16 = torchvision.models.vgg16(pretrained=False)\n# \u4fdd\u5b58\u65b9\u5f0f1,\u6a21\u578b\u7ed3\u6784+\u6a21\u578b\u53c2\u6570   \u6a21\u578b + \u53c2\u6570 \u90fd\u4fdd\u5b58\ntorch.save(vgg16, \"vgg16_method1.pth\")  # \u5f15\u53f7\u91cc\u662f\u4fdd\u5b58\u8def\u5f84\n# \u4fdd\u5b58\u65b9\u5f0f2\uff0c\u6a21\u578b\u53c2\u6570\uff08\u5b98\u65b9\u63a8\u8350\uff09 \uff0c\u56e0\u4e3a\u8fd9\u4e2a\u65b9\u5f0f\uff0c\u50a8\u5b58\u91cf\u5c0f\uff0c\u5728terminal\u4e2d\uff0cls -all\u53ef\u4ee5\u67e5\u770b\n torch.save(vgg16.state_dict(), \"vgg16_method2.pth\")\n</code></pre> <p>\u5bf9\u5e94\u52a0\u8f7d\u65b9\u5f0f</p> <pre><code># \u65b9\u5f0f1\uff0c\u52a0\u8f7d\u6a21\u578b\nmodel = torch.load(\"vgg16_method1.pth\")\nprint(model)\n\n# \u65b9\u5f0f2\uff0c\u52a0\u8f7d\u6a21\u578b\nvgg16 = torchvision.models.vgg16(pretrained=False)\nvgg16.load_state_dict(torch.load(\"vgg16_method2.pth\"))\nmodel = torch.load(\"vgg16_method2.pth\")\nprint(vgg16)\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p26%20%E6%A8%A1%E5%9E%8B%E4%BF%9D%E5%AD%98%E5%92%8C%E5%8A%A0%E8%BD%BD/#_2","title":"\u9677\u9631","text":"<p>\u5373\u65b9\u5f0f1\u9700\u8981\u58f0\u660e\u4e0b\u6a21\u578b\u5b9a\u4e49/\u5bfc\u5165\u4e0b\u6a21\u578b\u7c7b\u3002</p> <pre><code># \u9677\u9631\uff0c\u7528\u7b2c\u4e00\u79cd\u65b9\u5f0f\u4fdd\u5b58\u65f6\uff0c\u5982\u679c\u662f\u81ea\u5df1\u7684\u6a21\u578b\uff0c\u5c31\u9700\u8981\u5728\u52a0\u8f7d\u4e2d\uff0c\u628aclass\u91cd\u65b0\u5199\u4e00\u904d\uff0c\u4f46\u5e76\u4e0d\u9700\u8981\u5b9e\u4f8b\u5316\uff0c\u5373\u53ef\n# \u8fd9\u4e2a\u9677\u9631\uff0c\u4e5f\u662f\u53ef\u4ee5\u907f\u514d\u7684\uff0c\u6700\u4e0a\u9762\u7684 from model_save import *\uff0c\u5c31\u662f\u5728\u505a\u8fd9\u4e2a\u4e8b\u60c5\uff0c\u907f\u514d\u51fa\u73b0\u9519\u8bef\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.conv1 = nn.Conv2d(3, 64, kernel_size=3)\n\n    def forward(self, x):\n        x = self.conv1(x)\n        return x\n\nmodel = torch.load('tudui_method1.pth')\nprint(model)\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p27-p29%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%A5%97%E8%B7%AF/","title":"\u6a21\u578b\u8bad\u7ec3\u5957\u8def","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p27-p29%20%E6%A8%A1%E5%9E%8B%E8%AE%AD%E7%BB%83%E5%A5%97%E8%B7%AF/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fc7\u4e00\u904d\u6d41\u7a0b\uff0c\u4e00\u6c14\u5475\u6210\u3002</p> <p>\u9884\u5904\u7406-&gt;\u6a21\u578b-&gt;\u5b66\u4e60\u7387\u3001\u4f18\u5316\u5668\u3001\u635f\u5931\u51fd\u6570\u3001epoch\u3002 </p> <p>\u8fd9\u91cc\u7528tensorboard\u8bb0\u5f55\u635f\u5931\u503c\u8fd8\u662f\u8981\u5173\u6ce8\u4e0b\u7684\uff0c\u8fd8\u6709\u6b63\u786e\u7387\u7684\u8868\u793a\u65b9\u6cd5\u3002</p> <p>model.py</p> <pre><code>import torch\nfrom torch import nn\n\n\n# \u642d\u5efa\u795e\u7ecf\u7f51\u7edc\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(64 * 4 * 4, 64),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\nif __name__ == '__main__':\n    tudui = Tudui()\n    input = torch.ones((64, 3, 32, 32))  # \u4e3a\u4ec0\u4e48\u7528ones\uff1f\u524d\u9762\u4e5f\u662f\u7528\u7684ones\u5417\uff1f\n    output = tudui(input)\n    print(output.shape)\n</code></pre> <p>train.py</p> <pre><code>import torchvision\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\nfrom model import *\n\n# \u51c6\u5907\u6570\u636e\u96c6\ntrain_data = torchvision.datasets.CIFAR10(root=\"./data\", train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\ntest_data = torchvision.datasets.CIFAR10(root=\"./data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n\n# length \u957f\u5ea6\ntrain_data_size = len(train_data)\ntest_data_size = len(test_data)\n\n# \u5982\u679ctrain_data_size=10, \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a10\nprint(\"\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(train_data_size))\nprint(\"\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(test_data_size))\n\n# \u5229\u7528 DataLoader \u6765\u52a0\u8f7d\u6570\u636e\u96c6\ntrain_dataloader = DataLoader(train_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n\n# \u521b\u5efa\u7f51\u7edc\u6a21\u578b\ntudui = Tudui()\n\n# \u635f\u5931\u51fd\u6570\nloss_fn = nn.CrossEntropyLoss()\n\n# \u4f18\u5316\u5668\n# learning_rate = 0.01\n# 1e-2=1 x (10)^(-2) = 1 /100 = 0.01\nlearning_rate = 1e-2\noptimizer = torch.optim.SGD(tudui.parameters(), lr=learning_rate)  # \u8fd9\u91cc\u7684\u53c2\u6570\uff0cSGD\u91cc\u9762\u7684\uff0c\u53ea\u8981\u5b9a\u4e49\u4e24\u4e2a\u53c2\u6570\uff0c\u4e00\u4e2a\u662ftudui.parameters()\u672c\u8eab\uff0c\u53e6\u4e00\u4e2a\u662flr\n\n# \u8bbe\u7f6e\u8bad\u7ec3\u7f51\u7edc\u7684\u4e00\u4e9b\u53c2\u6570\n\n# \u8bb0\u5f55\u8bad\u7ec3\u7684\u6b21\u6570\ntotal_train_step = 0\n\n# \u8bb0\u5f55\u6d4b\u8bd5\u7684\u6b21\u6570\ntotal_test_step = 0\n\n# \u8bad\u7ec3\u7684\u8f6e\u6570\nepoch = 10\n\n# \u6dfb\u52a0tensorboard\nwriter = SummaryWriter(\"logs_train\")\n\nfor i in range(epoch):\n    print(\"------------\u7b2c {} \u8f6e\u8bad\u7ec3\u5f00\u59cb------------\".format(i + 1))\n\n    # \u8bad\u7ec3\u6b65\u9aa4\u5f00\u59cb\n    tudui.train()  # \u8fd9\u4e24\u4e2a\u5c42\uff0c\u53ea\u5bf9\u4e00\u90e8\u5206\u5c42\u8d77\u4f5c\u7528\uff0c\u6bd4\u5982 dropout\u5c42\uff1b\u5982\u679c\u6709\u8fd9\u4e9b\u7279\u6b8a\u7684\u5c42\uff0c\u624d\u9700\u8981\u8c03\u7528\u8fd9\u4e2a\u8bed\u53e5\n    for data in train_dataloader:\n        imgs, targets = data\n        outputs = tudui(imgs)\n        loss = loss_fn(outputs, targets)\n\n        # \u4f18\u5316\u5668\u4f18\u5316\u6a21\u578b\n        optimizer.zero_grad()  # \u4f18\u5316\u5668\uff0c\u68af\u5ea6\u6e05\u96f6\n        loss.backward()\n        optimizer.step()\n\n        total_train_step = total_train_step + 1\n        if total_train_step % 100 == 0:\n            print(\"\u8bad\u7ec3\u6b21\u6570\uff1a{}, Loss: {}\".format(total_train_step, loss.item()))  # \u8fd9\u91cc\u7528\u5230\u7684 item()\u65b9\u6cd5\uff0c\u6709\u8bf4\u6cd5\u7684\uff0c\u5176\u5b9e\u52a0\u4e0d\u52a0\u90fd\u884c\uff0c\u5c31\u662f\u8f93\u51fa\u7684\u5f62\u5f0f\u4e0d\u4e00\u6837\u800c\u5df2\n            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)  # \u8fd9\u91cc\u662f\u4e0d\u662f\u5728\u753b\u66f2\u7ebf\uff1f\n\n    # \u6bcf\u8bad\u7ec3\u5b8c\u4e00\u8f6e\uff0c\u8fdb\u884c\u6d4b\u8bd5\uff0c\u5728\u6d4b\u8bd5\u96c6\u4e0a\u6d4b\u8bd5\uff0c\u4ee5\u6d4b\u8bd5\u96c6\u7684\u635f\u5931\u6216\u8005\u6b63\u786e\u7387\uff0c\u6765\u8bc4\u4f30\u6709\u6ca1\u6709\u8bad\u7ec3\u597d\uff0c\u6d4b\u8bd5\u65f6\uff0c\u5c31\u4e0d\u8981\u8c03\u4f18\u4e86\uff0c\u5c31\u662f\u4ee5\u5f53\u524d\u7684\u6a21\u578b\uff0c\u8fdb\u884c\u6d4b\u8bd5\uff0c\u6240\u4ee5\u4e0d\u7528\u518d\u4f7f\u7528\u68af\u5ea6\uff08with no_grad \u90a3\u53e5\uff09\n\n    # \u6d4b\u8bd5\u6b65\u9aa4\u5f00\u59cb\n    tudui.eval()  # \u8fd9\u4e24\u4e2a\u5c42\uff0c\u53ea\u5bf9\u4e00\u90e8\u5206\u5c42\u8d77\u4f5c\u7528\uff0c\u6bd4\u5982 dropout\u5c42\uff1b\u5982\u679c\u6709\u8fd9\u4e9b\u7279\u6b8a\u7684\u5c42\uff0c\u624d\u9700\u8981\u8c03\u7528\u8fd9\u4e2a\u8bed\u53e5\n    total_test_loss = 0\n    total_accuracy = 0\n    with torch.no_grad():  # \u8fd9\u6837\u540e\u9762\u5c31\u6ca1\u6709\u68af\u5ea6\u4e86\uff0c  \u6d4b\u8bd5\u7684\u8fc7\u7a0b\u4e2d\uff0c\u4e0d\u9700\u8981\u66f4\u65b0\u53c2\u6570\uff0c\u6240\u4ee5\u4e0d\u9700\u8981\u68af\u5ea6\uff1f\n        for data in test_dataloader:  # \u5728\u6d4b\u8bd5\u96c6\u4e2d\uff0c\u9009\u53d6\u6570\u636e\n            imgs, targets = data\n            outputs = tudui(imgs)  # \u5206\u7c7b\u7684\u95ee\u9898\uff0c\u662f\u53ef\u4ee5\u8fd9\u6837\u7684\uff0c\u7528\u4e00\u4e2aoutput\u8fdb\u884c\u7ed8\u5236\n            loss = loss_fn(outputs, targets)\n            total_test_loss = total_test_loss + loss.item()  # \u4e3a\u4e86\u67e5\u770b\u603b\u4f53\u6570\u636e\u4e0a\u7684 loss\uff0c\u521b\u5efa\u7684 total_test_loss\uff0c\u521d\u59cb\u503c\u662f0\n            accuracy = (outputs.argmax(1) == targets).sum()  # \u6b63\u786e\u7387\uff0c\u8fd9\u662f\u5206\u7c7b\u95ee\u9898\u4e2d\uff0c\u7279\u6709\u7684\u4e00\u79cd\uff0c\u8bc4\u4ef7\u6307\u6807\uff0c\u8bed\u4e49\u5206\u5272\u4e4b\u7c7b\u7684\uff0c\u4e0d\u4e00\u5b9a\u975e\u8981\u6709\u8fd9\u4e2a\u4e1c\u897f\uff0c\u8fd9\u91cc\u662f\u5b58\u7591\u7684\uff0c\u518d\u770b\u3002\n            total_accuracy = total_accuracy + accuracy\n\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684Loss: {}\".format(total_test_loss))\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6b63\u786e\u7387: {}\".format(total_accuracy / test_data_size))  # \u5373\u4fbf\u662f\u8f93\u51fa\u4e86\u4e0a\u4e00\u884c\u7684 loss\uff0c\u4e5f\u4e0d\u80fd\u5f88\u597d\u7684\u8868\u73b0\u51fa\u6548\u679c\u3002\n    # \u5728\u5206\u7c7b\u95ee\u9898\u4e0a\u6bd4\u8f83\u7279\u6709\uff0c\u901a\u5e38\u4f7f\u7528\u6b63\u786e\u7387\u6765\u8868\u793a\u4f18\u52a3\u3002\u56e0\u4e3a\u5176\u4ed6\u95ee\u9898\uff0c\u53ef\u4ee5\u53ef\u89c6\u5316\u5730\u663e\u793a\u5728tensorbo\u4e2d\u3002\n    # \u8fd9\u91cc\u5728\uff08\u4e8c\uff09\u4e2d\uff0c\u8bb2\u4e86\u5f88\u590d\u6742\u7684\uff0c\u6ca1\u4ed4\u7ec6\u542c\u3002\u8fd9\u91cc\u5f88\u6709\u8bf4\u6cd5\uff0cargmax\uff08\uff09\u76f8\u5173\u7684\uff0c\u6709\u622a\u56fe\u5728word\u7b14\u8bb0\u4e2d\u3002\n    writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n    writer.add_scalar(\"test_accuracy\", total_accuracy / test_data_size, total_test_step)\n    total_test_step = total_test_step + 1\n\n    torch.save(tudui, \"tudui_{}.pth\".format(i))  # \u4fdd\u5b58\u65b9\u5f0f\u4e00\uff0c\u5176\u5b9e\u540e\u7f00\u90fd\u53ef\u4ee5\u81ea\u5df1\u53d6\uff0c\u4e60\u60ef\u7528 .pth\u3002\n    print(\"\u6a21\u578b\u5df2\u4fdd\u5b58\")\n\nwriter.close()\n</code></pre> <p>\u7ec6\u8282\u4e0d\u5c55\u5f00\uff0c\u81ea\u5df1\u56de\u5934\u770b\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p30-p31%20GPU/","title":"\u4f7f\u7528GPU","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p30-p31%20GPU/#_1","title":"\u4ecb\u7ecd","text":"<p>GPU\u7684\u8bad\u7ec3\u901f\u5ea6\u662f\u8fdc\u9ad8\u4e8eCPU\u7684\u3002</p> <p>\u4e24\u79cd\u65b9\u5f0f\u4ea4\u7ed9GPU\u8bad\u7ec3\uff0c\u4e00\u822c\u8fd8\u662f\u65b9\u5f0f2\u3002</p> <p>\u8981\u6539\u7684\u6709\uff1a</p> <p>\u200b   \u6a21\u578b\u3001\u635f\u5931\u51fd\u6570\u3001\u4f18\u5316\u5668\u3002   \u6570\u636e(\u9700\u8981a=a.b)  </p> <p>\u65b9\u5f0f1</p> <p>\u200b   \u9010\u4e2a.cuda()\u5f88\u9ebb\u70e6</p> <pre><code>import time\nimport torch\nimport torchvision\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\ntrain_data = torchvision.datasets.CIFAR10(root=\"../data\", train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\ntest_data = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n\n# length \u957f\u5ea6\ntrain_data_size = len(train_data)\ntest_data_size = len(test_data)\n# \u5982\u679ctrain_data_size=10, \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a10\nprint(\"\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(train_data_size))\nprint(\"\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(test_data_size))\n\n# \u5229\u7528 DataLoader \u6765\u52a0\u8f7d\u6570\u636e\u96c6\ntrain_dataloader = DataLoader(train_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n\n\n# \u521b\u5efa\u7f51\u7edc\u6a21\u578b\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(64 * 4 * 4, 64),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\ntudui = Tudui()\nif torch.cuda.is_available():\n    tudui = tudui.cuda()  # \u8fd9\u662fGPU\u52a0\u901f\u8bad\u7ec3\u7684\u7b2c\u4e00\u90e8\u5206\n\n# \u635f\u5931\u51fd\u6570\nloss_fn = nn.CrossEntropyLoss()\nif torch.cuda.is_available():\n    loss_fn = loss_fn.cuda()  # \u8fd9\u662fGPU\u52a0\u901f\u8bad\u7ec3\u7684\u7b2c\u4e8c\u90e8\u5206\n# \u4f18\u5316\u5668\n# learning_rate = 0.01\n# 1e-2=1 x (10)^(-2) = 1 /100 = 0.01\nlearning_rate = 1e-2\noptimizer = torch.optim.SGD(tudui.parameters(), lr=learning_rate)\n\n# \u8bbe\u7f6e\u8bad\u7ec3\u7f51\u7edc\u7684\u4e00\u4e9b\u53c2\u6570\n# \u8bb0\u5f55\u8bad\u7ec3\u7684\u6b21\u6570\ntotal_train_step = 0\n# \u8bb0\u5f55\u6d4b\u8bd5\u7684\u6b21\u6570\ntotal_test_step = 0\n# \u8bad\u7ec3\u7684\u8f6e\u6570\nepoch = 10\n\n# \u6dfb\u52a0tensorboard\nwriter = SummaryWriter(\"logs_train\")\n\n# \u6dfb\u52a0\u5f00\u59cb\u65f6\u95f4\n\nstart_time = time.time()\n\nfor i in range(epoch):\n    print(\"-------\u7b2c {} \u8f6e\u8bad\u7ec3\u5f00\u59cb-------\".format(i + 1))\n\n    # \u8bad\u7ec3\u6b65\u9aa4\u5f00\u59cb\n    tudui.train()\n    for data in train_dataloader:\n        imgs, targets = data\n        if torch.cuda.is_available():\n            imgs = imgs.cuda()  # \u8fd9\u4e24\u884c\u662fGPU\u52a0\u901f\u7684\u7b2c\u4e09\u90e8\u5206\uff08\u672a\u5b8c\uff09\n            targets = targets.cuda()\n        outputs = tudui(imgs)\n        loss = loss_fn(outputs, targets)\n\n        # \u4f18\u5316\u5668\u4f18\u5316\u6a21\u578b\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_train_step = total_train_step + 1\n        if total_train_step % 100 == 0:\n            end_time = time.time()   # \u7ed3\u675f\u65f6\u95f4\n            print(end_time - start_time)\n\n            print(\"\u8bad\u7ec3\u6b21\u6570\uff1a{}, Loss: {}\".format(total_train_step, loss.item()))\n            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n\n    # \u6d4b\u8bd5\u6b65\u9aa4\u5f00\u59cb\n    tudui.eval()\n    total_test_loss = 0\n    total_accuracy = 0\n    with torch.no_grad():\n        for data in test_dataloader:\n            imgs, targets = data\n            if torch.cuda.is_available():  # \u8fd9\u4e24\u884c\u4e5f\u662f\u5fc5\u4e0d\u53ef\u5c11\u7684\uff0cGPU\u52a0\u901f\u8bad\u7ec3\u7684\u90e8\u5206\n                imgs = imgs.cuda()\n                targets = targets.cuda()\n            outputs = tudui(imgs)\n            loss = loss_fn(outputs, targets)\n            total_test_loss = total_test_loss + loss.item()\n            accuracy = (outputs.argmax(1) == targets).sum()\n            total_accuracy = total_accuracy + accuracy\n\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684 Loss: {}\".format(total_test_loss))\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6b63\u786e\u7387: {}\".format(total_accuracy / test_data_size))\n    writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n    writer.add_scalar(\"test_accuracy\", total_accuracy / test_data_size, total_test_step)\n    total_test_step = total_test_step + 1\n\n    torch.save(tudui, \"tudui_{}.pth\".format(i))\n    print(\"\u6a21\u578b\u5df2\u4fdd\u5b58\")\n\nwriter.close()\n\n'''\n\n\u7f51\u7edc\u6a21\u578b\u3001\u635f\u5931\u51fd\u6570\u3001\u6570\u636e\uff08\u8f93\u5165\u3001\u6807\u6ce8\uff09\n\n\u8c03\u7528 .cuda\n\n\u4ee5\u4e0a\u4e09\u8005\u6709cuda\u65b9\u6cd5\uff0c\u80fd\u591f\u5b9e\u73b0 \n'''\n</code></pre> <p>\u65b9\u5f0f2</p> <p>\u200b   \u5148\u5b9a\u4e49device \u518dtodevice()</p> <pre><code>import time\nimport torch\nimport torchvision\n# \u51c6\u5907\u6570\u636e\u96c6\nfrom torch import nn\nfrom torch.utils.data import DataLoader\nfrom torch.utils.tensorboard import SummaryWriter\n\n# \u5b9a\u4e49\u8bad\u7ec3\u7684\u8bbe\u5907\ndevice = torch.device(\"cuda\")  # \u5b9a\u4e49\u8bad\u7ec3\u7684\u8bbe\u5907\nprint(device)\n\ntrain_data = torchvision.datasets.CIFAR10(root=\"../data\", train=True, transform=torchvision.transforms.ToTensor(),\n                                          download=True)\ntest_data = torchvision.datasets.CIFAR10(root=\"../data\", train=False, transform=torchvision.transforms.ToTensor(),\n                                         download=True)\n\n# length \u957f\u5ea6\ntrain_data_size = len(train_data)\ntest_data_size = len(test_data)\n# \u5982\u679ctrain_data_size=10, \u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a10\nprint(\"\u8bad\u7ec3\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(train_data_size))\nprint(\"\u6d4b\u8bd5\u6570\u636e\u96c6\u7684\u957f\u5ea6\u4e3a\uff1a{}\".format(test_data_size))\n\n# \u5229\u7528 DataLoader \u6765\u52a0\u8f7d\u6570\u636e\u96c6\ntrain_dataloader = DataLoader(train_data, batch_size=64)\ntest_dataloader = DataLoader(test_data, batch_size=64)\n\n\n# \u521b\u5efa\u7f51\u7edc\u6a21\u578b\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(64 * 4 * 4, 64),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\ntudui = Tudui()\ntudui.to(device)  # \u8fd9\u91cc\u65b0\u6dfb\u52a0\u4e86gpu\u52a0\u901f\u7684\u5185\u5bb9    \u8fd9\u91cc\uff0c\u5176\u5b9e\u4e0d\u7528\u53e6\u5916\u8d4b\u503c tudui = xxx\uff0c\u76f4\u63a5\u8c03\u7528 tudui.to(device)\u5c31\u53ef\u4ee5\u7684\n\n# \u635f\u5931\u51fd\u6570\nloss_fn = nn.CrossEntropyLoss()\nloss_fn.to(device)  # \u8fd9\u91cc\u6dfb\u52a0\u4e86\u52a0\u901f\u8bbe\u5907\uff0c\u5176\u5b9e\u4e5f\u662f\u4e0d\u9700\u8981\u91cd\u65b0\u8d4b\u503c\u7684\uff0c\u76f4\u63a5\u8c03\u7528\u5c31\u53ef\u4ee5\u4e86\n# \u4f18\u5316\u5668\n# learning_rate = 0.01\n# 1e-2=1 x (10)^(-2) = 1 /100 = 0.01\nlearning_rate = 1e-2\noptimizer = torch.optim.SGD(tudui.parameters(), lr=learning_rate)\n\n# \u8bbe\u7f6e\u8bad\u7ec3\u7f51\u7edc\u7684\u4e00\u4e9b\u53c2\u6570\n# \u8bb0\u5f55\u8bad\u7ec3\u7684\u6b21\u6570\ntotal_train_step = 0\n# \u8bb0\u5f55\u6d4b\u8bd5\u7684\u6b21\u6570\ntotal_test_step = 0\n# \u8bad\u7ec3\u7684\u8f6e\u6570\nepoch = 10\n\n# \u6dfb\u52a0tensorboard\nwriter = SummaryWriter(\"logs_train\")\n\nstart_time = time.time()\nfor i in range(epoch):\n    print(\"-------\u7b2c {} \u8f6e\u8bad\u7ec3\u5f00\u59cb-------\".format(i + 1))\n\n    # \u8bad\u7ec3\u6b65\u9aa4\u5f00\u59cb\n    tudui.train()\n    for data in train_dataloader:\n        imgs, targets = data\n        imgs = imgs.to(device)  # \u8fd9\u91cc\u662f\u6570\u636e\uff0c\u9700\u8981\u91cd\u65b0\u8d4b\u503c\n        targets = targets.to(device)  # \u8fd9\u91cc\u4e00\u6837\n        outputs = tudui(imgs)\n        loss = loss_fn(outputs, targets)\n\n        # \u4f18\u5316\u5668\u4f18\u5316\u6a21\u578b\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_train_step = total_train_step + 1\n        if total_train_step % 100 == 0:\n            end_time = time.time()\n            print(end_time - start_time)\n            print(\"\u8bad\u7ec3\u6b21\u6570\uff1a{}, Loss: {}\".format(total_train_step, loss.item()))\n            writer.add_scalar(\"train_loss\", loss.item(), total_train_step)\n\n    # \u6d4b\u8bd5\u6b65\u9aa4\u5f00\u59cb\n    tudui.eval()\n    total_test_loss = 0\n    total_accuracy = 0\n    with torch.no_grad():\n        for data in test_dataloader:\n            imgs, targets = data\n            imgs = imgs.to(device)\n            targets = targets.to(device)\n            outputs = tudui(imgs)\n            loss = loss_fn(outputs, targets)\n            total_test_loss = total_test_loss + loss.item()\n            accuracy = (outputs.argmax(1) == targets).sum()\n            total_accuracy = total_accuracy + accuracy\n\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684Loss: {}\".format(total_test_loss))\n    print(\"\u6574\u4f53\u6d4b\u8bd5\u96c6\u4e0a\u7684\u6b63\u786e\u7387: {}\".format(total_accuracy / test_data_size))\n    writer.add_scalar(\"test_loss\", total_test_loss, total_test_step)\n    writer.add_scalar(\"test_accuracy\", total_accuracy / test_data_size, total_test_step)\n    total_test_step = total_test_step + 1\n\n    torch.save(tudui, \"tudui_{}.pth\".format(i))\n    print(\"\u6a21\u578b\u5df2\u4fdd\u5b58\")\n\nwriter.close()\n\n'''\n\nGPU\u52a0\u901f\u7684\u7b2c\u4e8c\u4e2a\u65b9\u6cd5\n\n.to(device)\n\ndevice = torch.device(\u201ccpu\u201d)\n\ntorch.device\uff08\u201ccuda\u201d\uff09\u4e5f\u53ef\u4ee5\n\ntorch.device\uff08\u201ccuda\uff1a0\u201d\uff09\u6709\u591a\u4e2a\u663e\u5361\u65f6\u4f7f\u7528\n\n'''\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p32%20%E9%AA%8C%E8%AF%81%E5%A5%97%E8%B7%AF/","title":"\u6a21\u578b\u9a8c\u8bc1","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p32%20%E9%AA%8C%E8%AF%81%E5%A5%97%E8%B7%AF/#_1","title":"\u4ecb\u7ecd","text":"<p>\u4e0d\u60f3\u591a\u8bf4\u3002</p> <p>\u200b   \u6ce8\u610f\u4e0b torch.no_grad()\u548cmodel.eval()\u5c31\u884c\u4e86\u3002</p> <p>torch.no_grad()\u7981\u7528\u68af\u5ea6\u64cd\u4f5c\uff0c\u8282\u7ea6\u6027\u80fd\u3002</p> <p>model.eval()\u4e0d\u53ea\u662f\u4e0d\u53cd\u5411\u4f20\u64ad\uff0c\u91cc\u9762\u67d0\u4e9b\u5c42\uff08\u5982dropout)\u662f\u4e0d\u751f\u6548\u7684\u3002</p> <pre><code>image_path = \"../imgs/airplane.png\"\n# image_path = \"TuDui/imgs/airplane.png\"   # \u590d\u5236\u76f8\u5bf9\u8def\u5f84\uff0c\u5c31\u662f\u5bf9\u7684\u4e86\nimage = Image.open(image_path)  # PIL\u7c7b\u578b\u7684\u56fe\u7247\nprint(image)\nimage = image.convert('RGB')  # \u8fd9\u91cc\u5728word\u4e2d\uff0c\u6709\u622a\u56fe\uff0c\u662f\u8ddfpng\u7684\u901a\u9053\u6570\u6709\u5173\u7cfb\u7684\n\n# \u56fe\u50cf\u5927\u5c0f\uff0c\u53ea\u80fd\u662f\u6a21\u578b\u4e2d\u768432\uff0c32\uff0c\u7136\u540e\u8f6c\u4e3a totensor \u6570\u636e\u7c7b\u578b\ntransform = torchvision.transforms.Compose([torchvision.transforms.Resize((32, 32)),\n                                            torchvision.transforms.ToTensor()])\n\nimage = transform(image)  # \u5e94\u7528 transform\nprint(image.shape)  # \u6253\u5370\u56fe\u50cf\u5927\u5c0f\n\n\nclass Tudui(nn.Module):\n    def __init__(self):\n        super(Tudui, self).__init__()\n        self.model = nn.Sequential(\n            nn.Conv2d(3, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 32, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Conv2d(32, 64, 5, 1, 2),\n            nn.MaxPool2d(2),\n            nn.Flatten(),\n            nn.Linear(64 * 4 * 4, 64),\n            nn.Linear(64, 10)\n        )\n\n    def forward(self, x):\n        x = self.model(x)\n        return x\n\n\nmodel = torch.load(\"tudui_0.pth\", map_location=torch.device('cpu'))  # \u52a0\u8f7d\u8bad\u7ec3\u6a21\u578b\nprint(model)\nimage = torch.reshape(image, (1, 3, 32, 32))\nmodel.eval()\nwith torch.no_grad():  # \u8fd9\u6b65\u53ef\u4ee5\u8282\u7ea6\u5185\u5b58\uff0c\u63d0\u9ad8\u6027\u80fd\n    output = model(image)\nprint(output)\n\nprint(output.argmax(1))\n</code></pre>"},{"location":"DeepLearning/Pytorch_tudui_intro/p4%20dir%E5%92%8Chelp%E5%87%BD%E6%95%B0/","title":"dir \u548c help \u51fd\u6570","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p4%20dir%E5%92%8Chelp%E5%87%BD%E6%95%B0/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91cc\u628apytorch\u770b\u51fa\u4e00\u4e2a**\u5de5\u5177\u7bb1**\uff0c\u6bcf\u4e2a\u5de5\u5177\u7bb1\u91cc\u9762\u6709\u5c0f\u7684**\u5de5\u5177\u5305**\uff0c\u4e00\u76f4\u5230\u6700\u5c0f\u7684\u76f4\u63a5\u4f7f\u7528\u7684**\u5de5\u5177**\u3002</p> <p>dir()  \u6253\u5f00\u4e0e\u67e5\u770b</p> <p>help() \u6559\u4f60\u4f7f\u7528</p> <p>\u4f8b\u5982\uff1a</p> <p>\u200b       dir(pytorch)-&gt;\u8f93\u51fa1\uff0c2\uff0c3\uff0c4</p> <p>\u200b       dir(pytorch.3)-&gt;\u8f93\u51faa,b,c</p> <p>\u800chelp\u5219\u662f\u5177\u4f53\u4f7f\u7528</p> <p>\u4f8b\u5982\uff1a</p> <p>\u200b       help(pytorch.3.a)-&gt;\u8f93\u51fa\uff1a\u87ba\u4e1d\u5200\u7684\u4f7f\u7528\u65b9\u6cd5\u3002</p> <p>vscode\u81ea\u5df1\u5b9e\u64cd\uff1a</p> <pre><code>import torch\nprint(\"\u67e5\u770btorch\u7684\u5de5\u5177\u5305\\n\",dir(torch))\nprint(\"\u67e5\u770btorch.cuda\u7684\u5de5\u5177\u5305\\n\",dir(torch.cuda))\nprint(\"\u67e5\u770btorch.cuda.is_available\u7684\u5de5\u5177\u5305\\n\",dir(torch.cuda.is_available))\nprint(\"\u67e5\u770bis_available\u65b9\u6cd5\u7684\u4f7f\u7528\\n\",help(torch.cuda.is_available))\n</code></pre> <p>\u7ed3\u679c\u5982\u4e0b\uff1a</p> <p>\u67e5\u770btorch\u7684\u5de5\u5177\u5305  ['AVG', 'AggregationType', 'AliasDb', 'Any', 'AnyType', 'Argument', 'ArgumentSpec', 'AwaitType', 'BFloat16Storage', 'BFloat16Tensor', 'BenchmarkConfig', 'preload_cuda_deps', '_prelu_kernel', '_prims', '_prims_common', '_propagate_xla_data', '_refs', '_register_device_module', '_remove_batch_dim', '_reshape_alias_copy', '_reshape_from_tensor', '_resize_output', 'rowwise_prune', '_running_with_deploy', '_sample_dirichlet', '_saturate_weight_to_fp16', '_scaled_dot_product_attention_math', '_scaled_dot_product_efficient_attention', '_scaled_dot_product_flash_attention', '_scaled_mm', '_segment_reduce', '_shape_as_tensor', '_sobol_engine_draw', '_sobol_engine_ff', 'sobol_engine_initialize_state', 'sobol_engine_scramble', 'softmax', '_softmax_backward_data', '_sources', '_sparse_broadcast_to', '_sparse_broadcast_to_copy', '_sparse_coo_tensor_unsafe', '_sparse_csr_prod', '_sparse_csr_sum', '_sparse_log_softmax_backward_data', '_sparse_semi_structured_linear', '_sparse_softmax_backward_data', '_sparse_sparse_matmul', '_sparse_sum', '_stack', '_standard_gamma', '_standard_gamma_grad', '_storage_classes', '_subclasses', '_sync', '_tensor', '_tensor_classes', '_tensor_str', '_test_autograd_multiple_dispatch', '_test_autograd_multiple_dispatch_view', '_test_autograd_multiple_dispatch_view_copy', '_test_check_tensor', '_test_functorch_fallback', '_test_serialization_subcmul', '_to_cpu', '_to_functional_tensor', '_to_sparse_semi_structured', '_transform_bias_rescale_qkv', '_transformer_encoder_layer_fwd', '_trilinear', '_triton_multi_head_attention', '_triton_scaled_dot_attention', '_unique', '_unique2', '_unpack_dual', '_unsafe_index', '_unsafe_index_put', '_use_cudnn_ctc_loss', '_use_cudnn_rnn_flatten_weight', '_utils', '_utils_internal', '_validate_compressed_sparse_indices', '_validate_sparse_bsc_tensor_args', '_validate_sparse_bsr_tensor_args', '_validate_sparse_compressed_tensor_args', '_validate_sparse_coo_tensor_args', '_validate_sparse_csc_tensor_args', '_validate_sparse_csr_tensor_args', '_values_copy', '_vmap_internals', '_warn_typed_storage_removal', '_weight_norm', '_weight_norm_interface', '_weights_only_unpickler', 'abs', 'abs', 'absolute', 'acos', 'acos_', 'acosh', 'acosh_', 'adaptive_avg_pool1d', 'adaptive_max_pool1d', 'add', 'addbmm', 'addcdiv', 'addcmul', 'addmm', 'addmv', 'addmv_', 'addr', 'adjoint', 'affine_grid_generator', 'alias_copy', 'align_tensors', 'all', 'allclose', 'alpha_dropout', 'alpha_dropout_', 'amax', 'amin', 'aminmax', 'amp', 'angle', 'any', 'ao', 'arange', 'arccos', 'arccos_', 'arccosh', 'arccosh_', 'arcsin', 'arcsin_', 'arcsinh', 'arcsinh_', 'arctan', 'arctan2', 'arctan_', 'arctanh', 'arctanh_', 'are_deterministic_algorithms_enabled', 'argmax', 'argmin', 'argsort', 'argwhere', 'as_strided', 'as_strided_', 'as_strided_copy', 'as_strided_scatter', 'as_tensor', 'asarray', 'asin', 'asin_', 'asinh', 'asinh_', 'atan', 'atan2', 'atan_', 'atanh', 'atanh_', 'atleast_1d', 'atleast_2d', 'atleast_3d', 'attr', 'autocast', 'autocast_decrement_nesting', 'autocast_increment_nesting', 'autograd', 'avg_pool1d', 'backends', 'baddbmm', 'bartlett_window', 'base_py_dll_path', 'batch_norm', 'batch_norm_backward_elemt', 'batch_norm_backward_reduce', 'batch_norm_elemt', 'batch_norm_gather_stats', 'batch_norm_gather_stats_with_counts', 'batch_norm_stats', 'batch_norm_update_stats', 'bernoulli', 'bfloat16', 'bilinear', 'binary_cross_entropy_with_logits', 'bincount', 'binomial', 'bits16', 'bits1x8', 'bits2x4', 'bits4x2', 'bits8', 'bitwise_and', 'bitwise_left_shift', 'bitwise_not', 'bitwise_or', 'bitwise_right_shift', 'bitwise_xor', 'blackman_window', 'block_diag', 'bmm', 'bool', 'broadcast_shapes', 'broadcast_tensors', 'broadcast_to', 'bucketize', 'builtins', 'can_cast', 'candidate', 'cartesian_prod', 'cat', 'ccol_indices_copy', 'cdist', 'cdouble', 'ceil', 'ceil_', 'celu', 'celu_', 'cfloat', 'chain_matmul', 'chalf', 'channel_shuffle', 'channels_last', 'channels_last_3d', 'cholesky', 'cholesky_inverse', 'cholesky_solve', 'choose_qparams_optimized', 'chunk', 'clamp', 'clamp_', 'clamp_max', 'clamp_max_', 'clamp_min', 'clamp_min_', 'classes', 'classproperty', 'clear_autocast_cache', 'clip', 'clip_', 'clone', 'col_indices_copy', 'column_stack', 'combinations', 'compile', 'compiled_with_cxx11_abi', 'compiler', 'complex', 'complex128', 'complex32', 'complex64', 'concat', 'concatenate', 'conj', 'conj_physical', 'conj_physical_', 'constant_pad_nd', 'contiguous_format', 'conv1d', 'conv2d', 'conv3d', 'conv_tbc', 'conv_transpose1d', 'conv_transpose2d', 'conv_transpose3d', 'convolution', 'copysign', 'corrcoef', 'cos', 'cos_', 'cosh', 'cosh_', 'cosine_embedding_loss', 'cosine_similarity', 'count_nonzero', 'cov', 'cpp', 'cpu', 'cross', 'crow_indices_copy', 'ctc_loss', 'ctypes', 'cuda', 'cuda_path', 'cuda_version', 'cudnn_affine_grid_generator', 'cudnn_batch_norm', 'cudnn_convolution', 'cudnn_convolution_add_relu', 'cudnn_convolution_relu', 'cudnn_convolution_transpose', 'cudnn_grid_sampler', 'cudnn_is_acceptable', 'cummax', 'cummin', 'cumprod', 'cumsum', 'cumulative_trapezoid', 'default_generator', 'deg2rad', 'deg2rad_', 'dequantize', 'det', 'detach', 'detach_', 'detach_copy', 'device', 'diag', 'diag_embed', 'diagflat', 'diagonal', 'diagonal_copy', 'diagonal_scatter', 'diff', 'digamma', 'dist', 'distributed', 'distributions', 'div', 'divide', 'dll', 'dll_path', 'dll_paths', 'dlls', 'dot', 'double', 'dropout', 'dropout_', 'dsmm', 'dsplit', 'dstack', 'dtype', 'e', 'eig', 'einsum', 'embedding', 'embedding_bag', 'embedding_renorm_', 'empty', 'empty_like', 'empty_permuted', 'empty_quantized', 'empty_strided', 'enable_grad', 'eq', 'equal', 'erf', 'erf_', 'erfc', 'erfc_', 'erfinv', 'exp', 'exp2', 'exp2_', 'exp_', 'expand_copy', 'expm1', 'expm1_', 'export', 'eye', 'fake_quantize_per_channel_affine', 'fake_quantize_per_tensor_affine', 'fbgemm_linear_fp16_weight', 'fbgemm_linear_fp16_weight_fp32_activation', 'fbgemm_linear_int8_weight', 'fbgemm_linear_int8_weight_fp32_activation', 'fbgemm_linear_quantize_weight', 'fbgemm_pack_gemm_matrix_fp16', 'fbgemm_pack_quantized_matrix', 'feature_alpha_dropout', 'feature_alpha_dropout_', 'feature_dropout', 'feature_dropout_', 'fft', 'fill', 'fill_', 'finfo', 'fix', 'fix_', 'flatten', 'flip', 'fliplr', 'flipud', 'float', 'float16', 'float32', 'float64', 'float8_e4m3fn', 'float8_e5m2', 'float_power', 'floor', 'floor_', 'floor_divide', 'fmax', 'fmin', 'fmod', 'fork', 'frac', 'frac_', 'frexp', 'frobenius_norm', 'from_dlpack', 'from_file', 'from_numpy', 'frombuffer', 'full', 'full_like', 'func', 'functional', 'fused_moving_avg_obs_fake_quant', 'futures', 'fx', 'gather', 'gcd', 'gcd_', 'ge', 'geqrf', 'ger', 'get_autocast_cpu_dtype', 'get_autocast_gpu_dtype', 'get_autocast_ipu_dtype', 'get_autocast_xla_dtype', 'get_default_dtype', 'get_deterministic_debug_mode', 'get_device', 'get_file_path', 'get_float32_matmul_precision', 'get_num_interop_threads', 'get_num_threads', 'get_rng_state', 'glob', 'gradient', 'greater', 'greater_equal', 'grid_sampler', 'grid_sampler_2d', 'grid_sampler_3d', 'group_norm', 'gru', 'gru_cell', 'gt', 'half', 'hamming_window', 'hann_window', 'hardshrink', 'has_lapack', 'has_mkl', 'has_openmp', 'has_spectral', 'heaviside', 'hinge_embedding_loss', 'histc', 'histogram', 'histogramdd', 'hsmm', 'hsplit', 'hspmm', 'hstack', 'hub', 'hypot', 'i0', 'i0_', 'igamma', 'igammac', 'iinfo', 'imag', 'import_ir_module', 'import_ir_module_from_buffer', 'index_add', 'index_copy', 'index_fill', 'index_put', 'index_put_', 'index_reduce', 'index_select', 'indices_copy', 'inf', 'inference_mode', 'init_num_threads', 'initial_seed', 'inner', 'inspect', 'instance_norm', 'int', 'int16', 'int32', 'int64', 'int8', 'int_repr', 'inverse', 'is_anomaly_check_nan_enabled', 'is_anomaly_enabled', 'is_autocast_cache_enabled', 'is_autocast_cpu_enabled', 'is_autocast_enabled', 'is_autocast_ipu_enabled', 'is_autocast_xla_enabled', 'is_complex', 'is_conj', 'is_deterministic_algorithms_warn_only_enabled', 'is_distributed', 'is_floating_point', 'is_grad_enabled', 'is_inference', 'is_inference_mode_enabled', 'is_loaded', 'is_neg', 'is_nonzero', 'is_same_size', 'is_signed', 'is_storage', 'is_tensor', 'is_vulkan_available', 'is_warn_always_enabled', 'isclose', 'isfinite', 'isin', 'isinf', 'isnan', 'isneginf', 'isposinf', 'isreal', 'istft', 'jit', 'kaiser_window', 'kernel32', 'kl_div', 'kron', 'kthvalue', 'last_error', 'layer_norm', 'layout', 'lcm', 'lcm_', 'ldexp', 'ldexp_', 'le', 'legacy_contiguous_format', 'lerp', 'less', 'less_equal', 'lgamma', 'library', 'linalg', 'linspace', 'load', 'lobpcg', 'log', 'log10', 'log10_', 'log1p', 'log1p_', 'log2', 'log2_', 'log_', 'log_softmax', 'logaddexp', 'logaddexp2', 'logcumsumexp', 'logdet', 'logical_and', 'logical_not', 'logical_or', 'logical_xor', 'logit', 'logit_', 'logspace', 'logsumexp', 'long', 'lstm', 'lstm_cell', 'lstsq', 'lt', 'lu', 'lu_solve', 'lu_unpack', 'manual_seed', 'margin_ranking_loss', 'masked',  'masked_fill', 'masked_scatter', 'masked_select', 'math', 'matmul', 'matrix_exp', 'matrix_power', 'matrix_rank', 'max', 'max_pool1d', 'max_pool1d_with_indices', 'max_pool2d', 'max_pool3d', 'maximum', 'mean', 'median', 'memory_format', 'merge_type_from_type_comment', 'meshgrid', 'min', 'minimum', 'miopen_batch_norm', 'miopen_convolution', 'miopen_convolution_add_relu', 'miopen_convolution_relu', 'miopen_convolution_transpose', 'miopen_depthwise_convolution', 'miopen_rnn', 'mkldnn_adaptive_avg_pool2d', 'mkldnn_convolution', 'mkldnn_linear_backward_weights', 'mkldnn_max_pool2d', 'mkldnn_max_pool3d', 'mkldnn_rnn_layer', 'mm', 'mode', 'moveaxis', 'movedim', 'mps', 'msort', 'mul', 'multinomial', 'multiply', 'multiprocessing', 'mv', 'mvlgamma', 'name', 'nan', 'nan_to_num', 'nan_to_num_', 'zeros_like'] \u67e5\u770btorch.cuda\u7684\u5de5\u5177\u5305  ['Any', 'BFloat16Storage', 'BFloat16Tensor', 'BoolStorage', ......,  'warnings'] \u67e5\u770btorch.cuda.is_available\u7684\u5de5\u5177\u5305  ['annotations', 'call', 'class', 'closure', 'code', 'defaults', 'delattr', 'dict', 'dir', 'doc', 'eq', 'format', 'ge', 'get', 'getattribute', 'globals', 'gt', 'hash', 'init', 'init_subclass', 'kwdefaults', 'le', 'lt', 'module', 'name', 'ne', 'new', 'qualname', 'reduce', 'reduce_ex', 'repr', 'setattr', 'sizeof', 'str', 'subclasshook'] Help on function is_available in module torch.cuda:</p> <p>is_available() -&gt; bool     Returns a bool indicating if CUDA is currently available.</p> <p>\u67e5\u770bis_available\u65b9\u6cd5\u7684\u4f7f\u7528  None</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p6-p7%20Dataset/","title":"Dataset","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p6-p7%20Dataset/#_1","title":"\u4ecb\u7ecd","text":"<p>\u8fd9\u91cc\u7528\u5783\u573e\u5206\u7c7b\u6765\u5904\u7406\u3002</p> <p>Dataset: \u63d0\u4f9b\u4e00\u79cd\u65b9\u6cd5\u628a\u6570\u636e\u52a0\u8f7d\u8fdb\u6765\uff0c\u63d0\u4f9b\u4e24\u4e2a\u65b9\u6cd5\uff0c\u5206\u522b\u5b9e\u73b0\u83b7\u53d6\u6570\u636e\u548clabel\u3001\u83b7\u53d6\u6570\u636e\u603b\u91cf\u7684\u529f\u80fd\u3002</p> <p>Dataloader: \u628a\u6570\u636e\u6253\u5305\u4ea4\u7ed9\u6a21\u578b\u8bad\u7ec3\u3001\u5177\u4f53\u540e\u9762\u518d\u63d0\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p6-p7%20Dataset/#_2","title":"\u4ee3\u7801\u5b9e\u6218","text":"<p>\u8fd9\u91cc\u7528\u4e86\u4e00\u4e2a\u4e8c\u5206\u7c7b\u6570\u636e\u96c6\uff1aants\u548cbees</p> <pre><code>from torch.utils.data import Dataset\nfrom PIL import Image\nimport os\n\nclass MyData(Dataset):\n  def __init__(self,root_dir,label_dir):\n    self.root_dir = root_dir\n    self.label_dir = label_dir\n    self.path = os.path.join(self.root_dir,self.label_dir)\n    self.img_path = os.listdir(self.path)\n\n  def __getitem__(self, idx): # \u83b7\u53d6\u6307\u5b9a\u7d22\u5f15\u7684\u6837\u672c\n    img_name = self.img_path[idx]\n    img_item_path = os.path.join(self.root_dir,self.label_dir,img_name)\n    img = Image.open(img_item_path)\n    label = self.label_dir\n    return img,label\n\n  def __len__(self):  # \u83b7\u53d6\u6837\u672c\u603b\u6570\n    return len(self.img_path)\n\n\nif __name__ == '__main__':\n    root_dir = \"data\\\\train\"\n    ants_label_dir = \"ants\"\n    bees_label_dir = \"bees\"\n    ants_dataset = MyData(root_dir, ants_label_dir)\n    bees_dataset = MyData(root_dir, bees_label_dir)\n    train_dataset = ants_dataset + bees_dataset\n    print(len(train_dataset))\n    img,label = train_dataset[200]\n    img.show()\n</code></pre> <p>\u200b \u5305\u542b\u56fe\u7247\u53ca\u5176label\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p8-p9%20TensorBoard/","title":"TensorBoard","text":""},{"location":"DeepLearning/Pytorch_tudui_intro/p8-p9%20TensorBoard/#_1","title":"\u4ecb\u7ecd","text":"<p>TensorBoard\u53ef\u4ee5\u8ba9\u6211\u4eec\u67e5\u770b\u5177\u4f53\u67d0\u4e00\u6b65\u9aa4\u7684\u60c5\u51b5\uff0c\u5982\u4e0a\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p8-p9%20TensorBoard/#_2","title":"\u4ee3\u7801","text":"<pre><code>from torch.utils.tensorboard import SummaryWriter\n# \u4e00\u4e2a\u7c7b\uff0c\u5f80\u4e8b\u4ef6\u6587\u4ef6\u5939\u91cc\u5199\u4e1c\u897f\n\nwriter = SummaryWriter(\"logs\")\n\nfor i in range(100):\n  writer.add_scalar(\"y=x\",i,i)\n\nwriter.close()\n</code></pre> <p>\u8fd9\u91cc\u7684add_scalar\u662f\u5f80\u91cc\u9762\u6dfb\u52a0\u4e00\u4e2a\u6807\u91cf\u7684\u65b9\u6cd5\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p8-p9%20TensorBoard/#_3","title":"\u5177\u4f53\u4f7f\u7528","text":"<p>\u5728\u7ec8\u7aef\u4e2d\u8f93\u5165\uff1atensorboard --logdir=logs --port=6007</p> <p>--logdir\u6307\u5b9a\u6587\u4ef6\u5939  port\u6307\u5b9a\u7aef\u53e3(\u51cf\u5c11\u548c\u522b\u4eba\u7684\u51b2\u7a81)</p> <p>TensorBoard 2.10.0 at http://localhost:6007/ (Press CTRL+C to quit)</p> <p>\u8bbf\u95ee\u8fd9\u4e2a\u5730\u5740\uff1a</p> <p></p> <p>\u5982\u56fe\uff0c\u5177\u4f53\u540e\u7eed\u64cd\u4f5c\u53ef\u4ee5\u81ea\u5df1\u8bd5\u4e00\u4e0b\u3002</p> <p>\u4ee3\u7801\u4e2d\uff1a</p> <pre><code>for i in range(100):\n  writer.add_scalar(\"y=3x\",3*i,i)\n</code></pre> <p>\u524d\u9762\u6807\u7b7e\u4e0d\u6539\u7684\u8bdd\uff0c\u4f1a\u628a\u591a\u4e2a\u6587\u4ef6\u653e\u5728\u4e00\u4e2a\u56fe\u91cc\uff0c\u4f1a\u5f88\u4e71\uff0c\u6240\u4ee5\u8bb0\u5f97\u6539\u6807\u7b7e/\u6362\u4e2a\u6587\u4ef6\u5939\u5b58\u653e\u4e8b\u4ef6\u3002</p>"},{"location":"DeepLearning/Pytorch_tudui_intro/p8-p9%20TensorBoard/#add_image","title":"add_image\u76f8\u5173\u64cd\u4f5c","text":"<pre><code>from torch.utils.tensorboard import SummaryWriter\n# \u4e00\u4e2a\u7c7b\uff0c\u5f80\u4e8b\u4ef6\u6587\u4ef6\u5939\u91cc\u5199\u4e1c\u897f\n\nimport numpy as np\nfrom PIL import Image\n\nwriter = SummaryWriter(\"logs\")\nimage_path = \"data\\\\train\\\\bees\\\\16838648_415acd9e3f.jpg\"  # \u76f8\u5bf9\u8def\u5f84\nimg_PIL = Image.open(image_path) # PIL\u683c\u5f0f\nimg_array = np.array(img_PIL) # \u8f6c\u4e3anumpy\u683c\u5f0f\nprint(img_array.shape)\n\nwriter.add_image(\"test\",img_array,2,dataformats='HWC')\n\n# for i in range(100):\n#   writer.add_scalar(\"y=x\",i,i)\n\nwriter.close()\n</code></pre> <p>\u8fd9\u91ccadd_image\u8981\u6ce8\u610f\u7684\u662f\u56fe\u7247\u7684\u683c\u5f0f\uff0c\u8fd9\u91cc\u56fe\u7247\u683c\u5f0f\u662f(height,width,channel)\uff0c\u8981\u5728\u53c2\u6570\u4e2d\u6307\u51fa\u3002</p> <p>\u7136\u540e\u7b2c\u4e09\u4e2a\u53c2\u6570\u662fstep,\u5373\u7b2cn\u6b65\uff0c\u6211\u4eec\u53ef\u4ee5\u62d6\u52a8\u67e5\u770b\u7b2cn\u6b65\u7684\u56fe\u7247(\u5373\u795e\u7ecf\u7f51\u7edc\u7684\u8f93\u5165)\u3002</p> <p>\u5982\u56fe\uff1a </p>"},{"location":"DeepLearning/other/pytorch_basic/","title":"Pytorch\u57fa\u672c\u529f","text":""},{"location":"DeepLearning/other/pytorch_basic/#_1","title":"\u5e38\u89c1\u7684\u7b97\u5b50","text":""},{"location":"DeepLearning/other/pytorch_basic/#matmul","title":"matmul","text":"<pre><code># test matmul\nmatrix1 = torch.tensor([[1, 2], [3, 4]])\nmatrix2 = torch.tensor([[5, 6], [7, 8]])\n\nresult = torch.matmul(matrix1, matrix2)\n\nexpected_result = torch.tensor([[19, 22], [43, 50]])\n\n# Test if the result is as expected\nassert torch.equal(result, expected_result), f\"Test failed: {result} != {expected_result}\"\n</code></pre>"},{"location":"DeepLearning/other/pytorch_basic/#sigmoid","title":"sigmoid","text":"<p>Sigmoid \u662f\u4e00\u79cd\u5e38\u7528\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u7279\u522b\u662f\u5728\u795e\u7ecf\u7f51\u7edc\u4e2d\u3002\u5b83\u7684\u6570\u5b66\u8868\u8fbe\u5f0f\u4e3a\uff1a</p> <p>\\[ \\sigma(x) = \\frac{1}{1 + e^{-x}} \\]</p> <p>\u5176\u4e2d \\( e \\) \u662f\u81ea\u7136\u5bf9\u6570\u7684\u5e95\u6570\u3002</p> <pre><code>import torch.nn.functional as F\n\n# Define a tensor\ninput_tensor = torch.tensor([0.0, 2.0, -2.0])\n\n# Apply sigmoid function\nresult = torch.sigmoid(input_tensor)\n\n# Expected result\nexpected_result = torch.tensor([0.5, 0.8808, 0.1192])\n\n# Test if the result is as expected\nassert torch.allclose(result, expected_result, atol=1e-4), f\"Test failed: {result} != {expected_result}\"\n</code></pre>"},{"location":"DeepLearning/other/pytorch_basic/#softmax","title":"softmax","text":"<p>Softmax \u662f\u4e00\u79cd\u5e38\u7528\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u7279\u522b\u662f\u5728\u591a\u5206\u7c7b\u95ee\u9898\u4e2d\u3002\u5b83\u5c06\u4e00\u4e2a\u5411\u91cf\u4e2d\u7684\u6bcf\u4e2a\u5143\u7d20\u8f6c\u6362\u4e3a 0 \u5230 1 \u4e4b\u95f4\u7684\u6982\u7387\u503c\uff0c\u5e76\u4e14\u8fd9\u4e9b\u6982\u7387\u503c\u7684\u603b\u548c\u4e3a 1\u3002Softmax \u51fd\u6570\u7684\u6570\u5b66\u8868\u8fbe\u5f0f\u4e3a\uff1a</p> <p>\\[ \\text{softmax}(x_i) = \\frac{e^{x_i}}{\\sum_{j} e^{x_j}} \\]</p> <p>\u5176\u4e2d \\( x_i \\) \u662f\u8f93\u5165\u5411\u91cf\u4e2d\u7684\u7b2c \\( i \\) \u4e2a\u5143\u7d20\uff0c \\( e \\) \u662f\u81ea\u7136\u5bf9\u6570\u7684\u5e95\u6570\u3002</p> <pre><code>import torch.nn.functional as F\n\n# Define a tensor\ninput_tensor = torch.tensor([1.0, 2.0, 3.0])\n\n# Apply softmax function\nresult = F.softmax(input_tensor, dim=0)\n\n# Expected result\nexpected_result = torch.tensor([0.0900, 0.2447, 0.6652])\n\n# Test if the result is as expected\nassert torch.allclose(result, expected_result, atol=1e-4), f\"Test failed: {result} != {expected_result}\"\n</code></pre>"},{"location":"DeepLearning/other/pytorch_basic/#linear","title":"linear","text":"<p>\u7ebf\u6027\u53d8\u6362\uff08Linear Transformation\uff09\u662f\u7ebf\u6027\u4ee3\u6570\u4e2d\u7684\u4e00\u4e2a\u57fa\u672c\u6982\u5ff5\uff0c\u5b83\u53ef\u4ee5\u7528\u6765\u63cf\u8ff0\u5411\u91cf\u7a7a\u95f4\u4e2d\u7684\u53d8\u6362\u3002\u5728\u7ebf\u6027\u4ee3\u6362\u4e2d\uff0c\u8f93\u5165\u5411\u91cf\u901a\u8fc7\u4e00\u4e2a\u77e9\u9635\u53d8\u6362\u4e3a\u8f93\u51fa\u5411\u91cf\u3002\u8fd9\u4e2a\u8fc7\u7a0b\u53ef\u4ee5\u7528\u77e9\u9635\u4e58\u6cd5\u6765\u8868\u793a\u3002</p>"},{"location":"DeepLearning/other/pytorch_basic/#_2","title":"\u65b9\u7a0b\u7ec4\u8868\u793a","text":"<p>\u5047\u8bbe\u6211\u4eec\u6709\u4e00\u4e2a\u7ebf\u6027\u65b9\u7a0b\u7ec4\uff1a</p> <p>\\[ y_1 = a_{11}x_1 + a_{12}x_2 + \\cdots + a_{1n}x_n \\]</p> <p>\\[ y_2 = a_{21}x_1 + a_{22}x_2 + \\cdots + a_{2n}x_n \\]</p> <p>\\[ \\vdots \\]</p> <p>\\[ y_m = a_{m1}x_1 + a_{m2}x_2 + \\cdots + a_{mn}x_n \\]</p> <p>\u5176\u4e2d\uff0c\\( x_1, x_2, \\ldots, x_n \\) \u662f\u8f93\u5165\u53d8\u91cf\uff0c\\( y_1, y_2, \\ldots, y_m \\) \u662f\u8f93\u51fa\u53d8\u91cf\uff0c\\( a_{ij} \\) \u662f\u7cfb\u6570\u3002</p>"},{"location":"DeepLearning/other/pytorch_basic/#_3","title":"\u77e9\u9635\u8868\u793a","text":"<p>\u4e0a\u8ff0\u65b9\u7a0b\u7ec4\u53ef\u4ee5\u7528\u77e9\u9635\u4e58\u6cd5\u6765\u8868\u793a\uff1a</p> <p>\\[ \\mathbf{y} = \\mathbf{A} \\mathbf{x} \\]</p> <p>\u5176\u4e2d\uff1a</p> <ul> <li>\\(\\mathbf{y}\\) \u662f\u8f93\u51fa\u5411\u91cf\uff0c\u5f62\u72b6\u4e3a \\( m \\times 1 \\)</li> <li>\\(\\mathbf{A}\\) \u662f\u7cfb\u6570\u77e9\u9635\uff0c\u5f62\u72b6\u4e3a \\( m \\times n \\)</li> <li>\\(\\mathbf{x}\\) \u662f\u8f93\u5165\u5411\u91cf\uff0c\u5f62\u72b6\u4e3a \\( n \\times 1 \\)</li> </ul> <p>\u5177\u4f53\u6765\u8bf4\uff1a</p> <pre><code>| y1 |     | a11 a12 ... a1n |   | x1 |\n| y2 |     | a21 a22 ... a2n |   | x2 |\n| .. |  =  | ... ... ... ... | * | .. |\n| ym |     | am1 am2 ... amn |   | xn |\n</code></pre>"},{"location":"DeepLearning/other/pytorch_basic/#_4","title":"\u793a\u4f8b","text":"<p>\u5728 PyTorch \u4e2d\uff0c\u53ef\u4ee5\u4f7f\u7528 <code>torch.nn.Linear</code> \u6765\u5b9e\u73b0\u7ebf\u6027\u53d8\u6362\u3002\u4ee5\u4e0b\u662f\u4e00\u4e2a\u7b80\u5355\u7684\u793a\u4f8b\uff1a</p> <p>\\[ y_1 = 0.1 \\cdot x + 0.2 \\cdot y + 0.1 \\]</p> <p>\\[y_2 = 0.3 \\cdot x + 0.4 \\cdot y + 0.2 \\]</p> <p>\\[y_3 = 0.5 \\cdot x + 0.6 \\cdot y + 0.3 \\]</p> <p>\u4ee4 \\[x = 1\uff0c y = 2 \\]. \u5176\u5b9e\u73b0\u4ee3\u7801\u5982\u4e0b</p> <pre><code>import torch\nimport torch.nn as nn\n\n| y1 |     | 0.1  0.2 |   | 1.0 |   | 0.1 |\n| y2 |  =  | 0.3  0.4 | * |     | + | 0.2 |\n| y3 |     | 0.5  0.6 |   | 2.0 |   | 0.3 |\n\n# \u5b9a\u4e49\u8f93\u5165\u5411\u91cf\ninput_tensor = torch.tensor([1.0, 2.0])\n\n# \u5b9a\u4e49\u7ebf\u6027\u5c42\nlinear_layer = nn.Linear(2, 3)  # \u8f93\u5165\u7ef4\u5ea6\u4e3a2\uff0c\u8f93\u51fa\u7ef4\u5ea6\u4e3a3\n\n# \u521d\u59cb\u5316\u6743\u91cd\u548c\u504f\u7f6e\nlinear_layer.weight = nn.Parameter(torch.tensor([[0.1, 0.2], [0.3, 0.4], [0.5, 0.6]]))\nlinear_layer.bias = nn.Parameter(torch.tensor([0.1, 0.2, 0.3]))\n\n# \u8fdb\u884c\u7ebf\u6027\u53d8\u6362\noutput_tensor = linear_layer(input_tensor)\n\nprint(output_tensor)\n\n\n#out =&gt; tensor([0.6000, 1.3000, 2.0000], grad_fn=&lt;ViewBackward0&gt;)\n</code></pre>"},{"location":"DeepLearning/other/pytorch_basic/#relu","title":"relu","text":"<p><code>ReLU</code> \u662f Rectified Linear Unit \u7684\u7f29\u5199\uff0c\u662f\u4e00\u79cd\u5e38\u7528\u7684\u6fc0\u6d3b\u51fd\u6570\uff0c\u5e7f\u6cdb\u5e94\u7528\u4e8e\u795e\u7ecf\u7f51\u7edc\u4e2d\u3002\u5b83\u7684\u5b9a\u4e49\u5982\u4e0b\uff1a</p> <p>\\[ \\text{ReLU}(x) = \\max(0, x) \\]</p> <p>\u5728 PyTorch \u4e2d\uff0c\u53ef\u4ee5\u901a\u8fc7 <code>torch.nn.ReLU</code> \u6216 <code>torch.nn.functional.relu</code> \u6765\u4f7f\u7528 ReLU \u6fc0\u6d3b\u51fd\u6570\u3002\u4f8b\u5982\uff1a</p> <pre><code>import torch\nimport torch.nn as nn\n\n# \u4f7f\u7528 nn.ReLU\nrelu = nn.ReLU()\ninput_tensor = torch.tensor([-1.0, 0.0, 1.0, 2.0])\noutput_tensor = relu(input_tensor)\nprint(output_tensor)  # \u8f93\u51fa: tensor([0., 0., 1., 2.])\n\n# \u4f7f\u7528 torch.nn.functional.relu\nimport torch.nn.functional as F\noutput_tensor = F.relu(input_tensor)\nprint(output_tensor)  # \u8f93\u51fa: tensor([0., 0., 1., 2.])\n</code></pre>"},{"location":"DeepLearning/other/pytorch_basic/#_5","title":"\u64cd\u4f5c\u6570\u636e\u96c6","text":"<p>\u5728 PyTorch \u4e2d\uff0c<code>Dataset</code> \u548c <code>DataLoader</code> \u662f\u5904\u7406\u548c\u52a0\u8f7d\u6570\u636e\u7684\u4e24\u4e2a\u91cd\u8981\u5de5\u5177\u3002<code>Dataset</code> \u7528\u4e8e\u5b58\u50a8\u6837\u672c\u53ca\u5176\u5bf9\u5e94\u7684\u6807\u7b7e\uff0c\u800c <code>DataLoader</code> \u7528\u4e8e\u5c06 <code>Dataset</code> \u5bf9\u8c61\u5305\u88c5\u6210\u4e00\u4e2a\u53ef\u8fed\u4ee3\u5bf9\u8c61\uff0c\u4ee5\u4fbf\u8fdb\u884c\u6279\u91cf\u5904\u7406\u3002</p>"},{"location":"DeepLearning/other/pytorch_basic/#1-dataset","title":"1. \u521b\u5efa\u81ea\u5b9a\u4e49 Dataset","text":"<p>\u9996\u5148\uff0c\u6211\u4eec\u9700\u8981\u521b\u5efa\u4e00\u4e2a\u81ea\u5b9a\u4e49\u7684 <code>Dataset</code> \u7c7b\u3002\u8fd9\u4e2a\u7c7b\u9700\u8981\u7ee7\u627f <code>torch.utils.data.Dataset</code> \u5e76\u5b9e\u73b0\u4ee5\u4e0b\u4e09\u4e2a\u65b9\u6cd5\uff1a</p> <ul> <li><code>__init__</code>: \u521d\u59cb\u5316\u6570\u636e\u96c6</li> <li><code>__len__</code>: \u8fd4\u56de\u6570\u636e\u96c6\u7684\u5927\u5c0f</li> <li><code>__getitem__</code>: \u6839\u636e\u7d22\u5f15\u8fd4\u56de\u4e00\u4e2a\u6837\u672c</li> </ul> <pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\n\nclass CustomDataset(Dataset):\n    def __init__(self, data, labels):\n        self.data = data\n        self.labels = labels\n\n    def __len__(self):\n        return len(self.data)\n\n    def __getitem__(self, idx):\n        sample = self.data[idx]\n        label = self.labels[idx]\n        return sample, label\n</code></pre>"},{"location":"DeepLearning/other/pytorch_basic/#2-dataloader","title":"2. \u521b\u5efa DataLoader","text":"<p>\u63a5\u4e0b\u6765\uff0c\u6211\u4eec\u4f7f\u7528 <code>DataLoader</code> \u6765\u52a0\u8f7d\u6570\u636e\u96c6\u3002<code>DataLoader</code> \u53ef\u4ee5\u81ea\u52a8\u5c06\u6570\u636e\u96c6\u5206\u6210\u5c0f\u6279\u91cf\uff0c\u5e76\u5728\u8bad\u7ec3\u65f6\u8fdb\u884c\u968f\u673a\u6253\u4e71\u3002</p> <pre><code># \u793a\u4f8b\u6570\u636e\ndata = torch.randn(100, 3)  # 100 \u4e2a\u6837\u672c\uff0c\u6bcf\u4e2a\u6837\u672c\u6709 3 \u4e2a\u7279\u5f81\nlabels = torch.randint(0, 2, (100,))  # 100 \u4e2a\u6807\u7b7e\uff0c\u503c\u4e3a 0 \u6216 1\n\n# \u521b\u5efa\u81ea\u5b9a\u4e49\u6570\u636e\u96c6\ndataset = CustomDataset(data, labels)\n\n# \u521b\u5efa DataLoader\ndataloader = DataLoader(dataset, batch_size=10, shuffle=True)\n</code></pre>"},{"location":"DeepLearning/other/pytorch_basic/#3-dataloader","title":"3. \u8fed\u4ee3 DataLoader","text":"<p>\u6700\u540e\uff0c\u6211\u4eec\u53ef\u4ee5\u4f7f\u7528 <code>DataLoader</code> \u6765\u8fed\u4ee3\u6570\u636e\u96c6\u3002\u5728\u8bad\u7ec3\u6a21\u578b\u65f6\uff0c\u6211\u4eec\u901a\u5e38\u4f1a\u5728\u6bcf\u4e2a epoch \u4e2d\u8fed\u4ee3\u6574\u4e2a\u6570\u636e\u96c6\u3002</p> <pre><code>for epoch in range(5):  # \u8bad\u7ec3 5 \u4e2a epoch\n    for batch_data, batch_labels in dataloader:\n        # \u5728\u8fd9\u91cc\u8fdb\u884c\u8bad\u7ec3\n        print(f\"\u6570\u636e: {batch_data}, \u6807\u7b7e: {batch_labels}\")\n</code></pre>"},{"location":"DeepLearning/other/pytorch_basic/#_6","title":"\u635f\u5931\u51fd\u6570\uff0c\u4f18\u5316\u5668\u7684\u4f7f\u7528","text":"<p>\u5728\u6df1\u5ea6\u5b66\u4e60\u4e2d\uff0c\u635f\u5931\u51fd\u6570\u7528\u4e8e\u8861\u91cf\u6a21\u578b\u9884\u6d4b\u503c\u4e0e\u771f\u5b9e\u503c\u4e4b\u95f4\u7684\u5dee\u8ddd\uff0c\u800c\u4f18\u5316\u5668\u5219\u7528\u4e8e\u8c03\u6574\u6a21\u578b\u53c2\u6570\u4ee5\u6700\u5c0f\u5316\u635f\u5931\u51fd\u6570\u7684\u503c\u3002PyTorch \u63d0\u4f9b\u4e86\u591a\u79cd\u635f\u5931\u51fd\u6570\u548c\u4f18\u5316\u5668\uff0c\u4e0b\u9762\u6211\u4eec\u5c06\u4ecb\u7ecd\u5982\u4f55\u4f7f\u7528\u5b83\u4eec\u3002</p>"},{"location":"DeepLearning/other/pytorch_basic/#1","title":"1. \u635f\u5931\u51fd\u6570","text":"<p>PyTorch \u5728 <code>torch.nn</code> \u6a21\u5757\u4e2d\u63d0\u4f9b\u4e86\u591a\u79cd\u5e38\u7528\u7684\u635f\u5931\u51fd\u6570\u3002\u4ee5\u4e0b\u662f\u4e00\u4e9b\u5e38\u89c1\u7684\u635f\u5931\u51fd\u6570\u53ca\u5176\u4f7f\u7528\u793a\u4f8b\uff1a</p> <ul> <li><code>nn.MSELoss</code>: \u5747\u65b9\u8bef\u5dee\u635f\u5931\uff0c\u5e38\u7528\u4e8e\u56de\u5f52\u4efb\u52a1\u3002</li> <li><code>nn.CrossEntropyLoss</code>: \u4ea4\u53c9\u71b5\u635f\u5931\uff0c\u5e38\u7528\u4e8e\u5206\u7c7b\u4efb\u52a1\u3002</li> </ul> <pre><code>import torch\nimport torch.nn as nn\n\n# \u793a\u4f8b\uff1a\u5747\u65b9\u8bef\u5dee\u635f\u5931\nmse_loss = nn.MSELoss()\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.randn(3, 5)\nloss = mse_loss(input, target)\nprint(f\"MSE Loss: {loss.item()}\")\n\n# \u793a\u4f8b\uff1a\u4ea4\u53c9\u71b5\u635f\u5931\ncross_entropy_loss = nn.CrossEntropyLoss()\ninput = torch.randn(3, 5, requires_grad=True)\ntarget = torch.tensor([1, 0, 4])\nloss = cross_entropy_loss(input, target)\nprint(f\"Cross Entropy Loss: {loss.item()}\")\n</code></pre>"},{"location":"DeepLearning/other/pytorch_basic/#gpu","title":"\u7528gpu\u624b\u5199\u8bad\u7ec3\u548c\u9884\u6d4b\u4e00\u4e2a\u6a21\u578b","text":""},{"location":"DeepLearning/other/%E5%85%B3%E4%BA%8E%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E7%90%86%E8%A7%A3/","title":"\u540e\u5411\u4f20\u64ad","text":""},{"location":"DeepLearning/other/%E5%85%B3%E4%BA%8E%E5%89%8D%E5%90%91%E4%BC%A0%E6%92%AD%E5%92%8C%E5%8F%8D%E5%90%91%E4%BC%A0%E6%92%AD%E7%9A%84%E7%90%86%E8%A7%A3/#_1","title":"\u53c2\u8003\u9605\u8bfb","text":"<p>\u4e0b\u9762\u662f\u6211\u6536\u96c6\u7684\u5b66\u4e60\u8fc7\u7a0b\u79cd\u89c9\u5f97\u4e0d\u9519\u7684\u8d44\u6599</p> <p>\u5148\u770b\u8fd9\u4e2a\uff0c\u8fd9\u4e2a\u8bb2\u4e86\u53cd\u5411\u4f20\u64ad\u7684\u539f\u7406</p> <ul> <li>\u6df1\u5ea6\u5b66\u4e60\u7cfb\u5217\uff082\uff09\uff1a\u524d\u5411\u4f20\u64ad\u548c\u540e\u5411\u4f20\u64ad\u7b97\u6cd5</li> </ul> <p>\u8fd9\u4e2a\u4ecb\u7ecd\u5f97\u6bd4\u8f83\u5168\u9762</p> <ul> <li>\u53cd\u5411\u4f20\u64ad\u7b97\u6cd5\uff08\u8fc7\u7a0b\u53ca\u516c\u5f0f\u63a8\u5bfc</li> </ul>"}]}